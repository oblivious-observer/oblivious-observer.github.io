[{"content":"A friend of mine just forgot the fulldisk encryption password for a laptop.. sounds like a fun little adventure!\nThis is not exactly a new topic, there are nice posts about this problem such as this one, however this is how I\u0026rsquo;ve tried to tackle the problem.\nThe password was generated according to a couple of rules and only partially lost.\nLets assume here for the sake of this little post that the password was generated by using a word list and contained multiple words, some of which are missing.\nLets build ourselves a little prototype:\nFirst let\u0026rsquo;s grab a wordlist, in this case lets grab a copy of the EFFs short word list from here:\n$ wget https://www.eff.org/files/2016/09/08/eff_short_wordlist_1.txt --2023-11-03 18:34:27-- https://www.eff.org/files/2016/09/08/eff_short_wordlist_1.txt Resolving www.eff.org (www.eff.org)... 2a04:4e42:8d::201, 146.75.116.201 Connecting to www.eff.org (www.eff.org)|2a04:4e42:8d::201|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 13660 (13K) [text/plain] Saving to: ‘eff_short_wordlist_1.txt’ eff_short_wordlist_1.txt 100%[====================================================================================================\u0026gt;] 13.34K --.-KB/s in 0.002s 2023-11-03 18:34:28 (6.58 MB/s) - ‘eff_short_wordlist_1.txt’ saved [13660/13660] $ head eff_short_wordlist_1.txt 1111 acid 1112 acorn 1113 acre 1114 acts 1115 afar 1116 affix 1121 aged 1122 agent 1123 agile 1124 aging $ cat wordlist.txt | wc -l 1296 Taking a look at the wordlist, we can see it contains two columns and is 1.3k lines long, since we are only interested in the words itself and this is an example, lets create a new shorter wordlist file:\n$ shuf -n 100 eff_short_wordlist_1.txt | sort | cut -f 2- \u0026gt; wordlist.txt $ head wordlist.txt aloft arson bagel bunny cache cheer cleat clump comma crush $ cat wordlist.txt | wc -l 100 Now lets generate a file containing all possible passwords according to what we know. We have a couple of words we know and are missing two of them, but we know the positions of the missing words.\nLets grab a couple of words start with so we can generate a test password:\n$ shuf -n 6 wordlist.txt march scoff large drove wavy crush So lets assume our password is the following string with X and Y being the missing words:\nmarch scoff X large drove wavy Y crush Based on this we can quickly generate a list of possible passwords from the wordlist:\ninput_string=\u0026#34;march scoff X large drove wavy Y crush\u0026#34; readarray -t words \u0026lt; wordlist.txt for word_x in \u0026#34;${words[@]}\u0026#34;; do for word_y in \u0026#34;${words[@]}\u0026#34;; do replaced_string=\u0026#34;${input_string/X/$word_x}\u0026#34; replaced_string=\u0026#34;${replaced_string/Y/$word_y}\u0026#34; echo \u0026#34;$replaced_string\u0026#34; done done \u0026gt; password_list This gives us a list with 10000 possible combnations for passphrases.\nHowever we can probably assume, that the forgotten passphrase does not contain the same two words twice, so lets cut it down a bit further:\ninput_string=\u0026#34;march scoff X large drove wavy Y crush\u0026#34; readarray -t words \u0026lt; wordlist.txt for word_x in \u0026#34;${words[@]}\u0026#34;; do for word_y in \u0026#34;${words[@]}\u0026#34;; do replaced_string=\u0026#34;${input_string/X/$word_x}\u0026#34; replaced_string=\u0026#34;${replaced_string/Y/$word_y}\u0026#34; if [[ \u0026#34;$word_x\u0026#34; != \u0026#34;$word_y\u0026#34; \u0026amp;\u0026amp; \u0026#34;$input_string\u0026#34; != *\u0026#34;$word_x\u0026#34;* \u0026amp;\u0026amp; \u0026#34;$input_string\u0026#34; != *\u0026#34;$word_y\u0026#34;* ]]; then echo \u0026#34;$replaced_string\u0026#34; fi done done \u0026gt; password_list This way we end up with only 8742 possible words, so we have already eliminated ~13% of possible passwords, nice.\nAt this point we\u0026rsquo;ve generated every possible combination that makes sense to check, however the word list still looks like this:\n$ head password_list march scoff aloft large drove wavy arson crush march scoff aloft large drove wavy bagel crush march scoff aloft large drove wavy bunny crush march scoff aloft large drove wavy cache crush march scoff aloft large drove wavy cheer crush march scoff aloft large drove wavy cleat crush march scoff aloft large drove wavy clump crush march scoff aloft large drove wavy comma crush march scoff aloft large drove wavy cycle crush march scoff aloft large drove wavy dab crush Lets add some randomness into the mix by shuffling the order around:\n$ shuf password_list \u0026gt; passwords $ head passwords march scoff thud large drove wavy icon crush march scoff deck large drove wavy slimy crush march scoff said large drove wavy slept crush march scoff jet large drove wavy snap crush march scoff penny large drove wavy fiber crush march scoff decoy large drove wavy slept crush march scoff wipe large drove wavy spoon crush march scoff clump large drove wavy recap crush march scoff deck large drove wavy fiber crush march scoff said large drove wavy radar crush This looks way better already, now a script running through every line of that list won\u0026rsquo;t be prone to extensive runtime due to alphabetical ordering. Hopefully that will speed things up a bit.\nNext we need to take a quick look at LUKS partitions. Luckily for us, LUKS stores the encryption key in its header, which can be used independently from the rest of the partition.\nSo lets quickly create a small (128MB+Header) LUKS container which we can use to test our little script:\n$ dd if=/dev/urandom of=luks_container.img bs=1M count=128 $ cryptsetup --verify-passphrase luksFormat luks_container.img WARNING! ======== This will overwrite data on luks_container.img irrevocably. Are you sure? (Type \u0026#39;yes\u0026#39; in capital letters): YES Enter passphrase for luks_container.img: Verify passphrase: In my case I chose the words from line 100 of the password file as passphrase:\n$ head -n 100 passwords | tail -n 1 march scoff deck large drove wavy jet crush We can now non-interactively check for the password and get a return code of 0 in case of success and 2 in case of the wrong password:\n$ echo \u0026#34;march scoff deck large drove wavy jet crush\u0026#34; | cryptsetup luksOpen --test-passphrase luks_container.img $ echo $? 0 $ echo \u0026#34;not the password!\u0026#34; | cryptsetup luksOpen --test-passphrase luks_container.img No key available with this passphrase. $ echo $? 2 Now lets detach the LUKS container header:\n$ cryptsetup luksHeaderBackup --header-backup-file luks_container_header luks_container.img $ ls -lah Permissions Size User Date Modified Name ... .rw-r--r-- 134M tiwa 3 Nov 19:18 luks_container.img .r-------- 17M tiwa 3 Nov 19:22 luks_container_header ... At this point we are left with a 17MB header file, which we can use independently from the encrypted data to check for the password. The header behaves exactly like the LUKS container:\n$ echo \u0026#34;march scoff deck large drove wavy jet crush\u0026#34; | cryptsetup luksOpen --test-passphrase luks_container_header $ echo $? 0 $ echo \u0026#34;not the password!\u0026#34; | cryptsetup luksOpen --test-passphrase luks_container_header No key available with this passphrase. $ echo $? 2 Bruteforcing the password is quite easily done with a simple loop:\nstart_time=$(date +%s) line_number=0 while IFS= read -r line; do ((line_number += 1)) echo $line_number \u0026gt; progress echo \u0026#34;$line\u0026#34; | cryptsetup luksOpen --test-passphrase luks_container_header return_code=$? if [ $return_code -eq 0 ]; then end_time=$(date +%s) runtime=$((end_time - start_time)) formatted_runtime=$(date -u -d @$runtime +\u0026#39;%H:%M:%S\u0026#39;) echo \u0026#34;Total runtime : $formatted_runtime\u0026#34; \u0026gt;\u0026gt; result echo \u0026#34;PASSPHRASE : $line\u0026#34; \u0026gt;\u0026gt; result exit 0 elif [ $return_code -eq 2 ]; then continue fi done \u0026lt; passwords Running the script with the correct password in line 100 took about 4min on my laptop:\nTotal runtime : 00:03:58 PASSPHRASE : march scoff deck large drove wavy jet crush Now lets add parallel into the mix, a tool, which can run bash functions concurrently on their input. On NixOS we can simply use nix-shell to grab a version of it:\n$ nix-shell -p parallel ... $ parallel --help Usage: parallel [options] [command [arguments]] \u0026lt; list_of_arguments parallel [options] [command [arguments]] (::: arguments|:::: argfile(s))... cat ... | parallel --pipe [options] [command [arguments]] ... From there we just need to rewrite our code to use a function instead of a while loop and then use parallel in order to input the passphrases line by line.\nstart_time=$(date +%s) check_passphrase() { start_time=\u0026#34;$1\u0026#34; line=\u0026#34;${@: -1}\u0026#34; echo \u0026#34;$line\u0026#34; | cryptsetup luksOpen --test-passphrase luks_container_header return_code=$? echo -n \u0026#39;.\u0026#39; \u0026gt;\u0026gt; progress if [ $return_code -eq 0 ]; then end_time=$(date +%s) runtime=$((end_time - start_time)) formatted_runtime=$(date -u -d @$runtime +\u0026#39;%H:%M:%S\u0026#39;) echo \u0026#34;Total runtime : $formatted_runtime\u0026#34; \u0026gt;\u0026gt; result echo \u0026#34;PASSPHRASE : $line\u0026#34; \u0026gt;\u0026gt; result exit 1 fi } export -f check_passphrase parallel --halt 1 --line-buffer -a passwords check_passphrase $start_time Using parallel the script ran only a tiny bit faster, which initially did not look too impressive for a speedup:\nTotal runtime : 00:03:52 PASSPHRASE : march scoff deck large drove wavy jet crush However my laptop only has 4 cores, whereas my desktop for example has 12 all of which are a bit more powerful, there already the speedup is quite significant already almost cutting the time it took to find the password in half:\n$ cat result # without parallel Total runtime : 00:04:15 PASSPHRASE : march scoff deck large drove wavy jet crush $ cat result # with parallel Total runtime : 00:01:46 PASSPHRASE : march scoff deck large drove wavy jet crush In conclusion, given with access to multiple machines (and by splitting up the password list in a way that makes sense), it is probably possible to get ahold of the password in time.\nIf something should go wrong we can even recover the progress by looking into the ./progress file, counting the dots and removing as many lines from the password list.\n","permalink":"https://oblivious.observer/posts/recovering-a-lost-password/","summary":"A friend of mine just forgot the fulldisk encryption password for a laptop.. sounds like a fun little adventure!\nThis is not exactly a new topic, there are nice posts about this problem such as this one, however this is how I\u0026rsquo;ve tried to tackle the problem.\nThe password was generated according to a couple of rules and only partially lost.\nLets assume here for the sake of this little post that the password was generated by using a word list and contained multiple words, some of which are missing.","title":"Recovering a forgotten LUKS password"},{"content":"In the last post, I wrote about how to convert a \u0026rsquo;normal\u0026rsquo; NixOS system to one that is managed by a flake. This one will build on top of, or rather scale out of managing single hosts and dive into how to do remote management and deployment for multiple systems.\nIntroduction Before taking a peek into Deployments, lets set a bit of a frame of reference here first:\nWhen talking about deployments a lot of people think about quite complex setups and the use of services such as autoscaling, instance creation etc. The setup I\u0026rsquo;m going to describe here is fairly simple in comparison.\nMy starting point was just a bunch of hosts, some of them physical, some virtual (as in VM, not as in container) and all of them - at least so far - manually managed and upgraded. Boiling it down, I had access to all of the hosts using ssh.\nThis is where I started to look into deployment tools for NixOS and due to the fact that NixOS is built on top of an amazing package manager, there are quite a bunch of them out there: Here is a list of tools on the awesome-nix page on github (lollipop seems to be missing at the moment).\nUnder the hood most of these tools seem to first build the system configuration somewhere, then copy over the closure of the configuration to the specific host it is intended for and finally switch over to it. Before continuing here, you might want to take a peek into how deployment on NixOS works under the hood, here are a few very interesting pointers:\nThere is a nice talk (+transcript) by Vaibhav Sagar from 2018\u0026rsquo;s linux.conf.au about how to do deployment with NixOS, as well as another nice blog post on how to basically deploy NixOS using bash. Another very nice post by Chris Martin goes into detail on how this can be done on AWS using bash and haskell. Last but definitely not least there is one very nice video as well as a couple of accompanying blog articles by Solène Rapenne on how she implemented deployment on NixOS using a pull instead of a push semantic.\nDeploying NixOS using Colmena When I got started with deploying on NixOS, I initially looked at some of the deployment tools available and after reading through various bits and pieces of documentation and blog posts I decided to give colmena a try.\nAdding all Hosts to the flake To get started, we first need all of the hosts we want to deploy to managed inside a single flake, this is fairly straightforward and was already described in the last post. After adding all of your hosts to the flake, it should look somewhat like this:\n{ description = \u0026#34;Oblivious Infrastructure\u0026#34;; inputs = { flake-utils.url = \u0026#34;github:numtide/flake-utils\u0026#34;; nix.url = \u0026#34;github:NixOS/nix/2.5.1\u0026#34;; nixpkgs.url = \u0026#34;github:NixOS/nixpkgs/nixos-23.05\u0026#34;; nixos-hardware.url = \u0026#34;github:NixOS/nixos-hardware\u0026#34;; }; outputs = { self, nixpkgs, nix, ... }: { nixosConfigurations = { host0 = nixpkgs.lib.nixosSystem { system = \u0026#34;x86_64-linux\u0026#34;; modules = [ ./hosts/host0/configuration.nix ]; }; host1 = nixpkgs.lib.nixosSystem { system = \u0026#34;x86_64-linux\u0026#34;; modules = [ ./hosts/host1/configuration.nix ]; }; ... }; }; } Setting up Host Access In order to access and manage all systems, you need to create a keypair, that has access to the root user on the remote machines. Since colmena does not seem to support the option of using a specific ssh key for a host, I opted to simply defining everything host specific outside of colmena and in the ssh config instead.\nAt this point I\u0026rsquo;d like to mention that apparently colmena (the same applies to deploy-rs) does not seem to support password protected ssh keys, if you try using one you\u0026rsquo;ll see output like this during the deployment step:\n[ERROR] Failed to complete requested operation - Last 1 lines of logs: [ERROR] failure) Child process exited with error code: 1 [ERROR] Failed to push system closure to azwraith - Last 5 lines of logs: [ERROR] created) [ERROR] state) Running [ERROR] stderr) root@1.2.3.4: Permission denied (publickey,password,keyboard-interactive). [ERROR] stderr) error: cannot connect to \u0026#39;root@colmena.host\u0026#39; [ERROR] failure) Child process exited with error code: 1 [ERROR] ----- [ERROR] Operation failed with error: Child process exited with error code: 1 Also colmena does not seem to come with a way to define which key is supposed to be used, when accessing a node: while there are the options deployment.targetHost, deployment.targetPort and deployment.targetUser, there is no option such as deployment.sshKey1.\nSo basically you\u0026rsquo;re supposed to just have a key lying around on your system, which gives you passwordless root level access to all of your infrastructure and is at the same time the default method of accessing all of your nodes via ssh from a terminal. Sounds like a great idea, after all, what could possibly go wrong?\nLuckily, there is actually a fairly easy workaround, which solves all of the little problems I just mentioned. I\u0026rsquo;m not sure how many people are using it at the moment, I haven\u0026rsquo;t really read about it anywhere else, but then again, I might just haven\u0026rsquo;t stumbled over it so far:\nFirst (on the host you want to use to manage your infrastructure with) we need to create a ssh key with a password, e.g. ~/.ssh/colmena.key. using ssh-keygen.\nNext we add the password protected ssh key to the ssh-agent, this way, unlocking the key will be managed by the ssh-agent instead of colmena, so whenever we need to unlock the key, we get a GUI or CLI popup and can simply insert the password and we even get the nice little timeframe wherein the key stays unlocked, so it does not have to be input every single time:\n$ ssh-add ~/.ssh/colmena.key $ ssh-add -L ssh-ed25519 ... user@host Next create an additional colmena entry for the hosts you want to manage in ~/.ssh/config, so in my case I now have 3 entries per host (for the unlock.host entry check out my earlier post, which explains remote decryption):\nHost host Hostname 1.2.3.4 User user Host unlock.host Hostname 1.2.3.4 User root Port 2222 Host colmena.host Hostname 1.2.3.4 User root IdentityFile ~/.ssh/colmena.key You can put the ssh config in a file in your flake repo, which you then include by adding \u0026lsquo;Include /path/to/file\u0026rsquo; at the top of your .ssh/config file. This only works at the top of the config, not inbetween:\n# oblivious hosts: Include /home/user/git/infrastructure/.ssh/config Then create a small module on the managed hosts, that enables everything colmena needs to work, e.g. modules/colmena/default.nix:\n{ # https://github.com/NixOS/nix/issues/2330#issuecomment-451650296 nix.trustedUsers = [ \u0026#34;root\u0026#34; \u0026#34;@wheel\u0026#34; ]; users.users.root.openssh.authorizedKeys.keys = [ \u0026#34;ssh-ed25519 ...\u0026#34; \u0026#34;ssh-rsa ...\u0026#34; ]; } Finally import the module into all of your hosts configurations and one last time update them all manually.\nGetting started with Colmena In order to use colmena, the flake.nix has to be slightly modified, under outputs add a colmena section parallell to nixosConfigurations containing the hosts information, like this:\n... outputs = { self, nixpkgs, nix, ... }: { colmena = { meta = { nixpkgs = import nixpkgs { system = \u0026#34;x86_64-linux\u0026#34;; }; }; host0 = { deployment = { targetHost = \u0026#34;colmena.host0\u0026#34;; # \u0026lt;- defined in ~/.ssh/config }; imports = [ ./hosts/host0/configuration.nix ]; }; ... }; nixosConfigurations = { ... }; }; Note the use of the colmena.host0 entry we earlier defined, so we\u0026rsquo;ll automagically use the correct user and ssh key, when connecting. colmena comes with a targetPort option, but since we are basically defining everything specific to the host access inside of the ssh config, I\u0026rsquo;d advise against using it here, targetPort is preferred over the ssh config, if the option is set. (Using the option directly after adding a new host combined with the copy and paste mistake of not adjusting the targetPort option, can lead to very annoying node-breaking deployments when you\u0026rsquo;re accessing hosts behind a NAT.)\nUpdate the flake git repos on all the hosts using nixos-rebuild --flake in order to enable the root ssh access.\nNext check out the repository containing the host(s) managed and grab a version of colmena, e.g. using nix shell github:zhaofengli/colmena.\nThen on the management host grab and enter a copy of the flake repo and check if everything works by using colmena exec2 in order to execute some commands on the remote hosts. Depending on whether or not the ssh key is still unlocked you should either get a pinentry popup, or everything should just work:\n[user@host:~/git/infrastructure]$ colmena exec -v -- \u0026#39;hostname\u0026#39; [INFO ] Using flake: git+file:///home/user/git/infrastructure [INFO ] Enumerating nodes... copying path \u0026#39;/nix/store/39i5vz1nf5l6q3mxj61yv2ymfms3xhid-source\u0026#39; from \u0026#39;https://cache.nixos.org\u0026#39;... copying path \u0026#39;/nix/store/f1zgyzaq53q962w78gv54rxmmisfqbrk-source\u0026#39; from \u0026#39;https://cache.nixos.org\u0026#39;... [INFO ] Selected all 2 nodes. host0 | host1 | host0 | host0 host0 | Succeeded host1 | host1 host1 | Succeeded | All done! Voila, we just executed some commands on a couple of remote hosts in parallel!\nFrom here on managing the hosts is fairly straightforward. To build the system configurations you can use the build command:\n[user@host:~/git/infrastructure]$ colmena build [INFO ] Using flake: git+file:///home/user/git/infrastructure [INFO ] Enumerating nodes... [INFO ] Selected all 2 nodes. ✅ 39s All done! (...) ✅ 15s Evaluated host0 and host1 host0 ✅ 22s Built \u0026#34;/nix/store/y2w31fwaazjxxq94rcc28qi10xmc0rgz-nixos-system-host0-21.11pre-git host1 ✅ 24s Built \u0026#34;/nix/store/v3ba77v92hbf1grr3kz649ykrlfrrglm-nixos-system-host1-21.11pre-git Deploying to the hosts is done with the apply command:\n[user@host:~/git/infrastructure]$ colmena apply [INFO ] Using flake: git+file:///home/user/git/infrastructure [INFO ] Enumerating nodes... [INFO ] Selected all 2 nodes. ✅ 28s All done! (...) ✅ 14s Evaluated host0 and host1 host0 ✅ 0s Built \u0026#34;/nix/store/y2w31fwaazjxxq94rcc28qi10xmc0rgz-nixos-system-host0-21.11pre-git\u0026#34; host1 ✅ 0s Built \u0026#34;/nix/store/v3ba77v92hbf1grr3kz649ykrlfrrglm-nixos-system-host1-21.11pre-git\u0026#34; host0 ✅ 3s Pushed system closure host1 ✅ 9s Pushed system closure host0 ✅ 5s Activation successful host1 ✅ 5s Activation successful After deploying, at this point in time we have new system state on the managed hosts. However their respective /etc/nixos directories are still on some random version of the repo. We can easily solve this using the exec command however. So on the managment host we can push the repo and then use colmena to make sure the revision on all of the hosts is always the same3:\n[user@host:~/git/infrastructure]$ colmena exec -- \u0026#39;cd /etc/nixos \u0026amp;\u0026amp; git pull\u0026#39; [INFO ] Using flake: git+file:///home/user/git/infrastructure [INFO ] Enumerating nodes... [INFO ] Selected all 2 nodes. ✅ 1s All done! host0 ✅ 1s Succeeded host1 ✅ 1s Succeeded One noteworthy point is that when using colmena, we seem to loose the ability to update a system using conventional methods:\n[user@host:~]$ sudo nixos-rebuild switch --flake /etc/nixos/#host error: infinite recursion encountered at /nix/store/n9dk853bl3ny2mqv4pjh7440jglm8wz8-source/lib/modules.nix:512:28: 511| builtins.addErrorContext (context name) 512| (args.${name} or config._module.args.${name}) | ^ 513| ) (lib.functionArgs f); (use \u0026#39;--show-trace\u0026#39; to show detailed location information) In order to update a host locally, we need to set the allowLocalDeployment option option in flake.nix (optionally it might make sense to set targetHost to null):\n... outputs = { self, nixpkgs, nix, ... }: { colmena = { meta = { nixpkgs = import nixpkgs { system = \u0026#34;x86_64-linux\u0026#34;; }; }; host0 = { deployment = { targetHost = \u0026#34;colmena.host0\u0026#34;; # \u0026lt;- optionally set this to \u0026#39;null\u0026#39; allowLocalDeployment = true; }; imports = [ ./hosts/host0/configuration.nix ]; }; ... }; ... }; ... Then on the host, we can simply grab a copy of the flake repository, build a system configuration using something like colmena build --on host and finally update the system by using the apply-local command:\n[user@host]$ colmena apply-local --sudo [INFO ] Using flake: git+file:///home/user/git/infrastructure host | Evaluating mortimer host | host | Evaluated host host | Building host host | /nix/store/i8vasqriqb184gdc0hnyy38yr9ii2y9w-nixos-system-host-23.05pre-git host | Built \u0026#34;/nix/store/i8vasqriqb184gdc0hnyy38yr9ii2y9w-nixos-system-host-23.05pre-git\u0026#34; host | Pushing system closure host | Pushed system closure host | No pre-activation keys to upload host | Activating system profile [sudo] password for user: host | updating GRUB 2 menu... host | activating the configuration... host | setting up /etc... host | reloading user units for user... host | setting up tmpfiles host | Activation successful host | No post-activation keys to upload | All done! At this point we can basically manage, update and deploy hosts as we want.\nCleaning up the flake Now that everything works, we can start cleaning up the flake a bit. Ever since adding in the colmena section, we have to reference the hosts configurations in two places, so lets include a little let statement and store the information we need in hostConfigs:\n{ description = \u0026#34;Oblivious Infrastructure\u0026#34;; inputs = { flake-utils.url = \u0026#34;github:numtide/flake-utils\u0026#34;; nix.url = \u0026#34;github:NixOS/nix/2.5.1\u0026#34;; nixpkgs.url = \u0026#34;github:NixOS/nixpkgs/nixos-23.05\u0026#34;; nixos-hardware.url = \u0026#34;github:NixOS/nixos-hardware\u0026#34;; }; outputs = {self, nixpkgs, nix, ... }: let hostConfigs = { host0 = [ ./hosts/host0/configuration.nix ]; host1 = [ ./hosts/host1/configuration.nix ]; }; in { colmena = { meta = { nixpkgs = import nixpkgs { system = \u0026#34;x86_64-linux\u0026#34;; }; }; host0 = { deployment = { targetHost = \u0026#34;colmena.host0\u0026#34;; }; imports = [] ++ hostConfigs.host0; }; host1 = { deployment = { targetHost = \u0026#34;colmena.host1\u0026#34;; }; imports = [] ++ hostConfigs.host1; }; }; nixosConfigurations = { host0 = nixpkgs.lib.nixosSystem { system = \u0026#34;x86_64-linux\u0026#34;; modules = [] ++ hostConfigs.host0; }; host1 = nixpkgs.lib.nixosSystem { system = \u0026#34;x86_64-linux\u0026#34;; modules = [] ++ hostConfigs.host1; }; }; }; } Adding a Development Shell into the Mix A very nice feature, when it comes to flakes is the development shell, simply define a shell, then type nix develop (or even let direnv do all the work) and you end up with a very nice working environment.\nLet\u0026rsquo;s add a development shell to our flake. In order to do that, we can add a devShell section to our outputs. There is a nice article about how to do this here. I decided to keep the shell definition in a separate shell.nix file. Add the following to flake.nix:\n{ description = \u0026#34;Oblivious Infrastructure\u0026#34;; inputs = { ... }; outputs = {self, nixpkgs, nix, ... }: let pkgs = nixpkgs.legacyPackages.x86_64-linux; hostConfigs = { ... }; in { devShell.x86_64-linux = import ./shell.nix {inherit pkgs;}; colmena = { ... }; nixosConfigurations = { ... }; }); } Next create a shell.nix file inside the flake repo:\n{ pkgs ? import \u0026lt;nixpkgs\u0026gt; {} }: with pkgs; mkShell { buildInputs = [ colmena ]; shellHook = \u0026#39;\u0026#39; export PS1=\u0026#39;\\n\\[\\033[1;34m\\][oblivious]\\$\\[\\033[0m\\] \u0026#39; echo \u0026#34;\u0026lt;some ASCII art\u0026gt;\u0026#34; | base64 -d sync () { # local state # Update all the Inputs: for input in $(cat flake.nix | awk \u0026#39;/inputs = {/,/};/\u0026#39; | grep .url | cut -d \u0026#39;.\u0026#39; -f 1 | tr -d \u0026#39; \u0026#39;) do nix flake lock --update-input $input done if [[ `git status --porcelain` ]] then cm=\u0026#34;.\u0026#34; read -p \u0026#34;Commit message: \u0026#34; commit_message if ! [[ \u0026#34;$commit_message\u0026#34; == \u0026#34;\u0026#34; ]] then cm=\u0026#34;$commit_message\u0026#34; else cm=\u0026#34;.\u0026#34; fi git add . git commit -m \u0026#34;$cm\u0026#34; git push fi # get the channel we are following in our flake nixpkgsVersion=$(cat flake.nix | grep nixpkgs.url | cut -d \u0026#39;\u0026#34;\u0026#39; -f 2 | cut -d \u0026#39;/\u0026#39; -f 3) # remote state echo \u0026#39;[INFO ] pulling repo on remote hosts\u0026#39; colmena exec -v -- \u0026#39;cd /etc/nixos \u0026amp;\u0026amp; old_rev=$(git rev-parse --short HEAD) \u0026amp;\u0026amp; git pull | grep \u0026#34;Already up to date\u0026#34; || echo \u0026#34;$old_rev -\u0026gt; $(git rev-parse --short HEAD)\u0026#34;\u0026#39; echo \u0026#39;[INFO ] updating nix channel on remote hosts\u0026#39; colmena exec -v -- \u0026#34;nix-channel --add https://nixos.org/channels/$nixpkgsVersion nixos \u0026amp;\u0026amp; nix-channel --update\u0026#34; # check channelSyncSuccessful=$(echo $( colmena exec -v -- \u0026#39;echo nixos channel revision:$(cat /nix/var/nix/profiles/per-user/root/channels/nixos/.git-revision)\u0026#39; 2\u0026gt;\u0026amp;1 | grep \u0026#39;nixos channel revision\u0026#39; | cut -d \u0026#39;:\u0026#39; -f 2 cat flake.lock | jq \u0026#34;.nodes.$(cat flake.lock | jq \u0026#39;.nodes.root.inputs.nixpkgs\u0026#39;).locked.rev\u0026#34; | tr -d \u0026#39;\u0026#34;\u0026#39; ) | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | uniq | wc -l ) if ! [[ \u0026#34;$channelSyncSuccessful\u0026#34; == \u0026#34;1\u0026#34; ]] then echo \u0026#34;[WARNING!] - Channels on local repo and remote nodes not synced!\u0026#34; echo \u0026#34;[WARNING!] - If you\u0026#39;re trying to e.g. manually start nixos-containers YMMV\u0026#34; # exit 1 fi } alias build=\u0026#34;sync \u0026amp;\u0026amp; colmena build\u0026#34; alias deploy=\u0026#34;sync \u0026amp;\u0026amp; colmena apply\u0026#34; \u0026#39;\u0026#39;; } As you can see I added in a sync function, which makes sure the /etc/nixos/ directory always reflects the most recent git commit on all nodes and also updates the nixos-channels on the remote nodes 4.\nI also added two little aliases build and deploy to make life a bit more easy.\nAs a sidenote colmena doesn\u0026rsquo;t seem to like it, when nodes with a set targetHost are not reachable, a viable solution to this would be to extract the machine addresses from e.g. the hostConfigs section of the flake, check for their availability and if not all hosts are reachable use colmenas --on option to make sure at least the nodes available can be updated without too much of a fuzz.\nAt this point we have a wonderful little devshell, we can jump into from basically anywhere in order to manage our systems:\n[user@host:~/git/infrastructure]$ nix develop _ _ _ _ | | | (_) (_) ___ | |__ | |___ ___ ___ _ _ ___ / _ \\| \u0026#39;_ \\| | \\ \\ / / |/ _ \\| | | / __| | (_) | |_) | | |\\ V /| | (_) | |_| \\__ \\ \\___/|_.__/|_|_| \\_/ |_|\\___/ \\__,_|___/ _ __ _ _ (_) / _| | | | | _ _ __ | |_ _ __ __ _ ___| |_ _ __ _ _ ___| |_ _ _ _ __ ___ | | \u0026#39;_ \\| _| \u0026#39;__/ _` / __| __| \u0026#39;__| | | |/ __| __| | | | \u0026#39;__/ _ \\ | | | | | | | | | (_| \\__ \\ |_| | | |_| | (__| |_| |_| | | | __/ |_|_| |_|_| |_| \\__,_|___/\\__|_| \\__,_|\\___|\\__|\\__,_|_| \\___| [oblivious]$ sync warning: updating lock file \u0026#39;/home/user/git/infrastructure/flake.lock\u0026#39;: • Updated input \u0026#39;nixpkgs\u0026#39;: \u0026#39;github:NixOS/nixpkgs/fcc147b1e9358a8386b2c4368bd928e1f63a7df2\u0026#39; (2023-07-13) → \u0026#39;github:NixOS/nixpkgs/fa793b06f56896b7d1909e4b69977c7bf842b2f0\u0026#39; (2023-07-20) warning: Git tree \u0026#39;/home/user/git/infrastructure\u0026#39; is dirty warning: Git tree \u0026#39;/home/user/git/infrastructure\u0026#39; is dirty warning: Git tree \u0026#39;/home/user/git/infrastructure\u0026#39; is dirty warning: Git tree \u0026#39;/home/user/git/infrastructure\u0026#39; is dirty Commit message: [master 204c4ba] . 1 file changed, 3 insertions(+), 3 deletions(-) Enumerating objects: 5, done. Counting objects: 100% (5/5), done. Delta compression using up to 12 threads Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 368 bytes | 368.00 KiB/s, done. Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github... 4733c2c..204c4ba master -\u0026gt; master [INFO ] pulling repo on remote hosts [INFO ] Using flake: git+file:///home/user/git/infrastructure [INFO ] Enumerating nodes... [INFO ] Selected all 2 nodes. host0 | host1 | host1 | From github... host1 | 4733c2c..204c4ba master -\u0026gt; origin/master host0 | From github... host0 | 4733c2c..204c4ba master -\u0026gt; origin/master host1 | 4733c2c -\u0026gt; 204c4ba host1 | Succeeded host0 | 4733c2c -\u0026gt; 204c4ba host0 | Succeeded | All done! [INFO ] updating nix channel on remote hosts [INFO ] Using flake: git+file:///home/user/git/infrastructure [INFO ] Enumerating nodes... [INFO ] Selected all 2 nodes. host1 | host0 | host0 | unpacking channels... host1 | unpacking channels... host0 | Succeeded host1 | Succeeded | All done! DIY Secret Management Next let\u0026rsquo;s take a quick look into how to manage secrets. With colmena a keyComand can be defined, so we can basically build our secret management with whatever tooling we want. For this little example, lets just use pass (I won\u0026rsquo;t go into detail here, how to set it up, there are enough posts on that on the internet).\nAs a quick recap, this is how our file structure currently looks like:\n[user@host:~/git/infrastructure] tree -CL 2 . ├── flake.nix # our flake file containing information about all hosts and their NixOS configs ├── flake.lock # the flake lock file (autogenerated) ├── shell.nix # our development shell ├── hosts # contains hosts configuration.nix files │ ├── host0 │ ├── host1 ... │ └── hostN ├── modules # modular configuration bits and pieces │ ├── baseUtils ... │ └── zfs ├── pkgs # packages that aren\u0026#39;t part of nixpkgs ├── services # services that are hosted on the hosts │ ├── host2 │ │ └── nextloud │ └── host3 ... │ └── navidrome ├── users # user configurations └── vpn # vpn configurations First we\u0026rsquo;ll use pass to create a secret, in this case we\u0026rsquo;ll use the following and incredibly misterious string:\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 Add it to your password store using something along the lines of:\n$ pass edit colmena-test-secret Next create a module which deploys this secret, e.g. ./modules/colmena-test-secret/default.nix:\n{ deployment.keys.\u0026#34;colmena-test-secret\u0026#34; = { keyCommand = [ \u0026#34;pass\u0026#34; \u0026#34;colmena-test-secret\u0026#34; ]; }; } Then add it to the configuration of on of our hosts configuration.nix:\n{ config, pkgs, ... }: { imports = [ ./base.nix ../../modules/colmena-test-secrets ]; } And finally run deploy from the development shell5. After finishing, check if the secret has been properly put on the host, or if our little deployment tool has let us down:\n$ sudo cat /run/keys/colmena-testing-secret | base64 -d ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠂⣀⣈⣒⣤⣤⠤⠤⢤⣀⣀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠄⠠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡠⠀⠀⡔⣿⣿⣿⣿⣿⣿⣿⣦⣄⠈⠑⠒⠀⠂⠀⠀⠀⠀⠈⠒⠒⠊⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⡅⠹⡄⢀⢤⡼⣹⣿⣿⣿⣿⣿⣿⣿⣿⣿⡦⠴⣶⠲⠀⠀⠀⢀⡤⡄⠀⠒⠀⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠰⠂⠑⠒⠒⠓⠃⠘⢾⣿⣾⡇⠀⠀⠉⠁⠈⠉⢻⣿⡇⠠⠄⠤⣀⠀⠀⠀⠻⢅⣀⣈⣒⠀⠀⠀⠀⠀⠀⠀⠲⡄⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡰⠃⢠⠴⠦⠦⢦⣘⢻⣿⣷⣦⣤⠠⣄⣀⡀⢸⣿⣧⡀⠀⠸⣻⡇⠀⠠⣦⡀⠉⠙⢤⣀⣀⠀⠀⠀⠀⠀⠀⠃⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⢀⠀⠀⠀⠀⠀⡠⠒⡇⠂⢲⠒⠒⣼⣮⣿⠉⠉⠙⠀⠻⠛⠛⢺⡏⠬⡭⣵⣀⠈⠁⠀⠀⡿⣟⣆⠀⠀⠀⠀⠣⣄⡀⠀⠢⡀⠀⠀⠀⠀ ⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⠀⠞⣀⠢⠀⠀⠀⠀⢑⡤⠄⠠⠬⢏⣉⣹⣿⡗⠀⠀⡿⡒⠀⠀⠀⢿⡗⠒⠒⢺⠊⡵⠀⠀⠀⡿⣭⠏⡄⠀⠁⢀⠀⠀⠓⢤⡀⠈⡆⠀⡀⠀ ⠀⠀⠄⠀⠀⢹⠀⠀⠀⠀⢀⡰⠛⠷⡀⠀⠀⠀⢗⠒⠒⠒⠚⠂⠤⠼⢤⣧⡈⠙⠯⠽⠲⠀⡖⠻⠀⢀⣉⣹⣉⣧⠀⠀⠀⠉⣵⢆⠑⢆⠀⢀⡨⠆⣀⣀⠀⠉⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠉⠈⠁⠢⢄⠏⠀⠒⠒⡇⠀⠀⠴⡈⠑⣎⣀⣁⣀⡴⠊⢁⡟⠷⢄⡀⠀⠤⢣⣤⡐⠒⠒⠦⢼⠖⠋⡀⠀⠀⣶⣟⣊⣢⡈⠒⣁⠀⢸⣿⡿⠁⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⣀⣸⠀⠀⠀⠀⠇⠀⠀⢀⡨⠆⠈⠉⠒⠚⢠⣤⢿⣇⠀⠀⠀⠀⢀⠞⣿⣿⣟⣤⣖⣊⠠⠞⠃⠀⠀⢻⠤⡧⠤⢣⣊⠁⠁⡀⠉⣀⠄⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⡠⠀⠈⣏⣹⠤⣆⡤⠀⠀⠀⠀⠀⠈⢀⡔⣫⠭⢹⣿⢯⡿⣾⣿⣤⡄⣀⠴⠋⢠⣿⣿⣿⣷⣦⣭⣭⣖⣺⣶⣤⡄⠒⠓⢺⠈⠠⣀⢠⠟⠀⠋⠁⠀⠒⠂⠀⠀ ⠀⠀⠀⢰⠀⠀⠠⡈⣬⢵⠟⣘⠲⠶⠀⠀⠀⣴⠟⣉⣴⣾⣿⣿⣸⢷⢸⠁⠹⡟⠃⠀⣠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⡀⢀⡿⣈⠲⣤⠏⡴⠦⡀⡖⠒⠲⠄⠀⠀ ⠀⠀⠀⠠⠤⠂⠊⠁⠈⠉⣎⣈⠵⢄⠀⠀⠀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣀⣇⣟⣓⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣇⠀⢠⠑⠇⠏⢴⡱⠒⣇⠘⢄⠀⠀⠀⠀ ⠀⠂⠀⣠⠂⠀⠀⠀⠀⠸⡀⢰⡀⢸⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠟⣿⣒⡺⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠆⠀⠧⠀⠀⠀⠉⠑⠛⠀⢧⠀⠀⠀⠀ ⢀⠤⠾⠷⣀⣀⠇⡴⣆⠀⠳⡀⠀⢹⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣛⣀⡿⠶⠭⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⠀⠀⠀⢀⠀⣟⠯⢦⡀⠈⠓⢤⡀⠀ ⡎⠓⠒⠂⠤⡞⢠⠓⠺⡀⠀⢧⠀⡘⠀⠀⠰⣿⣿⣿⣿⣿⣿⣿⣿⣿⡾⣍⡯⠭⣽⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⠀⠈⣆⠳⢬⠒⠚⠒⡄⠀⠀⠀ ⠘⠢⡀⠒⢒⠁⡎⢀⡰⠃⠤⡈⡤⢿⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⢭⣿⣟⣓⣿⣿⣿⣿⣿⣏⠩⠉⠉⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣦⣘⠈⠀⢇⣩⣉⣹⠂⠀⠀ ⢀⠐⢍⠁⠘⢰⢉⡽⢀⡀⢍⢳⠸⡷⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣛⣿⣒⣾⣿⣿⣿⣿⣿⠷⡀⠂⠀⠀⠹⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⠈⢉⢀⣖⠁⠀⡆⠀ ⠈⠱⠀⠀⠀⠛⠉⡴⠛⠘⠢⡉⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⣿⠭⢽⣿⣿⣿⣿⣿⣿⣭⣤⡄⠀⠀⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⣀⠔⢋⡤⡄⠐⠂ ⠀⠀⠀⠀⠀⡠⠚⠀⠀⠩⡍⠓⣄⠀⠀⠀⢸⣿⣿⣿⣿⠿⠿⠛⣛⣻⡿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣶⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⢀⡧⠀⠈⢢⠀ ⠀⠀⢰⠀⠀⢏⠉⠉⠉⠀⠉⠱⡼⠀⠀⠀⠰⣿⣿⣯⣤⡎⠭⠤⠜⣿⣿⣿⣓⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠿⢥⠀⠰⣿⣄⣀⠀⠀⠀ ⠀⠀⠸⠀⠀⡀⠉⡀⠀⠀⡰⠉⠀⠀⠀⠀⠀⠸⣿⣿⣿⡃⠒⣒⣽⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣻⣿⣿⣿⣿⣿⣿⡇⠀⠉⠉⠡⠄⠀⠀⠠⠜⠀⠀⠇⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⢰NEVER GONNA GIVE YOU UP⣿⣿⣿⣿⣿NEVER GONNA LET YOU DOWN⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠸⠀⠈⢳⠀⠔⠉⠀⣸⠀⠀⠀⠀⠀⠙⢢⠀⠌⣽⣿⣿⡿⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠊⠀⠀⠄⠀⠀⠀⠀⠀⠀⠀⣀⠌⠀⢠⢿⣿⣿⡇⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ I\u0026rsquo;d say that looks about right.\nColmena also supports other options than keyCommand, such as string or keyFile, but i kind of like the idea of not having all my secrets lying around unencrypted. Also there are a bunch of other options on how to configure secrets documented here.\nSetting some Boundaries While my relationship with colmena can so far be described as incredibly time saving and quite a bit of fun, there were a couple of things I ran into that should be considered. When going down the path of automated deployment you can actually run into a bit of trouble in paradise: the more time you spend with colmena, the more it will over time - and quite sneakily - steal away your storage space.\nThere are two main issues I ran into kind of frequently with colmena, that I wasn\u0026rsquo;t used to when (not regularly) updating my hosts manually:\nfirstly, the size of the nix store simply explodes secondly, the amount of boot configurations gets so big, the /boot partition runs out of space The size of the nix store can be easily constrained by creating a module for the nix garbage collector (./modules/nix-settings/default.nix):\n{ config, lib, pkgs, ... }: { nix = { gc = { automatic = true; dates = \u0026#34;weekly\u0026#34;; options = \u0026#34;--delete-older-than 14d\u0026#34;; }; extraOptions = \u0026#39;\u0026#39; min-free = ${toString (100 * 1024 * 1024)} max-free = ${toString (1024 * 1024 * 1024)} \u0026#39;\u0026#39;; autoOptimiseStore = true; # \u0026lt;- this option will hardlink identical files }; } Also we can limit the size of the systemd journal, by creating ./modules/limit-journal-size/default.nix:\n{ config, lib, pkgs, ... }: { services.journald.extraConfig = \u0026#39;\u0026#39; SystemMaxUse=100M MaxFileSec=7day \u0026#39;\u0026#39;; } And then referencing them inside our ./modules/colmena/default.nix:\n{ imports = [ ../nix-settings ../limit-journal-size ]; # https://github.com/NixOS/nix/issues/2330#issuecomment-451650296 nix.settings.trusted-users = [ \u0026#34;root\u0026#34; \u0026#34;@wheel\u0026#34; ]; users.users.root.openssh.authorizedKeys.keys = [ \u0026#34;ssh-ed25519 ...\u0026#34; \u0026#34;ssh-rsa ...\u0026#34; ]; } The second problem can be solved by creating a module, which constrains the max amount of available boot configurations. Create ./modules/bootConfigurationLimit/grub/default.nix:\n{ config, pkgs, ... }: { boot.loader.grub.configurationLimit = 16; } Respectively ./modules/bootConfigurationLimit/systemdboot/default.nix:\n{ config, pkgs, ... }: { boot.loader.systemd-boot.configurationLimit = 16; } And then import whichever applies to your hosts in their ./hosts/\u0026lt;hostname\u0026gt;/default.nix files.\nAfter rebuilding and deploying you should be somewhat save from colmenas nature of taking up your personal space.\nAt this point in time you should be able to manage and rollout your infrastructure fairly easily from a single flake. From here on setting up new services over multiple hosts gets to be way more fun.\nThere is maybe one last thing I should mention: sometimes colmena will fail to deploy properly on some of your hosts. This actually happens rather frequently, however most of the times this happens, a simple rerun of the deployment step will sucessfully activate the new configuration and sometimes, depending on what you do, it helps turning things off and on again.\nthere is an option called deployment.keys, but that is for secret management\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNote, using the -v flag returns the output of executed commands, whereas not using it simply returns success or failure with colmena.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is of course not strictly necessary, however I feel kind of comfortable to know there is always a description of what is running on a node, which you can use to look up what is going on\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWhen playing around with containers I noticed they apparently followed the channels, so it was possible to deploy a 22.05 container, whereas a container created on the remote host would still be on 21.11. Not sure how much of the sync function still applies, but it works for me (TM)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWhen using colmena, I\u0026rsquo;ve found, that inputing the password during an apply operation does not really seem to work nicely, so as a workaround I\u0026rsquo;ve created an empty entry in my password store called dummy, which I can use to unlock it before deploying by simply executing pass dummy before the deploy command.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://oblivious.observer/posts/nixos-multi-host-deployment-with-flakes-and-colmena/","summary":"In the last post, I wrote about how to convert a \u0026rsquo;normal\u0026rsquo; NixOS system to one that is managed by a flake. This one will build on top of, or rather scale out of managing single hosts and dive into how to do remote management and deployment for multiple systems.\nIntroduction Before taking a peek into Deployments, lets set a bit of a frame of reference here first:\nWhen talking about deployments a lot of people think about quite complex setups and the use of services such as autoscaling, instance creation etc.","title":"NixOS Deployment with Colmena"},{"content":"I\u0026rsquo;ve been meaning to write something for a while now, however before I can dive into a couple of more interesting topics here, I feel like it is the best to mention the transition to flakes. I\u0026rsquo;ll try and keep this one short, there are a bunch of other way more detailed posts on flakes already, however I didn\u0026rsquo;t find to many posts about how to switch from an existing configuration to using flakes and it seems to be quite doable.\nOn a freshly installed NixOS system, /etc/nixos looks like this:\n[user@host1:/etc/nixos]$ tree -CL 1 . ├── configuration.nix └── hardware-configuration.nix If you\u0026rsquo;ve played around a bunch, have more than one machine and have at some point in time started using imports, it\u0026rsquo;ll probably look more like this:\n[user@host1:/etc/nixos]$ tree -CL 1 . ├── configuration.nix -\u0026gt; hosts/host1/configuration.nix # symlinked nixos configuration (.gitignored) ├── hardware # hardware specific config ├── hosts # contains hosts configuration.nix files │ ├── host0 │ ├── host1 ... │ └── hostN ├── modules # modular configuration bits and pieces │ ├── baseUtils ... │ └── zfs ├── pkgs # packages that aren\u0026#39;t part of nixpkgs ├── services # services that are hosted on the hosts │ ├── host2 │ │ └── nextloud │ └── host3 │ ├── gitea │ ├── calendar │ └── navidrome ├── users # user configurations └── vpn # vpn configurations In my case I\u0026rsquo;ve created a bunch of directories containing the bits and pieces of configuration I use. Among them is a hosts directory, which contains all the host-specific files in host-specific directories and I symlink the configuration.nix from there into /etx/nixos/, so I can conveniently execute nixos-rebuild switch. The whole /etc/nixos directory is a git repo I share among all of my machines and the .gitignore file currently contains a single line:\n[user@host1:/etc/nixos]$ cat .gitignore /configuration.nix A typical /etc/nixos/configuration.nix then looks like this, note the paths are relative from the /etc/nixos/hosts/hostname/ directory:\n{ config, pkgs, ... }: { imports = [ ../../modules/baseUtils ../../services/host2/nextcloud ../../users ./hardware-configuration.nix ./vpn/tinc/networkname ]; boot.loader.grub.enable = true; boot.loader.grub.version = 2; boot.initrd.availableKernelModules = [ \u0026#34;e1000e\u0026#34; \u0026#34;virtio_pci\u0026#34; \u0026#34;e1000\u0026#34; ]; ... } Before switching to flakes I decided to do a bit of housekeeping inside the /etc/nixos/hosts/hostname direcories. While this is strictly unnecessary, I kind of liked the idea:\nI renamed /etc/nixos/hosts/host/configuration.nix to default.nix and created a new configuration.nix, which includes default.nix:\n[user@host1:/etc/nixos/hosts/host0]$ tree -CL 1 . ├── configuration.nix ├── default.nix └── hardware.nix Now I can just split my system configurations up into the more host-specific part, which I very rarely have to look into, containing options such as boot.loader.grub and another much shorter part that contains, what the system is actually configured for in form of module imports. The new configuration looks so far pretty empty, since we\u0026rsquo;ve renamed the configuration to default.nix, we can just reference it using ./.:\n{ config, pkgs, ... }: { imports = [ ./. # \u0026lt;- isn\u0026#39;t this just beautiful ]; } But back to topic: in order to start using flakes, we need be enable flakes on the system, so create ./modules/flakes/default.nix and import it in default.nix:\n{ lib, pkgs, config, ... }: { nix = { package = pkgs.nixFlakes; extraOptions = \u0026#39;\u0026#39; experimental-features = nix-command flakes \u0026#39;\u0026#39;; }; } Rebuild the system to enable the feature.\nThen in /etc/nixos create a minimal flake.nix file, which contains a link to the hosts configuration.nix inside nixosConfigurations:\n{ description = \u0026#34;Oblivious Infrastructure\u0026#34;; inputs = { flake-utils.url = \u0026#34;github:numtide/flake-utils\u0026#34;; nix.url = \u0026#34;github:NixOS/nix/2.5.1\u0026#34;; nixpkgs.url = \u0026#34;github:NixOS/nixpkgs/nixos-21.11\u0026#34;; nixos-hardware.url = \u0026#34;github:NixOS/nixos-hardware\u0026#34;; }; outputs = { self, nixpkgs, nix, ... }: { nixosConfigurations = { host0 = nixpkgs.lib.nixosSystem { system = \u0026#34;x86_64-linux\u0026#34;; modules = [ ./hosts/host0/configuration.nix ]; }; host1 = nixpkgs.lib.nixosSystem { system = \u0026#34;x86_64-linux\u0026#34;; modules = [ ./hosts/host1/configuration.nix ]; }; }; }; } In order to check if everything works just run nixos-rebuild switch --flake .#host from /etc/nixos/ or nixos-rebuild switch --flake /etc/nixos/#host from anywhere else.\nCongratulations, your system now runs on flakes.\n","permalink":"https://oblivious.observer/posts/nixos-configuration-using-flakes/","summary":"I\u0026rsquo;ve been meaning to write something for a while now, however before I can dive into a couple of more interesting topics here, I feel like it is the best to mention the transition to flakes. I\u0026rsquo;ll try and keep this one short, there are a bunch of other way more detailed posts on flakes already, however I didn\u0026rsquo;t find to many posts about how to switch from an existing configuration to using flakes and it seems to be quite doable.","title":"The Flake Awakens - Switching to a Flake-based Configuration"},{"content":"This is the second part of my little series about tinc and NixOS, where I first shortly introduced tinc and then explained how to set it up on Linux in general as well as on NixOS. This part is more or less a rewrite of a question I posted to the NixOS discourse a while ago.\nFor a while I\u0026rsquo;ve been using tinc on a bunch of infrastructure and so far it\u0026rsquo;s basically been rock solid: once set up nodes anywhere simply join their network and become reachable for anyone who needs to access them. The one downside I noticed was the fact that at some point maintaining and updating the node configurations became increasingly cumbersome and at some point I ended up with my laptop connected to two different VPN networks (or tunnels - hence the title) consisting of all in all almost 30 nodes.\nAs a result of tinc providing its so far excellent service, the powers that be handed me the exiting quest of scaling up the existing networks as well as adding a whole bunch of new networks to the growing mass of configuration I already had lying around. This is the tale of how I battled a bunch of long and confusing configuration files and wrote cargo culted my first NixOS module in the process, enjoy..\nIntroduction and Recap While I\u0026rsquo;ve found that using NixOS usually results in a whole lot less (and all in all an usually different) kind of headaches than running infrastructure on any other Linux distribution, I quickly came to a point, where I wasn\u0026rsquo;t really happy with the way the tinc configurations looked. As a quick recap, here is an example configuration for a small tinc network:\n{ config, lib, pkgs, ... }: { networking.firewall.allowedTCPPorts = [ 655 ]; networking.firewall.allowedUDPPorts = [ 655 ]; networking.interfaces.\u0026#34;tinc.example\u0026#34;.ipv4.addresses = [ { address = \u0026#34;10.0.0.0\u0026#34;; prefixLength = 24; } ]; services.tinc.networks = { example = { name = \u0026#34;node0\u0026#34;; hosts = { node0 = \u0026#39;\u0026#39; Address = 192.168.122.1 Subnet = 10.0.0.0 Subnet = 192.168.0.0/24 Ed25519PublicKey = ... -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- \u0026#39;\u0026#39;; node1 = \u0026#39;\u0026#39; Subnet = 10.0.0.1 Port = 655 Ed25519PublicKey = ... -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- \u0026#39;\u0026#39;; node2 = \u0026#39;\u0026#39; Address = 192.168.122.3 Subnet = 10.0.0.2 Ed25519PublicKey = ... -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- \u0026#39;\u0026#39;; ... # more nodes }; }; }; } Initially a port (655) is opened on the firewall, then the network interface tinc expects for the network called example (tinc.example) is created and assigned an IP address. Then the name of the node is defined and finally the hosts is filled with some / all other nodes configuration information of the example network (as well as the one of the local node).\nUsing this way of setting up tinc networks for some time, I noticed two things:\nMy config files grew into an unmaintainable mess as the number of hosts inside the VPNs increased Since tinc is a mesh VPN it is a nice Idea to deploy the keys of all participating nodes to all participating nodes, but doing this inside the configuration means that increasing the number of nodes inside a network results in the need to change the configuration of all other nodes in the network (However at this point I should clarify: adding the node to any reachable other nodes configuration and rebuilding it is sufficient for a new node to join the network) While the first thing is just a bit confusing, it also feels like it is some sort of a future accident or outage waiting to happen and especially the latter one felt kind of important to set up, but was at the same time something my lazy self never really got around to do.\nbuiltins.readfile to the rescue Initially I looked into how to make configuration files shorter and at some point I stumbled over builtins.readFile, which you can use to read in configuration bits and pieces from files. Instead of having all the node configurations lying around somewhere inside my config, I could create a per node configuration file containing something like this:\nAddress = 192.168.122.1 Subnet = 10.0.0.1 Subnet = 192.168.0.0/24 Ed25519PublicKey = ... -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- Then using builtin.readfiles inside the configuration everything immediately becomes much more readable:\n{ config, lib, pkgs, ... }: { networking.firewall.allowedTCPPorts = [ 655 ]; networking.firewall.allowedUDPPorts = [ 655 ]; networking.interfaces.\u0026#34;tinc.example\u0026#34;.ipv4.addresses = [ { address = \u0026#34;10.0.0.1\u0026#34;; prefixLength = 24; } ]; services.tinc.networks = { example = { name = \u0026#34;node0\u0026#34;; hosts = { node0 = (builtin.readFile /path/to/node0-config); node1 = (builtin.readFile /path/to/node1-config); node2 = (builtin.readFile /path/to/node2-config); }; }; }; } Using let The next thing to do was to look at everything and try to make my code a bit more generic, so I initially started out using the let statement in order to make replacing a bunch of values inside a tinc configuration easier by at least spatially grouping them together:\n{ config, lib, pkgs, ... }: let node_name = \u0026#34;node0\u0026#34;; vpn_name = \u0026#34;example\u0026#34;; port = 655; ipv4_address = \u0026#34;10.0.0.1\u0026#34;; ipv4_prefix = 24; in { networking.firewall.allowedTCPPorts = [ port ]; networking.firewall.allowedUDPPorts = [ port ]; networking.interfaces.(\u0026#34;tinc.\u0026#34; + vpn_name).ipv4.addresses = [ { address = ipv4_address; prefixLength = ipv4_prefix; } ]; services.tinc.networks = { example = { name = \u0026#34;node0\u0026#34;; hosts = { node0 = (builtin.readFile /path/to/node0-config); node1 = (builtin.readFile /path/to/node1-config); node2 = (builtin.readFile /path/to/node2-config); }; }; }; } Great! At this point a new network can be set up by easily copying the above code from another network configuration, then changing 5 lines at the top and finally explicitly defining each node inside the network inside hosts.\nReading all files from a directory At this point I decided to switch from defining every single host inside the configuration to simply adding the hosts configurations from a directory.\nI basically started a nix repl and begun writing some code until everything looked like it could work inside my configuration, then I pasted whatever I had into a config file and hoped nixos-rebuild switch wouldn\u0026rsquo;t throw any errors. This is probably not the best way to develop nix files, but having the repl to quickly try out a bunch of nix lines was a definite step up from just working inside the configuration files.\nThe only thing that was a bit frustrating was the fact that nix is really lazy and quickly stops evaluating things, so you frequently end up with something like this:\n$ nix repl Welcome to Nix version 2.3.10. Type :? for help. nix-repl\u0026gt; a = [ 1 2 3 4 ] nix-repl\u0026gt; b = [ 5 6 7 8 ] nix-repl\u0026gt; c = [ a b ] nix-repl\u0026gt; c [ [ ... ] [ ... ] ] On the other hand it\u0026rsquo;s really great to have things like :t in order to find out what type a value has:\nnix-repl\u0026gt; :t c a list nix-repl\u0026gt; :t 1 an integer nix-repl\u0026gt; :t \u0026#34;definitely not a string :)\u0026#34; a string nix-repl\u0026gt; :t {} a set Initially I wanted to read in every file from some directory and then fill up services.tinc.network.\u0026lt;name\u0026gt;.hosts with key-value pairs created from the files name and content. Later on however I noticed that hosts is an attrset and due to that discarded the idea. Instead I simply created a filled attrset that I then supplied to hosts. In order to do that, I switched from keeping the nodes config in regular files to using JSON:\n$ tree /etc/nixos/vpn/tinc/example /etc/nixos/vpn/tinc/example ├── node0.json ├── node1.json └── node2.json $ bat /etc/nixos/vpn/tinc/example/node0.json ───────┬─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── │ File: /etc/nixos/vpn/tinc/example/node0.json ───────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 1 │ { 2 │ \u0026#34;name\u0026#34; : \u0026#34;node0\u0026#34;, 3 │ \u0026#34;value\u0026#34; : \u0026#34;Subnet = 10.0.0.0\\nPort = 655\\nEd25519PublicKey = ...\\n-----BEGIN RSA PUBLIC KEY-----\\n...\\n-----END RSA PUBLIC KEY-----\u0026#34; 4 │ } ───────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── After a bit of trial and error, as well the revelation of nix not having loops, I arrived at the following code:\n{ config, lib, pkgs, ... }: let node_name = \u0026#34;node0\u0026#34;; vpn_name = \u0026#34;example\u0026#34;; port = 655; ipv4_address = \u0026#34;10.0.0.0\u0026#34;; ipv4_prefix = 24; in { networking.firewall.allowedTCPPorts = [ port ]; networking.firewall.allowedUDPPorts = [ port ]; networking.interfaces.(\u0026#34;tinc.\u0026#34; + vpn_name).ipv4.addresses = [ { address = ipv4_address; prefixLength = ipv4_prefix; } ]; services.tinc.networks = { mcrn0 = { name = node_name; hosts = let files = builtins.readDir (\u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + vpn_name); filenames = builtins.attrNames files; filepaths = map (x: \u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + vpn_name + \u0026#34;/\u0026#34; + x) filenames; filecontents = map builtins.readFile filepaths; jsondata = map (x: builtins.fromJSON x) filecontents; attrsetdata = builtins.listToAttrs jsondata; in attrsetdata; }; }; } Now a tinc network can be created by changing 5 lines and supplying a folder that contains the node configurations. This folder can also be shared between all nodes and adding a new node only requires a rebuild on at least one (better of course on every) other node of the network.\nManaging multiple networks however still requires copying the file and creates duplicates of configuration code, which is another future accident waiting to happen.\nCreating a Module At this point I decided to look into Modules and decided to try and write my own. Initially I searched for some kind of tutorial on how to create a module in NixOS (especially writing one that is using the \u0026lt;name\u0026gt; feature was quite a bit confusing to be honest), but I didn\u0026rsquo;t really find anything, so instead I decided to take a look at some existing modules and basically ended up copying and changing the original tinc module. Initially I simply ripped out pretty much everything and set up some options I could use to verify if everything worked, namely I created a new network device with an IP address as well as an opened a port on the firewall. I started basically setting up the following options, but in a generic way:\nnetworking.firewall.allowedTCPPorts = [ port ]; networking.firewall.allowedUDPPorts = [ port ]; networking.interfaces.(\u0026#34;tinc.\u0026#34; + vpn_name).ipv4.addresses = [ { address = ipv4_address; prefixLength = ipv4_prefix; } ]; Here is the code for this initial \u0026ldquo;dumb\u0026rdquo; version of the module:\n{ config, lib, pkgs, ... }: with lib; let cfg = config.services.tincDifferent; in { options = { services.tincDifferent = { networks = mkOption { default = { }; type = with types; attrsOf (submodule { options = { nodeName = mkOption { default = null; type = types.nullOr types.str; description = \u0026#39;\u0026#39; Name of the Node in the tinc network. \u0026#39;\u0026#39;; }; port = mkOption { default = 655; type = types.int; description = \u0026#39;\u0026#39; TCP port of the tinc network. \u0026#39;\u0026#39;; }; ipv4Address = mkOption { default = null; type = types.nullOr types.str; description = \u0026#39;\u0026#39; IPv4 Address of the machine on the tinc network. \u0026#39;\u0026#39;; example = \u0026#34;10.0.0.1\u0026#34;; }; ipv4Prefix = mkOption { default = null; type = types.nullOr types.int; description = \u0026#39;\u0026#39; IPv4 Prefix of the machine on the tinc network. \u0026#39;\u0026#39;; example = 24; }; }; }); description = \u0026#39;\u0026#39; Defines the tinc networks which will be started. Each network invokes a different daemon. \u0026#39;\u0026#39;; }; }; }; config = { networking.firewall = fold (a: b: a // b) { } (flip mapAttrsToList cfg.networks (network: data: { allowedTCPPorts = [ data.port ]; allowedUDPPorts = [ data.port ]; } )); networking.interfaces = fold (a: b: a // b) { } (flip mapAttrsToList cfg.networks (network: data: { \u0026#34;tinc.${network}\u0026#34;.ipv4.addresses = [ { address = data.ipv4Address; prefixLength = data.ipv4Prefix; } ]; } )); }; } At this point I was able to check if everything worked by simply importing the module, filling out the options, then looking if there was a new network interface and checking for open ports using something like this:\n{ config, lib, pkgs, ... }: { imports = [ ./modules/tincDifferent.nix ]; services.tincDifferent.networks.example.nodeName = \u0026#34;node0\u0026#34;; services.tincDifferent.networks.example.ipv4Address = \u0026#34;10.0.0.1\u0026#34;; services.tincDifferent.networks.example.ipv4Prefix = 24; # Port 655 is a default value, but it could of course be defined like this: # services.tincDifferent.networks.example.port = 655; } Note that, the code above is using a nested (unnamed) option in order to create the generic *.\u0026lt;name\u0026gt;.* functionality. If I understand correctly this can generally be achieved by doing something like this in the options part of the module:\n... options = { services.exampleService = { exampleOption = mkOption { default = { }; type = with types; attrsOf (submodule { options = { enable = mkOption { default = false; type = types.bool; description = \u0026#39;\u0026#39; just an example \u0026#39;\u0026#39;; }; }; }); description = \u0026#39;\u0026#39; \u0026lt;name\u0026gt; will be available under exampleOption.. \u0026#39;\u0026#39;; }; }; }; ... Then the enable option can be set for several exampleService services using e.g.:\nservices.exampleService.exampleOption.first.enable = true; services.exampleService.exampleOption.second.enable = true; services.exampleService.exampleOption.third.enable = false; Later on in the config part of the module, the \u0026lt;name\u0026gt; under exampleOption has to be retrieved somehow. This is also the point, where I asked for help on the NixOS discourse and while I still haven\u0026rsquo;t totally wrapped my head around, it is possible to do so by accessing cfg.exampleOption (in our example) and then using something like this to apply the value somehow:\nservices = fold (a: b: a // b) { } (flip mapAttrsToList cfg.exampleOption (serviceName: nestedOptions: serviceName = nestedOptions.enable; )); # or services = builtins.mapAttrs (serviceName: nestedOptions: { serviceName = nestedOptions.enable; }) cfg.networks; All in all I finally ended up with this code for the Module:\n{ config, lib, pkgs, ... }: with lib; let cfg = config.services.tincDifferent; in { options = { services.tincDifferent = { networks = mkOption { default = { }; type = with types; attrsOf (submodule { options = { nodeName = mkOption { default = null; type = types.nullOr types.str; description = \u0026#39;\u0026#39; Name of the Node in the tinc network. \u0026#39;\u0026#39;; }; port = mkOption { default = 655; type = types.int; description = \u0026#39;\u0026#39; TCP / UDP port used byt the tinc network (The Port has to be supplied in the node configuration as well, since the original tinc module takes the Port from there). \u0026#39;\u0026#39;; }; ipv4Address = mkOption { default = null; type = types.nullOr types.str; description = \u0026#39;\u0026#39; IPv4 Address of the machine on the tinc network. \u0026#39;\u0026#39;; example = \u0026#34;10.0.0.1\u0026#34;; }; ipv4Prefix = mkOption { default = null; type = types.nullOr types.int; description = \u0026#39;\u0026#39; IPv4 Prefix of the machine on the tinc network. \u0026#39;\u0026#39;; example = 24; }; }; }); description = \u0026#39;\u0026#39; Defines the tinc networks which will be started. Each network invokes a different daemon. \u0026#39;\u0026#39;; }; }; }; config = { networking.firewall = fold (a: b: a // b) { } (flip mapAttrsToList cfg.networks (network: data: { allowedTCPPorts = [ data.port ]; allowedUDPPorts = [ data.port ]; } )); services.tinc.networks = builtins.mapAttrs (network: data: { name = data.nodeName; hosts = let files = builtins.readDir (\u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + network); filenames = builtins.attrNames files; filepaths = map (x: \u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + network + \u0026#34;/\u0026#34; + x) filenames; filecontents = map builtins.readFile filepaths; jsondata = map (x: builtins.fromJSON x) filecontents; attrsetdata = builtins.listToAttrs jsondata; in attrsetdata; }) cfg.networks; networking.interfaces = fold (a: b: a // b) { } (flip mapAttrsToList cfg.networks (network: data: { \u0026#34;tinc.${network}\u0026#34;.ipv4.addresses = [ { address = data.ipv4Address; prefixLength = data.ipv4Prefix; } ]; } )); }; } Using this module, I can now create networks using only a few lines. The following would for example create two tinc networks, called 0 and 1, with their configuration files located in /etc/nixos/vpn/tinc/0 respectively /etc/nixos/vpn/tinc/1:\n{ config, lib, pkgs, ... }: { imports = [ ./modules/tincDifferent.nix ]; services.tincDifferent.networks.0.nodeName = \u0026#34;node0\u0026#34;; services.tincDifferent.networks.0.ipv4Address = \u0026#34;10.0.0.1\u0026#34;; services.tincDifferent.networks.0.ipv4Prefix = 24; services.tincDifferent.networks.0.port = 655; services.tincDifferent.networks.1.nodeName = \u0026#34;node0\u0026#34;; services.tincDifferent.networks.1.ipv4Address = \u0026#34;10.0.1.2\u0026#34;; services.tincDifferent.networks.1.ipv4Prefix = 24; services.tincDifferent.networks.1.port = 656; } Improving the Module - Switching from JSON to nix At this point let\u0026rsquo;s take a minute and recap a little: Initially we started out with a normal tinc configuration on NixOS. Next we started using the builtins.readfile function in order to read in the config of the VPN nodes from files (instead of putting everything tinc-specific into the nix configuration). This resulted in a way shorter and more readable configuration.\nThen we switched from explicitly defining nodes to reading in all configuration files inside a directory. While this added a bunch of lines to the configuration, it also solves the problem of having to manually change the configs of every node whenever a node is added or removed from a network. The distribution itself is at this point still up to the user, but there are numerous ways to take care of that problem, e.g. git, git-annex or syncthing.\nFinally we created a Module, which enabled us to configure not only a single but multiple networks with only a few little lines of code.\nIf you remember my last post, then you may also remember that in the example from last time, one of the nodes shared a local subnet with the rest of the network.\nSharing the subnet of a node using the module from above would still require every node to set up the subnet explicitly, resulting in losing the benefit of not having to rewrite the configuration of every node on the network, so in order to keep that advantage, the information has to be shared together with the rest of a nodes configuration.\nThe JSON file our module is using contains a name-value pair, that is used by the builtins.listToAttrs function in order to construct the hosts attrset and as far as I understand it, due to this, the JSON file does not support any additional content. So I switched from using JSON to plain nix to configure a node:\n$ bat /etc/nixos/vpn/tinc/example/node0.json ───────┬─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── │ File: /etc/nixos/vpn/tinc/example/node0.json ───────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 1 │ { 2 │ \u0026#34;name\u0026#34; : \u0026#34;node0\u0026#34;, 3 │ \u0026#34;value\u0026#34; : \u0026#34;Subnet = 10.0.0.0\\nPort = 655\\nEd25519PublicKey = ...\\n-----BEGIN RSA PUBLIC KEY-----\\n...\\n-----END RSA PUBLIC KEY-----\u0026#34; 4 │ } ───────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── $ bat /etc/nixos/vpn/tinc/example-nix/node0.nix ───────┬───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── │ File: /etc/nixos/vpn/tinc/example-nix/node0.nix ───────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 1 │ { 2 │ tinc = { 3 │ name = \u0026#34;node0\u0026#34;; 4 │ config = \u0026#39;\u0026#39; 5 │ Subnet = 10.0.0.0 6 │ Port = 655 7 │ Ed25519PublicKey = ... 8 │ -----BEGIN RSA PUBLIC KEY----- │ ... 20 │ -----END RSA PUBLIC KEY----- 21 │ \u0026#39;\u0026#39;; 22 │ }; 23 │ } ───────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Now when using nix files, the import (not imports) function can be used in order to read in a nix file and sections of that file, such as e.g. tinc in the example above can be directly accessed:\n$ nix repl Welcome to Nix version 2.3.10. Type :? for help. nix-repl\u0026gt; import ./node0.nix { tinc = { ... }; } nix-repl\u0026gt; (import ./node0.nix).tinc { config = \u0026#34;Subnet = 10.0.0.0\\nPort = 655\\nEd25519PublicKey = ...\\n-----BEGIN RSA PUBLIC KEY-----\\n...\\n-----END RSA PUBLIC KEY-----\\n\u0026#34;; name = \u0026#34;node0\u0026#34;; } This makes it really easy to change the section of our module, which sets up the tinc nodes to accept nix files instead of JSON files:\n# from this: services.tinc.networks = builtins.mapAttrs (network: data: { name = data.nodeName; hosts = let files = builtins.readDir (\u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + network); filenames = builtins.attrNames files; filepaths = map (x: \u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + network + \u0026#34;/\u0026#34; + x) filenames; filecontents = map builtins.readFile filepaths; jsondata = map (x: builtins.fromJSON x) filecontents; attrsetdata = builtins.listToAttrs jsondata; in attrsetdata; }) cfg.networks; # to this: services.tinc.networks = builtins.mapAttrs (network: data: { name = data.nodeName; hosts = let files = map (x: \u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + network + \u0026#34;/\u0026#34; + x) (builtins.attrNames (builtins.readDir (\u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + network +\u0026#34;/\u0026#34;))); ## CHECK!! use builtins.readFile before import and check if we notice changes on rebuilds! attrsetdata = builtins.listToAttrs (map (x: lib.nameValuePair x.name x.config) (map (x: (import x).tinc) files)); in attrsetdata; }) cfg.networks; (Note the comment, I\u0026rsquo;m not really sure if NixOS picks up on changed files, when using import as opposed to =builtins.readFile, guess I\u0026rsquo;ll have to check that out at some point..)\nWe first create a list called files that contains the absolute path to every file inside our vpn directory, and then create the hosts attrset by importing the tinc section of these files.\nNow in order add in node-specific options such as sharing a subnet, we can simply add in additional subsections on a nodes configuration, e.g. something like this in order to support ipv4 routes:\n{ tinc = { name = \u0026#34;node0\u0026#34;; config = \u0026#39;\u0026#39; Address = 192.168.0.1 Subnet = 10.0.0.0 Subnet = 192.168.0.0/24 Ed25519PublicKey = ... -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- \u0026#39;\u0026#39;; }; routes = { ipv4 = [ { address = \u0026#34;192.168.0.0\u0026#34;; prefixLength = 24; via = \u0026#34;10.0.0.0\u0026#34;; } ]; ipv6 = []; }; } Note that also let can be used to make sure these files become somewhat copy-paste-able:\nlet cfg = { ipv4 = \u0026#34;10.0.0.0\u0026#34;; routes = { ipv4 = { subnet = \u0026#34;192.168.0.0\u0026#34;; prefix = 24; }; }; }; in { tinc = { name = \u0026#34;node0\u0026#34;; config = \u0026#39;\u0026#39; Address = 192.168.0.1 Subnet = ${cfg.ipv4} Subnet = ${cfg.routes.ipv4.subnet}/${cfg.routes.ipv4.subnet} Ed25519PublicKey = ... -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- \u0026#39;\u0026#39;; }; routes = { ipv4 = [ { address = \u0026#34;${cfg.routes.ipv4.subnet}\u0026#34;; prefixLength = ${cfg.routes.ipv4.prefix}; via = \u0026#34;${cfg.ipv4}\u0026#34;; } ]; ipv6 = []; }; } The new route section can then be added to the module using something like this:\n# from this: networking.interfaces = fold (a: b: a // b) { } (flip mapAttrsToList cfg.networks (network: data: { \u0026#34;tinc.${network}\u0026#34;.ipv4.addresses = [ { address = data.ipv4Address; prefixLength = data.ipv4Prefix; } ]; } )); # to this: networking.interfaces = fold (a: b: a // b) { } (flip mapAttrsToList cfg.networks (network: data: { \u0026#34;tinc.${network}\u0026#34;.ipv4 = { addresses = [ { address = data.ipv4Address; prefixLength = data.ipv4Prefix; } ]; routes = let files = map (x: \u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + network + \u0026#34;/\u0026#34; + x) (builtins.attrNames (builtins.readDir (\u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + network +\u0026#34;/\u0026#34;))); routes = builtins.concatLists (map (x: x.ipv4) (map (x: (import x).routes) files)); in routes; }; } )); At this point the whole module - for now only supporting additional IPv4 routes - looks like this:\n{ config, lib, pkgs, ... }: with lib; let cfg = config.services.tincDifferent; in { options = { services.tincDifferent = { networks = mkOption { default = { }; type = with types; attrsOf (submodule { options = { nodeName = mkOption { default = null; type = types.nullOr types.str; description = \u0026#39;\u0026#39; Name of the Node in the tinc network. \u0026#39;\u0026#39;; }; port = mkOption { default = 655; type = types.int; description = \u0026#39;\u0026#39; TCP / UDP port used byt the tinc network (The Port has to be supplied in the node configuration as well, since the original tinc module takes the Port from there). \u0026#39;\u0026#39;; }; ipv4Address = mkOption { default = null; type = types.nullOr types.str; description = \u0026#39;\u0026#39; IPv4 Address of the machine on the tinc network. \u0026#39;\u0026#39;; example = \u0026#34;10.0.0.1\u0026#34;; }; ipv4Prefix = mkOption { default = null; type = types.nullOr types.int; description = \u0026#39;\u0026#39; IPv4 Prefix of the machine on the tinc network. \u0026#39;\u0026#39;; example = 24; }; }; }); description = \u0026#39;\u0026#39; Defines the tinc networks which will be started. Each network invokes a different daemon. \u0026#39;\u0026#39;; }; }; }; config = { networking.firewall = fold (a: b: a // b) { } (flip mapAttrsToList cfg.networks (network: data: { allowedTCPPorts = [ data.port ]; allowedUDPPorts = [ data.port ]; } )); services.tinc.networks = builtins.mapAttrs (network: data: { name = data.nodeName; hosts = let files = map (x: \u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + network + \u0026#34;/\u0026#34; + x) (builtins.attrNames (builtins.readDir (\u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + network +\u0026#34;/\u0026#34;))); ## CHECK!! use builtins.readFile before import and check if we notice changes on rebuilds! attrsetdata = builtins.listToAttrs (map (x: lib.nameValuePair x.name x.config) (map (x: (import x).tinc) files)); in attrsetdata; }) cfg.networks; networking.interfaces = fold (a: b: a // b) { } (flip mapAttrsToList cfg.networks (network: data: { \u0026#34;tinc.${network}\u0026#34;.ipv4 = { addresses = [ { address = data.ipv4Address; prefixLength = data.ipv4Prefix; } ]; routes = let files = map (x: \u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + network + \u0026#34;/\u0026#34; + x) (builtins.attrNames (builtins.readDir (\u0026#34;/etc/nixos/vpn/tinc/\u0026#34; + network +\u0026#34;/\u0026#34;))); routes = builtins.concatLists (map (x: x.ipv4) (map (x: (import x).routes) files)); in routes; }; } )); }; } And that is basically where I\u0026rsquo;m at for now.\nConclusion \u0026amp; Outlook In this post we started out with a bare tinc configuration on NixOS, refined it in order to add some readability and then created a Module in order to enable multiple tinc network configurations using only a few lines of code. Finally we added in the functionality of injecting node specific settings into the configuration.\nAs of now the module is still a bit of a half-baked affair, but I\u0026rsquo;m going to add in a bunch of things:\nthe ability to define a path for the network config files IPv6 addresses and routes (not entirely sure if there is the possibility make both IPv4 and IPv6 optional but still require one of both using nix) DNS using either networking.extraHosts or something like dnsmasq most importantly I need to consider Trust: since any node is able to simply inject configuration options it makes sense to: limit the options in some way that makes sense, e.g. using whitelisting for nodes that are allowed to make these kinds of changes only trust nodes which can prove that they are trustworthy, e.g. using gpg or minisign Also flakes are the new shit apparently and from what I\u0026rsquo;ve seen so far I really should take a closer look at those and make this module a flake.\nFor now the tincDifferent module in it\u0026rsquo;s current state of incompleteness can be found on github, feel free to give it a spin.\nAnyways, that\u0026rsquo;s all for now, I\u0026rsquo;ll be back at some point in the (hopefully near) future, thanks for having me!\n","permalink":"https://oblivious.observer/posts/tincdifferent-ii-tinc-nixos-module/","summary":"This is the second part of my little series about tinc and NixOS, where I first shortly introduced tinc and then explained how to set it up on Linux in general as well as on NixOS. This part is more or less a rewrite of a question I posted to the NixOS discourse a while ago.\nFor a while I\u0026rsquo;ve been using tinc on a bunch of infrastructure and so far it\u0026rsquo;s basically been rock solid: once set up nodes anywhere simply join their network and become reachable for anyone who needs to access them.","title":"Tinc different! - Part II: The Two Tunnels"},{"content":"This is a post about tinc - a nifty little Mesh VPN service. It is also the first part of a little series of posts related to tinc and NixOS. In this first part I\u0026rsquo;ll just write a bit about how to set up tinc, then in the second part I\u0026rsquo;ll take a closer look into how writing a NixOS module can managing tinc networks easier. And finally in the third part I\u0026rsquo;ll present a rewrite of the module with a bunch more features. For now, lets take a look at tinc..\nTINC - a low maintainance VPN tinc is one of my all-time favourite little tools, a nice little VPN daemon that has a bunch of really interesting properties such as NAT traversal, automatic use of TCP and UDP, a per node option of publishing of subnets and of course the fact that it is a mesh VPN, so nodes can go down without breaking everything. Also while most people apparently haven\u0026rsquo;t heard about tinc it has been around for ages.\nIn order to create a minimal tinc network you need to set up a network interface, create a tinc.conf file which contains information on the name of the node, the network device to use as well as the device type (tun or tap). There are a bunch of more or less optional files involved in configuring tinc : For example using two little scripts called tinc-up and tinc-down, the aforementioned network device that tinc is using can be created or removed on demand using whatever tools you prefer (e.g. iproute2 or ifconfig). Next a public-private key-pair (RSA and/or ECDSA) has to be generated. tinc provides an easy way to do so and generates a little node configuration, which only contains the public key(s) of the node. This config file can then filled with a bunch of configuration options such as the nodes IP address inside the mesh, it\u0026rsquo;s (optional) public IP address or subnets connected to a node that you want to make available to the rest of the mesh. Sounds good? Great, let\u0026rsquo;s look at the whole process in a bit more detail.\nSetting up a TINC Mesh on Linux Let\u0026rsquo;s set up a little example Mesh on a bunch of imaginary machines:\nnode0: with a public IP address, e.g. 10.10.10.1 node1: with a private IP address node2: with a private IP address and connected to the network 192.168.0.0/24, which should be made accessible to node0 and node1 First lets install tinc on all of our hosts using something like this:\nsudo apt install tinc Then the VPN network has to be set up on each of the nodes. tinc allows for more than one concurrent VPN connection, and keeps these VPN networks configured inside /etc/tinc/\u0026lt;vpn-name\u0026gt;, so create an example VPN network on all of the nodes (note that I\u0026rsquo;m using node{0,1,2} to express that this line should be executed on all three nodes):\n[user@node{0,1,2}:/etc/tinc/example]$ sudo mkdir /etc/tinc/example Next create the configuration file for this little example VPN network inside /etc/tinc/example/tinc.conf:\n[user@node0:/etc/tinc/example]$ cat tinc.conf Name = node0 DeviceType = tun [user@node1:/etc/tinc/example]$ cat tinc.conf Name = node1 DeviceType = tun ConnectTo = node0 [user@node2:/etc/tinc/example]$ cat tinc.conf Name = node2 DeviceType = tun ConnectTo = node0 Note that inside the configuration file, we set up the network device type only, not the device name, this can be done however using the Interface option. Also we set the nodes up in such a way that node1 as well as node2 will initially try to connect to node0 using the ConnectTo option. Interestingly some of the options seem to be optional, e.g. the NixOS tinc module sets up the explicit network interface name using the Interface option, but does not seem to use the ConnectTo option.\nNext on all of the hosts the keypairs need to be configured. In order to set up a VPN using a RSA key of length 4096, execute the following command on the nodes:\n[user@node{0,1,2}:/etc/tinc/example]$ sudo tincd -n example -K4096 tinc will then create a private-public keypair and store the private key inside of the node configuration file in /etc/tinc/example/hosts/\u0026lt;node-name\u0026gt; on each of the nodes. Apart from the public key these files will be empty:\n[user@node0:/etc/tinc/example]$ cat hosts/node0 -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- [user@node1:/etc/tinc/example]$ cat hosts/node1 -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- [user@node2:/etc/tinc/example]$ cat hosts/node2 -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- At this point it makes sense to think about stuff such as ports used by the VPN and the subnet to use for the nodes. Let\u0026rsquo;s use 10.0.0.0/24 to address our nodes and use the default tinc port 655. Additionally on node0 let\u0026rsquo;s add the nodes Public IP address and on node2 lets add the 192.168.0.0/24 subnet:\n[user@node0:/etc/tinc/example]$ cat hosts/node0 Address = 10.10.10.1 Subnet = 10.0.0.0 Port = 655 -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- [user@node1:/etc/tinc/example]$ cat hosts/node1 Subnet = 10.0.0.1 Port = 655 -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- [user@node2:/etc/tinc/example]$ cat hosts/node2 Subnet = 10.0.0.1 Subnet = 192.168.0.0/24 Port = 655 -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- After setting up the nodes configuration files, they need to be shared, since in our little example only node0 has a publicly reachable IP address the easiest thing would be to upload the config of node1 and node2 to node0 and at the same time grab the config files of node0. Ideally of course the configuration files should be synchronized between all nodes, but this is not a requirement for tinc, the example VPN will be accessible without having the config of node1 on node2 and vice versa as well.\nAt this point the setup of the network interface as well as the routes is still missing. Remember these optional setup files I mentioned earlier? Lets create /etc/tinc/example/tinc-{up,down} on all of the nodes:\n[user@node0:/etc/tinc/example]$ cat tinc-up #!/bin/sh ip link set $INTERFACE up ip addr add 10.0.0.0/32 dev $INTERFACE ip route add 192.168.0.0/24 dev $INTERFACE [user@node0:/etc/tinc/example]$ cat tinc-down #!/bin/sh ip route del 192.168.0.0/24 dev $INTERFACE ip addr del 10.0.0.0/32 dev $INTERFACE ip link set $INTERFACE down [user@node2:/etc/tinc/example]$ cat tinc-up #!/bin/sh ip link set $INTERFACE up ip addr add 10.0.0.2/32 dev $INTERFACE [user@node2:/etc/tinc/example]$ cat tinc-down #!/bin/sh ip addr del 10.0.0.2/32 dev $INTERFACE ip link set $INTERFACE down Note that since the configuration on node1 and node0 is pretty much identical (apart from the ip address of course) I have omitted the tinc-{up,down} scripts from node1.\nThe content of these scripts is pretty much straightforward:\ntinc-up creates a new network interface (tinc uses the environment variable INTERFACE to store the interfaces name), then assigns the IP address of the node and optionally (node0 and node1) add a route to a subnet that\u0026rsquo;s shared by one of the other nodes (in this case node2). tinc-down removes the IP address and route from the nodes and takes down the interface. Both tinc-up and tinc-down are really only shell scripts, which are run before respectively after the tinc VPN is created so writing anything custom should be relatively straight forward.\nOne important part is to make sure the tinc-{up,down} scripts are executable:\n[user@node{0,1,2}:/etc/tinc/example]$ sudo chmod +x tinc-{up,down} At this point everything is set up and enabling and starting the example VPN is as simple as:\n[user@node{0,1,2}:/etc/tinc/example]$ sudo systemctl enable tinc@example [user@node{0,1,2}:/etc/tinc/example]$ sudo systemctl start tinc@example After starting the example network, all nodes should be accessible via their VPN IP addresses and the 192.168.0.0/24 subnet should be accessible from all nodes as well.\nSetting up a TINC Mesh on NixOS As with a lot of other things, when using tinc on NixOS everything works a bit different. Instead of setting everything up by hand there is a neat little tinc module.\nSidenote: Using a little alias, it is possible to search for NixOS options without using the options search on the NixOS website:\n$ alias nix-search-options=\u0026#39;man configuration.nix | less -p \u0026#39; $ nix-search-options services.tinc This will drop you inside of a less buffer looking like this, where you can navigate using n and N to jump to the next respectively previous match:\n... services.tinc.networks Defines the tinc networks which will be started. Each network invokes a different daemon. Type: attribute set of submodules Default: { } Declared by: \u0026lt;nixpkgs/nixos/modules/services/networking/tinc.nix\u0026gt; services.tinc.networks.\u0026lt;name\u0026gt;.package The package to use for the tinc daemon\u0026#39;s binary. Type: package Default: \u0026#34;pkgs.tinc_pre\u0026#34; Declared by: \u0026lt;nixpkgs/nixos/modules/services/networking/tinc.nix\u0026gt; ... All in all there are a bunch of available options, but in order to get started you really only need the following two (that is from the tinc module specifically):\nservices.tinc.networks Defines the tinc networks which will be started. Each network invokes a different daemon. services.tinc.networks.\u0026lt;name\u0026gt;.hosts The name of the host in the network as well as the configuration for that host. This name should only contain alphanumerics and underscores. services.tinc.networks.\u0026lt;name\u0026gt;.name The name of the node which is used as an identifier when communicating with the remote nodes in the mesh. If null then the hostname of the system is used to derive a name (note that tinc may replace non- alphanumeric characters in hostnames by underscores). Apart from these configuration options we need to set up a bunch of other things, such as the networking interface, routes or ports on the firewall. This is similiar to what we did using the tinc-{up,down} scripts in the previous section using iproute2:\nThe interface tinc expects in NixOS is named tinc.\u0026lt;network-name\u0026gt;, so in this case tinc.example has to be created and set up, here is how this would look like for node0 from our little example:\n... networking.interfaces.\u0026#34;tinc.example\u0026#34;.ipv4 = { addresses = [ { address = \u0026#34;10.0.0.0\u0026#34;; prefixLength = 24; } ]; routes = [ { address = \u0026#34;192.168.0.0\u0026#34;; prefixLength = 24; via = \u0026#34;10.0.0.2\u0026#34;; } ]; }; ... If the firewall is enabled on the nodes, the port used (e.g. 655) has to be opened as well:\n... networking.firewall.allowedTCPPorts = [ 655 ]; networking.firewall.allowedUDPPorts = [ 655 ]; ... Now let\u0026rsquo;s walk through how to set up tinc using NixOS. In order to make everything less confusing, let\u0026rsquo;s keep everything inside a nix file, e.g. tinc-example.nix and import this file inside the configuration.nix by adding the former file to the imports statement:\n... imports = [ ... ./tinc-example.nix ... ]; ... Now in getting a tinc node up and running in NixOS is a two-step process: First write down the configuration of the node and rebuild the system.\n{ config, lib, pkgs, ... }: { networking.firewall.allowedTCPPorts = [ 655 ]; networking.firewall.allowedUDPPorts = [ 655 ]; networking.interfaces.\u0026#34;tinc.example\u0026#34;.ipv4 = { addresses = [ { address = \u0026#34;10.0.0.0\u0026#34;; prefixLength = 24; } ]; routes = [ { address = \u0026#34;192.168.0.0\u0026#34;; prefixLength = 24; via = \u0026#34;10.0.0.2\u0026#34;; } ]; }; services.tinc.networks = { example = { name = \u0026#34;node0\u0026#34;; hosts = {}; }; }; } At this point nix will check if the tinc network has already been configured, if not, the node will be bootstrapped, and /etc/tinc/example/hosts/\u0026lt;node-name\u0026gt; will be created containing the nodes public keys, but still missing the rest of the configuration.\nNow the node can be added to the hosts attrset, by grabbing the content from the newly generated file (e.g. /etc/tinc/example/hosts/node0) and then add the missing configuration options. From then on, the nodes config file will be overwritten by the configuration (along with all of the other configured hosts). The final tinc configuration will look like this:\n{ config, lib, pkgs, ... }: { networking.firewall.allowedTCPPorts = [ 655 ]; networking.firewall.allowedUDPPorts = [ 655 ]; networking.interfaces.\u0026#34;tinc.example\u0026#34;.ipv4 = { addresses = [ { address = \u0026#34;10.0.0.0\u0026#34;; prefixLength = 24; } ]; routes = [ { address = \u0026#34;192.168.0.0\u0026#34;; prefixLength = 24; via = \u0026#34;10.0.0.2\u0026#34;; } ]; }; services.tinc.networks = { example = { name = \u0026#34;node0\u0026#34;; hosts = { node0 = \u0026#39;\u0026#39; Address = Subnet = 10.0.0.0 Port = 655 Ed25519PublicKey = ... -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- \u0026#39;\u0026#39;; node1 = \u0026#39;\u0026#39; Subnet = 10.0.0.1 Port = 655 Ed25519PublicKey = ... -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- \u0026#39;\u0026#39;; node2 = \u0026#39;\u0026#39; Subnet = 10.0.0.2 Subnet = 192.168.0.0/24 Port = 655 Ed25519PublicKey = ... -----BEGIN RSA PUBLIC KEY----- ... -----END RSA PUBLIC KEY----- \u0026#39;\u0026#39;; }; }; }; } At this point the example network can be enabled by simply doing a nixos-rebuild switch and the node should be connected.\nIn the next part we\u0026rsquo;ll take a closer look at how writing a NixOS module can help with keeping a simpler and readable NixOS configuration.\n","permalink":"https://oblivious.observer/posts/tincdifferent-i-tinc-nixos/","summary":"This is a post about tinc - a nifty little Mesh VPN service. It is also the first part of a little series of posts related to tinc and NixOS. In this first part I\u0026rsquo;ll just write a bit about how to set up tinc, then in the second part I\u0026rsquo;ll take a closer look into how writing a NixOS module can managing tinc networks easier. And finally in the third part I\u0026rsquo;ll present a rewrite of the module with a bunch more features.","title":"Tinc different! - Part I: The Fellowship of the Ping"},{"content":"How I yet again decided to set up another iteration of this blog. This time using nix, hugo and of course orgmode..\nIntroduction This is my third attempt of setting up an easy to use blogging workflow. The first time I chose to try out use ox-hugo as well as github pages. Initially everything worked, but I quickly stopped writing posts because my initial setup using a single orgmode file with ox-hugo did not match my notetaking workflow, where I basically generate a bunch of random files and some of them get reworked or updated enough for me to think about publishing them. Also I wasn\u0026rsquo;t particularly happy about hugo, especially since I didn\u0026rsquo;t find a theme that I liked, so in the end I decided to put together my own, which was basically exactly what I had planned on not doing. Ideally I just want to publish my content without having to take care of anything else than the content I produce (apart from the whole theming I also took a short dive into hugos templating, when I tried to include an RSS feed and it simply wasn\u0026rsquo;t fun).\nKind of fed up with my initial attempt at creating a blog at some point I revisited the idea. This time I only shortly looked at hugo, but kind of had the idea in mind of using only emacs to set up everything. This time I took a closer look at org-publish and decided to build my own little solution, what I came up with was oblivious, a small 100 LoC shell script that in conjunction with emacs and org-mode can - probably among other things - function as a static site generator. Basically org-publish is used to define the way the website looks like, and a bunch of language-agnostic fetchers as well as templating snippets are defined (all using various built-in features of orgmode) and then emacs is executed using the oblivious command loading/tangling the org-file containing all the logic and settings that make up the site. I also included some some hackish lines that \u0026ldquo;infect\u0026rdquo; a users emacs config so one can start exporting everything using org-publish. Guess I\u0026rsquo;m part of the old and venerable society of static site generator generators now - yay!\nThis post describes my current setup in detail, I also link to the shell.nix as well as the config.yml at the end so anyone should be able to set up a similiar site pretty quickly.\nGetting started the nix way Since my little excursion into org-publish I have increasingly worked with NixOS and since totally oblivious to me after a reinstall my static site generator stopped working I started looking for another blogging workflow yet again, this time I revisited the idea of using hugo, but I wanted to do it in a nix way: set up a shell.nix file that generates my hugo environment. There are a bunch of examples on how to write such a nix file:\nkalbas.it computers.lol hugoreeves.com I started out with an empty directory:\nmkdir blog \u0026amp;\u0026amp; cd blog After I found myself a theme that seems to be nice and regularly updated, I started out with this shell.nix:\n{ pkgs ? import \u0026lt;nixpkgs\u0026gt; {} }: with pkgs; let hugo-theme-papermod = runCommand \u0026#34;hugo-theme-papermod\u0026#34; { pinned = builtins.fetchTarball { name = \u0026#34;hugo-theme-papermod\u0026#34;; # Store path name url = https://github.com/adityatelange/hugo-PaperMod/archive/master.tar.gz; sha256 = \u0026#34;03171mbdikjpx7f6m8ga9xgb62a7ars030dwpxv249vxs4zd2lq4\u0026#34;; # Hash obtained using `nix-prefetch-url --unpack \u0026lt;url\u0026gt;` }; patches = []; preferLocalBuild = true; } \u0026#39;\u0026#39; cp -r $pinned $out chmod -R u+w $out for p in $patches; do echo \u0026#34;Applying patch $p\u0026#34; patch -d $out -p1 \u0026lt; \u0026#34;$p\u0026#34; done \u0026#39;\u0026#39;; in mkShell { buildInputs = [ hugo ]; shellHook = \u0026#39;\u0026#39; mkdir -p themes ln -snf \u0026#34;${hugo-theme-papermod}\u0026#34; themes/hugo-theme-papermod \u0026#39;\u0026#39;; } This fetches the most up to date version of the theme as tarball, then optionally applies patches to that tarball and finally creates an item in the nix-store that contains the most recent patched version of the theme.\nThis is especially interesting because using standard unix patches, it is possible to keep the configuration that is specific to the blog separate from the code of the theme.\nNote that the link inside fetchTarball always points to the most recent version of the theme, so that executing nix-shell will fail due to containing the wrong sha checksum whenever there is an update on the theme repo. In this case the correct checksum can be retrieved like this:\n$ nix-prefetch-url --unpack https://github.com/adityatelange/hugo-PaperMod/archive/master.tar.gz unpacking... [0.2 MiB DL] path is \u0026#39;/nix/store/kr5m0bbs9sj8zvlx3ymm4ffiif3jvkwj-master.tar.gz\u0026#39; 0b8m1n2vn29mmgc9l2jpw9aiqbixh4nihsw0h43b04qwzh22ngaj First create a new hugo site using nix-shell:\n~/blog$ nix-shell -p hugo ~/blog$ hugo new site . ~/blog$ tree . ├── archetypes │ └── default.md ├── config.toml ├── content ├── data ├── layouts ├── shell.nix ├── static └── themes After making sure the checksum in shell.nix is correct and a nix-shell can be opened inside the blog directory in order to set up the themes directory:\n~/blog$ nix-shell unpacking \u0026#39;https://github.com/adityatelange/hugo-PaperMod/archive/master.tar.gz\u0026#39;... these derivations will be built: /nix/store/jx1ra8dxlhrp3q1imxpl93pn862p9nsb-hugo-theme-papermod.drv building \u0026#39;/nix/store/jx1ra8dxlhrp3q1imxpl93pn862p9nsb-hugo-theme-papermod.drv\u0026#39;... [nix-shell:~/blog]$ tree . ├── archetypes │ └── default.md ├── config.toml ├── content ├── data ├── layouts ├── shell.nix ├── static └── themes └── hugo-theme-papermod -\u0026gt; /nix/store/1vmg77dsz9rjwccjw2wqjhp4vpsgh49n-hugo-theme-papermod Now we can start setting up our blog: At this point I added a blog/org directory to house my posts (I hardlink the notes I want to publish from my notes directory here) and blog/patches in order to house the patches that customize the site to my liking.\nIn order to get started, remove the config.toml and instead lets grab the example configuration from the themes wiki page and set it up. At this point I added images to the blog/static folder, cause that is apparently the root location from where they are fetched by the config:\nbaseURL: \u0026#34;https://oblivious.observer\u0026#34; title: oblivious observer paginate: 5 theme: hugo-theme-papermod enableRobotsTXT: true buildDrafts: false buildFuture: false buildExpired: false minify: disableXML: true minifyOutput: true params: env: production # to enable google analytics, opengraph, twitter-cards and schema. title: oblivious.observer description: \u0026#34;a blog\u0026#34; author: Me # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors images: [\u0026#34;\u0026lt;link or path of image for opengraph, twitter-cards\u0026gt;\u0026#34;] defaultTheme: auto # dark, light disableThemeToggle: false ShowReadingTime: true ShowShareButtons: false disableSpecial1stPost: false comments: false hidemeta: false showtoc: false tocopen: false assets: # disableHLJS: true # to disable highlight.js # disableFingerprinting: true favicon: \u0026#34;/observer.png\u0026#34; favicon16x16: \u0026#34;/observer.png\u0026#34; favicon32x32: \u0026#34;/observer.png\u0026#34; apple_touch_icon: \u0026#34;/observer.png\u0026#34; safari_pinned_tab: \u0026#34;/observer.png\u0026#34; label: text: \u0026#34;Oblivious Observer\u0026#34; icon: /observer.png iconHeight: 35 # profile-mode profileMode: enabled: true # needs to be explicitly set title: Oblivious Observer subtitle: \u0026#34;simply a tech block..\u0026#34; imageUrl: \u0026#34;/observer.png\u0026#34; imageWidth: 120 imageHeight: 120 imageTitle: buttons: - name: Posts url: posts - name: Tags url: tags - name: Search url: search # home-info mode homeInfoParams: Title: \u0026#34;Hi there\u0026#34; Content: just a tech block.. socialIcons: - name: github url: \u0026#34;https://github.com/observer\u0026#34; - name: rss url: \u0026#34;/index.xml\u0026#34; cover: hidden: true # hide everywhere but not in structured data hiddenInList: true # hide on list pages and home hiddenInSingle: true # hide on single page # for search # https://fusejs.io/api/options.html fuseOpts: isCaseSensitive: false shouldSort: true location: 0 distance: 1000 threshold: 0.4 minMatchCharLength: 0 keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;content\u0026#34;] outputs: home: - HTML - RSS - JSON # is necessary menu: main: # - identifier: categories # name: categories # url: /categories/ # weight: 10 - identifier: search name: search url: /search/ weight: 19 - identifier: tags name: tags url: /tags/ weight: 20 - identifier: rss name: rss url: /index.xml weight: 21 # - identifier: example # name: example.org # url: https://example.org # weight: 30 # Read: https://github.com/adityatelange/hugo-PaperMod/wiki/FAQs#using-hugos-syntax-highlighter-chroma # pygmentsUseClasses: true # markup: # highlight: # # anchorLineNos: true # codeFences: true # guessSyntax: true # lineNos: true # style: monokai Setting up Doom Emacs From now on posts can be generated using ox-hugo. In order to set up ox-hugo on doom emacs, add a few lines to the configuration and use the doom script to install the packages:\n$ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; ~/.doom.d/packages.el (package! ox-hugo) EOF $ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; ~/.doom.d/packages.el (use-package! ox-hugo :after ox) EOF $ ~/.emacs.d/bin/doom sync Using ox-hugo from Emacs Then in order to post create a org file in blog/org or link one from somewhere else in there, write something and press C-c C-e H H in order to export a blog post. Only make sure you have set up the minimal necessary tags for ox-hugo, which seems to be #+HUGO_BASE_DIR:\n#+HUGO_BASE_DIR: ~/blog #+hugo_tags: NixOS orgmode hugo #+hugo_custom_front_matter: :ShowToc true Note that the theme specific options, such as ShowToc, can be set using hugo_custom_front_matter.\nPatching / Changing the theme Now at this point the idea would be to patch the theme before it is generated, however there is another option: Hugo allows for Overrides, basically replace any file in the theme directory with another one that can be found under the same path inside your site directory structure and you automatically override that part of the theme.\nAlthough I initially planned to make a bunch of patches which I keep inside of a patch directory, I decided against that for now. Initially I think I\u0026rsquo;d like to keep everything inside of a single shell.nix file.\nFont Awesome Fingerprint and About Icons Initially I added in an additional icon so I could link to my public key as well as an icon where I can link to a description on how this blog is built. Icons in the Papermod theme are defined by adding in svg code into blog/themes/hugo-theme-papermod/layouts/partials/svg.html. Inside the shellHook this can be done with a single sed statement:\n# add the font awesome fingerprint icon as gpg icon (source: https://fontawesome.com/icons/fingerprint) # also add the address card icon as about icon (source: https://fontawesome.com/icons/address-card) mkdir -p layouts/partials sed \u0026#39;/^{{- else if $icon_name -}}.*/i{{- else if (eq $icon_name \u0026#34;gpg\u0026#34;) -}}\\n\\\u0026lt;svg aria-hidden\\=\\\u0026#34;true\\\u0026#34; focusable\\=\\\u0026#34;false\\\u0026#34; data-prefix\\=\\\u0026#34;fas\\\u0026#34; data-icon\\=\\\u0026#34;fingerprint\\\u0026#34; class\\=\\\u0026#34;svg-inline--fa fa-fingerprint fa-w-16\\\u0026#34; role\\=\\\u0026#34;img\\\u0026#34; xmlns\\=\\\u0026#34;http:\\/\\/www.w3.org\\/2000\\/svg\\\u0026#34; viewBox\\=\\\u0026#34;0 0 512 512\\\u0026#34;\\\u0026gt;\\\u0026lt;path fill\\=\\\u0026#34;currentColor\\\u0026#34; d\\=\\\u0026#34;M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z\\\u0026#34;\\\u0026gt;\\\u0026lt;\\/path\\\u0026gt;\\\u0026lt;\\/svg\\\u0026gt;\\n{{- else if (eq $icon_name \u0026#34;about\u0026#34;) -}}\\n\\\u0026lt;svg aria-hidden\\=\\\u0026#34;true\\\u0026#34; focusable\\=\\\u0026#34;false\\\u0026#34; data-prefix\\=\\\u0026#34;far\\\u0026#34; data-icon\\=\\\u0026#34;address-card\\\u0026#34; class\\=\\\u0026#34;svg-inline--fa fa-address-card fa-w-18\\\u0026#34; role\\=\\\u0026#34;img\\\u0026#34; xmlns\\=\\\u0026#34;http:\\/\\/www.w3.org\\/2000\\/svg\\\u0026#34; viewBox\\=\\\u0026#34;0 0 576 512\\\u0026#34;\\\u0026gt;\\\u0026lt;path fill\\=\\\u0026#34;currentColor\\\u0026#34; d\\=\\\u0026#34;M528 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h480c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zm0 400H48V80h480v352zM208 256c35.3 0 64-28.7 64-64s-28.7-64-64-64-64 28.7-64 64 28.7 64 64 64zm-89.6 128h179.2c12.4 0 22.4-8.6 22.4-19.2v-19.2c0-31.8-30.1-57.6-67.2-57.6-10.8 0-18.7 8-44.8 8-26.9 0-33.4-8-44.8-8-37.1 0-67.2 25.8-67.2 57.6v19.2c0 10.6 10 19.2 22.4 19.2zM360 320h112c4.4 0 8-3.6 8-8v-16c0-4.4-3.6-8-8-8H360c-4.4 0-8 3.6-8 8v16c0 4.4 3.6 8 8 8zm0-64h112c4.4 0 8-3.6 8-8v-16c0-4.4-3.6-8-8-8H360c-4.4 0-8 3.6-8 8v16c0 4.4 3.6 8 8 8zm0-64h112c4.4 0 8-3.6 8-8v-16c0-4.4-3.6-8-8-8H360c-4.4 0-8 3.6-8 8v16c0 4.4 3.6 8 8 8z\\\u0026#34;\\\u0026gt;\\\u0026lt;\\/path\\\u0026gt;\\\u0026lt;\\/svg\\\u0026gt;\u0026#39; themes/hugo-theme-papermod/layouts/partials/svg.html \u0026gt; layouts/partials/svg.html Add comments I\u0026rsquo;m using utteranc.es as commenting system, which in turn uses the github API to store the comments inside of github issues. Since I\u0026rsquo;m using github pages anyway, I\u0026rsquo;m alright with this and prefer it to third party commenting systems.\nIn order to set up utterances a utterances.json has to be generated inside the blog/public folder, also a partial with the utteranc.es setup has to be created:\ncat \u0026lt;\u0026lt; EOF \u0026gt; ~/blog/public/utterances.json { \u0026#34;origins\u0026#34;: [ \u0026#34;https://oblivious.observer\u0026#34;, \u0026#34;https://observer.github.io\u0026#34; ] } EOF cat \u0026lt;\u0026lt; EOF \u0026gt; ~/blog/layouts/partials/comments.html \u0026lt;!-- Comments area start --\u0026gt; \u0026lt;script src=https://utteranc.es/client.js repo=observer/observer.github.io issue-term=url label=💬 theme=photon-dark crossorigin=anonymous async\u0026gt; \u0026lt;/script\u0026gt; \u0026lt;!-- Comments area end --\u0026gt; EOF Removing the html-footer I knid of like the idea of not having a footer that screams how and using which technology the site is built on every post, so I chose to remove it, there is this post explaining my workflow in detail as well as a short tl;dr I link to in the about page (probably look at the link in the about page, this one will probably be forgotten and outdated in no time). To remove the footer sed can be used:\nsed \u0026#39;/\u0026lt;footer/,/\u0026lt;\\/footer/d\u0026#39; themes/hugo-theme-papermod/layouts/partials/footer.html \u0026gt; layouts/partials/footer.html Add a CNAME file In order to make sure the site is published under its custom domain, a CNAME file has to be created inside the public directory:\nmkdir -p ~/blog/public cat \u0026lt;\u0026lt; EOF \u0026gt; ~/blog/public/CNAME oblivious.observer EOF Image Magic with imagemagick The Papermod themes config.yml contains a few lines that can be used to set up image files such as the sites favicon in the blog/static directory. Since I basically started out with only a single 16x16 png file, I decided to try setting everything up using the shell.nix.\nInitially I converted the image to text using base64:\nblog/static$ base64 observer.png iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAABhWlDQ1BJQ0MgcHJvZmlsZQAAKJF9 kT1Iw0AYht+mikUqCnYQcchQXbQgKuKoVShChVArtOpgcukfNGlIUlwcBdeCgz+LVQcXZ10dXAVB 8AfEydFJ0UVK/C4ptIjxjuMe3vvel7vvAKFeZprVMQ5oum2mEnExk10Vu14RQh/NUYgys4w5SUrC d3zdI8D3uxjP8q/7c/SoOYsBAZF4lhmmTbxBPL1pG5z3iSOsKKvE58RjJl2Q+JHrisdvnAsuCzwz YqZT88QRYrHQxkobs6KpEU8RR1VNp3wh47HKeYuzVq6y5j35C8M5fWWZ67SGkMAiliBBhIIqSijD Rox2nRQLKTqP+/gHXb9ELoVcJTByLKACDbLrB/+D37218pMTXlI4DnS+OM7HMNC1CzRqjvN97DiN EyD4DFzpLX+lDsx8kl5radEjoHcbuLhuacoecLkDDDwZsim7UpCWkM8D72f0TVmg/xboXvP61jzH 6QOQpl4lb4CDQ2CkQNnrPu8Otfft35pm/34AiXxysDj1F4kAAAAGYktHRAD/AP8A/6C9p5MAAAAJ cEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkCBAQLiQKSqHnAAAAGXRFWHRDb21tZW50AENyZWF0 ZWQgd2l0aCBHSU1QV4EOFwAAAiRJREFUOMulk7FO40AQhj/HS7wGOQYSB6KIiCJFGioKLKFDouYx eCiKE+/ASyCBeAOEIlEQhDf2OSRKNo7XvgJiHXdX3f3NjGa0M//M/GuFYVgaYwAoigJjDGVZUhQF AIvFgr29PYwxGGPQWpNlGb7vk+c5AuDk5IQkSXAch9FoxPb2Nt1uF4DHx0eCIMC2bYwxzOdzkiRh Z2eHp6cnapPJhNFoRK/XYzab0e12CcOQg4MDpJR0Oh3SNOXw8JBer4eUkuPjY7IsoygKrKOjo1JK +U8jTKdTxKbjMFaqKlCr1bAsC2MMUkpMlhG9vlYxYduUQBrHdPb3EX6jwYYQCCEAKgswXywA2HRd 8jwnW60A2BCCra0tsiz7WOK383P6/T7NZhOAOI4B2N3dBaDdbgMQRRHtdpsoigD4fnWFWC6XjMdj xuMxv+Pi4oI4jkmSpIqt/clkgmVZHwxarRb9fv+PAmsmd3d3/A0/0hRhWRZBEDAYDDg9PQXg+vqa ZrNZFQjDEIDLy0sAbm5uGA6HvDw/U4MSpRTq8xIAg8Gg8td7+RVBEKCUYrlcIlarnHq9jtaa29tb AJRSVfe1Bar8cDjEGPMh5XQ2A0BKWbHQWn+Z/ezsDM/zUEqhtcbzPFzX5eXtDeE4DovPewOUZYnj OF/0YNs20+kUAM/z0Frz/v5OWZaIer2O67rc399XD9YMWq0WAA8PD1VuLftGo4Hv+1j/+51/Agiw QFrcT1GrAAAAAElFTkSuQmCC Then in shell.nix define a string:\n... observer-16x16 = \u0026#39;\u0026#39; iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAABhWlDQ1BJQ0MgcHJvZmlsZQAAKJF9 kT1Iw0AYht+mikUqCnYQcchQXbQgKuKoVShChVArtOpgcukfNGlIUlwcBdeCgz+LVQcXZ10dXAVB 8AfEydFJ0UVK/C4ptIjxjuMe3vvel7vvAKFeZprVMQ5oum2mEnExk10Vu14RQh/NUYgys4w5SUrC d3zdI8D3uxjP8q/7c/SoOYsBAZF4lhmmTbxBPL1pG5z3iSOsKKvE58RjJl2Q+JHrisdvnAsuCzwz YqZT88QRYrHQxkobs6KpEU8RR1VNp3wh47HKeYuzVq6y5j35C8M5fWWZ67SGkMAiliBBhIIqSijD Rox2nRQLKTqP+/gHXb9ELoVcJTByLKACDbLrB/+D37218pMTXlI4DnS+OM7HMNC1CzRqjvN97DiN EyD4DFzpLX+lDsx8kl5radEjoHcbuLhuacoecLkDDDwZsim7UpCWkM8D72f0TVmg/xboXvP61jzH 6QOQpl4lb4CDQ2CkQNnrPu8Otfft35pm/34AiXxysDj1F4kAAAAGYktHRAD/AP8A/6C9p5MAAAAJ cEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkCBAQLiQKSqHnAAAAGXRFWHRDb21tZW50AENyZWF0 ZWQgd2l0aCBHSU1QV4EOFwAAAiRJREFUOMulk7FO40AQhj/HS7wGOQYSB6KIiCJFGioKLKFDouYx eCiKE+/ASyCBeAOEIlEQhDf2OSRKNo7XvgJiHXdX3f3NjGa0M//M/GuFYVgaYwAoigJjDGVZUhQF AIvFgr29PYwxGGPQWpNlGb7vk+c5AuDk5IQkSXAch9FoxPb2Nt1uF4DHx0eCIMC2bYwxzOdzkiRh Z2eHp6cnapPJhNFoRK/XYzab0e12CcOQg4MDpJR0Oh3SNOXw8JBer4eUkuPjY7IsoygKrKOjo1JK +U8jTKdTxKbjMFaqKlCr1bAsC2MMUkpMlhG9vlYxYduUQBrHdPb3EX6jwYYQCCEAKgswXywA2HRd 8jwnW60A2BCCra0tsiz7WOK383P6/T7NZhOAOI4B2N3dBaDdbgMQRRHtdpsoigD4fnWFWC6XjMdj xuMxv+Pi4oI4jkmSpIqt/clkgmVZHwxarRb9fv+PAmsmd3d3/A0/0hRhWRZBEDAYDDg9PQXg+vqa ZrNZFQjDEIDLy0sAbm5uGA6HvDw/U4MSpRTq8xIAg8Gg8td7+RVBEKCUYrlcIlarnHq9jtaa29tb AJRSVfe1Bar8cDjEGPMh5XQ2A0BKWbHQWn+Z/ezsDM/zUEqhtcbzPFzX5eXtDeE4DovPewOUZYnj OF/0YNs20+kUAM/z0Frz/v5OWZaIer2O67rc399XD9YMWq0WAA8PD1VuLftGo4Hv+1j/+51/Agiw QFrcT1GrAAAAAElFTkSuQmCC \u0026#39;\u0026#39;; ... Then add imagemagick to the buildInputs and finally add some lines in order to make sure create a bunch of upscaled images using imagesmagicks point filter:\n... buildInputs = [ hugo imagemagick ]; ... mkdir -p static for size in 16x16 32x32 64x64 128x128 256x256 512x512; do echo \u0026#39;${observer-16x16}\u0026#39; | base64 -d | convert - -filter point -resize $size static/observer-$size.png done ... I\u0026rsquo;m pretty sure this is not exactly how to do it the nix way, since we kind of circumvent the nix store, but I still need learn the language.\nFinally set up the new filenames in the config:\nassets: favicon: \u0026#34;/observer-16x16.png\u0026#34; favicon16x16: \u0026#34;/observer-16x16.png\u0026#34; favicon32x32: \u0026#34;/observer-32x32.png\u0026#34; apple_touch_icon: \u0026#34;/observer-512x512.png\u0026#34; safari_pinned_tab: \u0026#34;/observer-512x512.png\u0026#34; label: text: \u0026#34; \u0026#34; icon: /observer-64x64.png iconHeight: 35 # profile-mode profileMode: ... imageUrl: \u0026#34;/observer-512x512.png\u0026#34; ... Release Process using gpg The last time I set up a blog I started signing the files inside the public directory using gpg. Let\u0026rsquo;s add something similiar to the shellHook in shell.nix:\n... # let\u0026#39;s define an alias to release the blog content. However since # bash aliases do not support parameters, lets use a function instead release() { if [ \u0026#34;$1\u0026#34; == \u0026#34;the\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$2\u0026#34; == \u0026#34;kraken\u0026#34; ]; then cd ~/blog for f in $(find -L public/ | grep \u0026#39;.sig$\u0026#39;); do rm $f done hugo for f in $(find -L public/ | grep -v \u0026#34;.git\u0026#34;); do if ! [ -d \u0026#34;$f\u0026#34; ]; then gpg -u 48A1CC122B2B6E37FDA4B7B474344F2BCE39AA5D --output $f.sig --detach-sig $f fi done cd - else echo \u0026#39; ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░▄▄███░░░░░ ░░▄▄░░░░░░░░░░░░░░░░░░░░░░░░░███████░░░░ ░░███▄░░░░░░░░░░░░░░░░░░░░░▄█████▀░█░░░░ ░░▀█████▄▄▄▄▀▀▀▀▀▀▀▀░▄▄▄▄▄███▀▀░▀███░░░░ ░░░░███▀▀░░░░░░░░░░░░░░▀▀▀███░░░░██▀░░░░ ░░░░██░░░░░░▄░░░░░░░░░░░░░░░▀▀▄▄███░░░░░ ░░░░▄█▄▄████▀█░█▄██▄▄░░░░░░░░░████▀░░░░░ ░░░▄████████░░░██████▄▄▄▄░░░░░████░░░░░░ ░░░███░█░▀██░░░▀███░█░░███▄▄░░░░▀█░░░░░░ ░░░████▄███▄▄░░░███▄▄▄█████▀░░░░░██░░░░░ ░░▄████▀▀░▀██▀░░░▀█████████░░░░░░██░░░░░ ░░▀███░░░▄▄▀▀▀▄▄░░░░▀██████░░░░░░░█░░░░░ ░░░███░░█░░░░░░░▀░░░░▀███▀░░░░░░░░█░░░░░ ░░░████▄▀░░░░░░░░▀░░░████▄░░░░░░░░░█░░░░ ░░░██████▄░░░░░░░░░▀▀████▀░░░░░░░░░█░░░░ ░░▄█████████▀▀▀▀░░░░░░░░░░░░░░░░░░░▀█░░░ ░░███████████▄▄▄▄░░░░░░░░░░░░░░░░░░░█▄░░ ░░████████▀▀▀▀▀▀░░░░░░░░░░░░░░░░░░░░░█▄░ ░░████████▄▄░░░░░░░░░░░░░░░░░░░░░░░░░░█░ ░▄███████▄▄░░░░░░░░░░░░░░░░░░░░░░░░░░░░█ ░▀▀▀▀▀▀▀▀▀█▀▀▀░░░░░░░░░░░░░░░░░░░░░░░░░█ NOPE.. use them correct words human.. \u0026#39; fi } ... Perfect! Finally I\u0026rsquo;ve found a use for the phrase release the kraken. After executing the release function, I still have to add the files inside blog/public to the git repository and push them.\nPushing all the things In order to publish my posts, I defined another little function inside the shellHook, which generates a git commit of the blog/public repo (which contains the github pages link) and pushes everything using an ssh-key that I keep inside my password store:\n... publish() { if [ \u0026#34;$1\u0026#34; == \u0026#34;all\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$2\u0026#34; == \u0026#34;the\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$2\u0026#34; == \u0026#34;things\u0026#34; ]; then cd ~/blog/public git add . git commit -m \u0026#34;$(date \u0026#39;+%s\u0026#39; | perl -lpe \u0026#39;$_=join \u0026#34; \u0026#34;, unpack\u0026#34;(B8)*\u0026#34;\u0026#39;)\u0026#34; ssh-agent bash -c \u0026#39;ssh-add - \u0026lt;\u0026lt;\u0026lt; $(pass oblivious.observer/github/ssh-key-pair | grep -v \u0026#34;ssh-ed25519\u0026#34;); git push\u0026#39; cd - else echo \u0026#39; ... the same grumpy art as above ... NOPE.. use them correct words human.. \u0026#39; fi } ... shell.nix and config.yml If by any chance someone here stumbles over this post, here are the shell.nix as well as the config.yml I\u0026rsquo;m currently using, you should be able to get up and running from using these files as well as this blog post pretty easy. In case you do, feel free to drop me a line :)\n","permalink":"https://oblivious.observer/posts/blogging-with-ox-hugo-2.0/","summary":"How I yet again decided to set up another iteration of this blog. This time using nix, hugo and of course orgmode..\nIntroduction This is my third attempt of setting up an easy to use blogging workflow. The first time I chose to try out use ox-hugo as well as github pages. Initially everything worked, but I quickly stopped writing posts because my initial setup using a single orgmode file with ox-hugo did not match my notetaking workflow, where I basically generate a bunch of random files and some of them get reworked or updated enough for me to think about publishing them.","title":"Blogging with orgmode, hugo and nix"},{"content":"This is a bit of a strange one, but in case anyone else goes down the rabbit hole of using noweb inside orgmode, this might be interesting.\nnoweb syntax provides a really nifty way to split up or even autogenerate code blocks in orgmode documents. When writing literate documents noweb syntax also provides one with a way of staying on topic, whenever these files become more complex. If you\u0026rsquo;ve never used noweb syntax in orgmode you should definitely look it up.\nOne thing that is missing however was the the ability to include orgfiles containing noweb extensions and then use the snippets in another file like this:\n#+setupfile: snippet_collection.org # or: #+include: snippet_collection.org Using the library of babel we can however load snippets from other files like this:\n(org-babel-lob-ingest \u0026#34;./path/to/snippet_collection.org\u0026#34;) It is also possible to use a little function like this in order to load multiple files from a directory:\n(loop for filename in (file-expand-wildcards \u0026#34;./path/to/snippet_collection/*.org\u0026#34;) collect (org-babel-lob-ingest filename)) Now in order to load the snippets upon opening the orgmode file add something like this or set up a .dir-locals.el file:\n# Local Variables: # eval: (loop for filename in (file-expand-wildcards \u0026#34;./dir/*.org\u0026#34;) collect (org-babel-lob-ingest filename)) # End: Of course instead of adding this line to every single file, it is also possible to create a setupfile containing these variables and to load the snippets using something like this:\n#+setupfile: snippet_collection.org # or: #+include: snippet_collection.org ","permalink":"https://oblivious.observer/posts/orgmode-include-noweb-extensions/","summary":"This is a bit of a strange one, but in case anyone else goes down the rabbit hole of using noweb inside orgmode, this might be interesting.\nnoweb syntax provides a really nifty way to split up or even autogenerate code blocks in orgmode documents. When writing literate documents noweb syntax also provides one with a way of staying on topic, whenever these files become more complex. If you\u0026rsquo;ve never used noweb syntax in orgmode you should definitely look it up.","title":"Including noweb snippets in orgmode"},{"content":"In this post I\u0026rsquo;ll describe how to set up a sparse VM image with full disk encryption and NixOS on ZFS, which can be uploaded to a VPS provider and then unlocked on boot using ssh.\nFirst we need to create a virtual machine image file. Initially I tried using qemu-img, but somehow the image file was missing some information and the VM would not recognize a disk. Instead I went with the easy way and used virt-manager to create a new VM with the correct image size. virt-manager seems to create images allocated with the full size:\n$ ls -lh /var/lib/libvirt/images -rw------- 1 root root 161G Aug 24 08:59 nixosVM160GB.qcow2 -rw------- 1 root root 41G Aug 24 00:22 nixosVM40GB.qcow2 Uploading 200GB of mostly empty VM images can take quite a while, lets make them smaller using the wonderful libguestfs-tools.\n$ nix-shell -p libguestfs-with-appliance --run \u0026#34;sudo virt-sparsify /var/lib/libvirt/images/nixosVM40GB.qcow2 \\ /var/lib/libvirt/images/nixosVM40GB-sparse.qcow2\u0026#34; [ 0.1] Create overlay file in /tmp to protect source disk [ 0.1] Examine source disk [ 1.8] Copy to destination and make sparse [ 46.6] Sparsify operation completed with no errors. .virt-sparsify-wrapped: Before deleting the old disk, carefully check that the target disk boots and works correctly. $ nix-shell -p libguestfs-with-appliance --run \u0026#34;sudo virt-sparsify /var/lib/libvirt/images/nixosVM160GB.qcow2 \\ /var/lib/libvirt/images/nixosVM160GB-sparse.qcow2\u0026#34; [ 0.0] Create overlay file in /tmp to protect source disk [ 0.1] Examine source disk [ 1.6] Copy to destination and make sparse [ 172.0] Sparsify operation completed with no errors. .virt-sparsify-wrapped: Before deleting the old disk, carefully check that the target disk boots and works correctly. Now the files are considerably smaller:\n$ ls -lh /var/lib/libvirt/images -rw------- 1 root root 161G Aug 24 08:59 nixosVM160GB.qcow2 -rw-r--r-- 1 root root 195K Aug 28 16:13 nixosVM160GB-sparse.qcow2 -rw------- 1 root root 41G Aug 24 00:22 nixosVM40GB.qcow2 -rw-r--r-- 1 root root 193K Aug 28 16:11 nixosVM40GB-sparse.qcow2 Next install the VMs. In order to do so, create a new VM in virt-manager using the sparse image, add a CDROM type storage device with the NixOS install iso and set up the virtual machine to boot from this device.\nThe following script will create a virtual machine with the following partition layout, (I\u0026rsquo;m using ZFS inside lvm in this setup):\nsda ├─sda1 GRUB ├─sda2 BOOT └─sda3 LINUX (LUKS CONTAINER) └─cryptroot LUKS MAPPER └─lvmvg-swap SWAP └─lvmvg-root ZFS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 #!/usr/bin/env bash set -e pprint () { local cyan=\u0026#34;\\e[96m\u0026#34; local default=\u0026#34;\\e[39m\u0026#34; # ISO8601 timestamp + ms local timestamp timestamp=$(date +%FT%T.%3NZ) echo -e \u0026#34;${cyan}${timestamp} $1${default}\u0026#34; 1\u0026gt;\u0026amp;2 } echo \u0026#34;These are the disks available:\u0026#34; ls -l /dev/disk/by-id/ | awk \u0026#39;{print $11, $10, $9}\u0026#39; | tr -d \u0026#39;./\u0026#39; | column -t echo # Set DISK select ENTRY in $(ls /dev/disk/by-id/); do echo $ENTRY DISK=\u0026#34;/dev/disk/by-id/$ENTRY\u0026#34; echo \u0026#34;Installing system on $ENTRY.\u0026#34; break done echo -n \u0026#34;Enter Swap size (e.g. 10G, 5M)\u0026#34; echo read SWAP_SIZE if [[ $SWAP_SIZE =~ ^[0-9]*(G|M)$ ]] then echo \u0026#34;Swap size set to $SWAP_SIZE\u0026#34; else echo \u0026#34;Please enter correct size\u0026#34; break fi read -p \u0026#34;\u0026gt; Do you want to wipe all data on $ENTRY ?\u0026#34; -n 1 -r echo # move to a new line if [[ \u0026#34;$REPLY\u0026#34; =~ ^[Yy]$ ]] then # Clear disk wipefs -af \u0026#34;$DISK\u0026#34; sgdisk -Zo \u0026#34;$DISK\u0026#34; fi pprint \u0026#34;Creating grub partition\u0026#34; sgdisk -n 0:0:+1000K -t 1:EF02 \u0026#34;$DISK\u0026#34; GRUB=\u0026#34;$DISK-part1\u0026#34; pprint \u0026#34;Creating boot partition\u0026#34; sgdisk -n 0:0:+512M -t 0:EF00 \u0026#34;$DISK\u0026#34; BOOT=\u0026#34;$DISK-part2\u0026#34; pprint \u0026#34;Creating Linux partition\u0026#34; sgdisk -n 0:0:0 -t 3:8300 \u0026#34;$DISK\u0026#34; LINUX=\u0026#34;$DISK-part3\u0026#34; # Inform kernel partprobe \u0026#34;$DISK\u0026#34; sleep 1 pprint \u0026#34;Format BOOT partition $BOOT\u0026#34; mkfs.vfat \u0026#34;$BOOT\u0026#34; pprint \u0026#34;Creating LUKS container on $LINUX\u0026#34; cryptsetup --type luks2 luksFormat \u0026#34;$LINUX\u0026#34; LUKS_DEVICE_NAME=cryptroot cryptsetup luksOpen \u0026#34;$LINUX\u0026#34; \u0026#34;$LUKS_DEVICE_NAME\u0026#34; LUKS_DISK=\u0026#34;/dev/mapper/$LUKS_DEVICE_NAME\u0026#34; # Create LVM physical volume pvcreate $LUKS_DISK LVM_VOLUME_GROUP=vg0 vgcreate \u0026#34;$LVM_VOLUME_GROUP\u0026#34; \u0026#34;$LUKS_DISK\u0026#34; lvcreate --name swap --size $SWAP_SIZE \u0026#34;$LVM_VOLUME_GROUP\u0026#34; SWAP=\u0026#34;/dev/$LVM_VOLUME_GROUP/swap\u0026#34; pprint \u0026#34;Enable SWAP on $SWAP\u0026#34; mkswap $SWAP swapon $SWAP # ZFS partition lvcreate --name root --extents 100%FREE \u0026#34;$LVM_VOLUME_GROUP\u0026#34; ZFS=\u0026#34;/dev/$LVM_VOLUME_GROUP/root\u0026#34; pprint \u0026#34;Create ZFS pool on $ZFS\u0026#34; # -f force # -m mountpoint zpool create -f -m none -R /mnt rpool \u0026#34;$ZFS\u0026#34; pprint \u0026#34;Create ZFS datasets\u0026#34; zfs create -o mountpoint=legacy rpool/root zfs create -o mountpoint=legacy rpool/root/nix zfs create -o mountpoint=legacy rpool/home zfs set com.sun:auto-snapshot=true rpool/home zfs snapshot rpool/root@blank pprint \u0026#34;Mount ZFS datasets\u0026#34; mount -t zfs rpool/root /mnt mkdir /mnt/nix mount -t zfs rpool/root/nix /mnt/nix mkdir /mnt/home mount -t zfs rpool/home /mnt/home mkdir /mnt/boot mount \u0026#34;$BOOT\u0026#34; /mnt/boot pprint \u0026#34;Generate NixOS configuration\u0026#34; nixos-generate-config --root /mnt # Add LUKS and ZFS configuration HOSTID=$(head -c8 /etc/machine-id) LINUX_DISK_UUID=$(blkid --match-tag UUID --output value \u0026#34;$LINUX\u0026#34;) HARDWARE_CONFIG=$(mktemp) cat \u0026lt;\u0026lt;CONFIG \u0026gt; \u0026#34;$HARDWARE_CONFIG\u0026#34; networking.hostId = \u0026#34;$HOSTID\u0026#34;; boot.initrd.luks.devices.\u0026#34;$LUKS_DEVICE_NAME\u0026#34;.device = \u0026#34;/dev/disk/by-uuid/$LINUX_DISK_UUID\u0026#34;; boot.zfs.devNodes = \u0026#34;$ZFS\u0026#34;; CONFIG pprint \u0026#34;Append configuration to hardware-configuration.nix\u0026#34; sed -i \u0026#34;\\$e cat $HARDWARE_CONFIG\u0026#34; /mnt/etc/nixos/hardware-configuration.nix After running the script, create the dropbear ssh keys:\n$ nix-shell -p dropbear --command \u0026#34;dropbearkey -t ecdsa -f /tmp/initrd-ssh-key\u0026#34; Copy the key into / as well as /mnt (for some reason the installer seems to fail, when the keys aren\u0026rsquo;t found in /):\n$ sudo mkdir -p /var/dropbear /mnt/var/dropbear $ sudo cp /tmp/initrd-ssh-key /var/dropbear/ $ sudo cp /tmp/initrd-ssh-key /mnt/var/dropbear/ Add in the boot config:\nboot.loader.grub.device = \u0026#34;/dev/sda\u0026#34;; boot.initrd.network.enable = true; boot.initrd.network.ssh = { enable = true; port = 2222; authorizedKeys = [ \u0026#34;ssh-rsa ...\u0026#34; ]; \u0026lt;-------- this is your list of ssh keys, which are allowed to log in hostECDSAKey = /var/dropbear/initrd-ssh-key; }; boot.initrd.network.postCommands = \u0026#39;\u0026#39; echo \u0026#34;cryptsetup-askpass\u0026#34; \u0026gt;\u0026gt; /root/.profile \u0026#39;\u0026#39;; Finally the VM has to be able to connect to the network, so the initramdisk needs to have the correct kernel modules available. It is possible to find out which kernel modules are necessary for a specific provider by simply launching a VM and looking at lsmod or clicking through the VM configuration and looking for a section on virtualisation drivers.\nboot.initrd.availableKernelModules = [ \u0026#34;e1000e\u0026#34; \u0026#34;virtio_pci\u0026#34; \u0026#34;e1000\u0026#34; ]; Finally install nixos using nixos-install and reboot.\nUsing different entries in ~/.ssh/config can make rebooting and decrypting the luks devices easier:\nHost vm Hostname 1.2.3.4 User user Host unlock.vm Hostname 1.2.3.4 User root Port 2222 This way there won\u0026rsquo;t be any key conflicts and you can conveniently unlock the VMs using:\n$ ssh unlock.vm # alternatively use: ssh -p 2222 root@192.168.122.x The authenticity of host \u0026#39;[192.168.122.x]:2222 ([192.168.122.x]:2222)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:... Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;[192.168.122.x]:2222\u0026#39; (ECDSA) to the list of known hosts. Passphrase for /dev/disk/by-uuid/...: ********************************* Waiting 10 seconds for LUKS to request a passphrase.......Connection to 192.168.122.x closed by remote host. Connection to 192.168.122.x closed. After installation these are the file sizes:\n$ sudo ls -lh /var/lib/libvirt/images/ -rw-r--r-- 1 root root 195K Aug 28 16:54 160GB-sparse-disk-template.qcow2 -rw-r--r-- 1 root root 193K Jul 31 18:36 20GB-sparse-disk-template.qcow2 -rw-r--r-- 1 root root 193K Aug 28 16:54 40GB-sparse-disk-template.qcow2 -rw------- 1 root root 161G Aug 24 08:59 nixosVM160GB.qcow2 -rw-r--r-- 1 root root 1.9G Aug 29 14:11 nixosVM160GB-sparse.qcow2 -rw------- 1 root root 41G Aug 24 00:22 nixosVM40GB.qcow2 -rw-r--r-- 1 root root 2.0G Aug 29 14:01 nixosVM40GB-sparse.qcow2 ","permalink":"https://oblivious.observer/posts/nixos-20.03-luks-zfs-kvm/","summary":"In this post I\u0026rsquo;ll describe how to set up a sparse VM image with full disk encryption and NixOS on ZFS, which can be uploaded to a VPS provider and then unlocked on boot using ssh.\nFirst we need to create a virtual machine image file. Initially I tried using qemu-img, but somehow the image file was missing some information and the VM would not recognize a disk. Instead I went with the easy way and used virt-manager to create a new VM with the correct image size.","title":"NixOS encrypted VM installation"},{"content":"This is a rather short post and the start of what will hopefully become a nice little series of posts related to NixOS.\nI just had to bulk install NixOS on a bunch of Intel NUCs, here is a little script to automate the installation process. Some bits and pieces are taken from other installation scripts, but it\u0026rsquo;s been a while, so I don\u0026rsquo;t know who to credit for them, I\u0026rsquo;m fairly sure this wonderful post from the NixOS discourse was involved though. The whole script can be found here as gist as well.\nStart with the usual #! and set the script up to exit on errors using set -e:\n#!/usr/bin/env bash set -e Next add a function for pretty printing, (I stole this part from somewhere else).\npprint () { local cyan=\u0026#34;\\e[96m\u0026#34; local default=\u0026#34;\\e[39m\u0026#34; # ISO8601 timestamp + ms local timestamp timestamp=$(date +%FT%T.%3NZ) echo -e \u0026#34;${cyan}${timestamp} $1${default}\u0026#34; 1\u0026gt;\u0026amp;2 } Instead of using a DISK variable and changing its value inside the script everytime it is run, simply let the user pick the device.\necho \u0026#34;These are the disks available:\u0026#34; ls -l /dev/disk/by-id/ | awk \u0026#39;{print $11, $10, $9}\u0026#39; | tr -d \u0026#39;./\u0026#39; | column -t echo # Set DISK select ENTRY in $(ls /dev/disk/by-id/); do echo $ENTRY DISK=\u0026#34;/dev/disk/by-id/$ENTRY\u0026#34; echo \u0026#34;Installing system on $ENTRY.\u0026#34; break done Next set up the SWAP Size:\necho -n \u0026#34;Enter Swap size (e.g. 10GiB, 5MiB)\u0026#34; echo read SWAP_SIZE if [[ $SWAP_SIZE =~ ^[0-9]*(GiB|MiB)$ ]] then echo \u0026#34;Swap size set to $SWAP_SIZE\u0026#34; else echo \u0026#34;Please enter correct size\u0026#34; break fi Ask for confirmation and wipe the disk:\nread -p \u0026#34;\u0026gt; Do you want to wipe all data on $ENTRY ?\u0026#34; -n 1 -r echo # move to a new line if [[ \u0026#34;$REPLY\u0026#34; =~ ^[Yy]$ ]] then # Clear disk wipefs -af \u0026#34;$DISK\u0026#34; sgdisk -Zo \u0026#34;$DISK\u0026#34; partprobe $DISK else exit 1 fi Initially I was using sgdisk in order to create all partitions. This usually seems to work, however I noticed, that on some hardware devices (NUC Canyons), the boot flag had to be set in order for them to actually boot up, so i switched to parted instead.\nparted $DISK -- mklabel gpt pprint \u0026#34;Creating system partition\u0026#34; parted $DISK -- mkpart primary 512MiB -${SWAP_SIZE} ROOT=\u0026#34;$DISK-part1\u0026#34; pprint \u0026#34;Creating swap partition\u0026#34; parted $DISK -- mkpart primary linux-swap -${SWAP_SIZE} 100% SWAP=\u0026#34;$DISK-part2\u0026#34; pprint \u0026#34;Creating boot (EFI) partition\u0026#34; parted $DISK -- mkpart ESP fat32 1MiB 512MiB parted $DISK -- set 3 boot on BOOT=\u0026#34;$DISK-part3\u0026#34; pprint \u0026#34;Running partprobe on $DISK\u0026#34; partprobe $DISK Now that the partitions have been created, lets create filesystems and mount them. The sleep at the beginning seems to be necessary, otherwise there are problems with creating the swap partition.\nsleep 2 # not sure why, but if we don\u0026#39;t wait, the following swap does not work properly pprint \u0026#34;Enable SWAP on $SWAP\u0026#34; mkswap -L swap $SWAP swapon -L swap # ZFS partition pprint \u0026#34;Create zfs pool rpool\u0026#34; zpool create -f -O mountpoint=none -O acltype=posixacl -O xattr=sa -R /mnt rpool $ROOT pprint \u0026#34;Create zfs dataset rpool/root\u0026#34; zfs create -o mountpoint=none rpool/root pprint \u0026#34;Create zfs dataset rpool/root/nixos\u0026#34; zfs create -o mountpoint=legacy rpool/root/nixos pprint \u0026#34;Create zfs dataset rpool/home\u0026#34; zfs create -o mountpoint=legacy rpool/home pprint \u0026#34;Mount datasets: rpool/root/nixos\u0026#34; mount -t zfs rpool/root/nixos /mnt pprint \u0026#34;Mount datasets: rpool/home\u0026#34; mkdir /mnt/home mount -t zfs rpool/home /mnt/home pprint \u0026#34;Mount boot partition\u0026#34; mkfs.fat -F 32 -n boot $BOOT mkdir /mnt/boot mount $BOOT /mnt/boot Finally create a nixos configuration and set the system up to boot from ZFS:\npprint \u0026#34;Generate NixOS configuration\u0026#34; nixos-generate-config --root /mnt # Add ZFS configuration HOSTID=$(head -c8 /etc/machine-id) HARDWARE_CONFIG=$(mktemp) cat \u0026lt;\u0026lt; CONFIG \u0026gt; \u0026#34;$HARDWARE_CONFIG\u0026#34; networking.hostId = \u0026#34;$HOSTID\u0026#34;; boot.supportedFilesystems = [ \u0026#34;zfs\u0026#34; ]; boot.zfs.devNodes = \u0026#34;$ROOT\u0026#34;; } CONFIG #pprint \u0026#34;Append configuration to hardware-configuration.nix\u0026#34; sed -i \u0026#34;\\$ s/.}\\$//;\\$e cat $HARDWARE_CONFIG\u0026#34; /mnt/etc/nixos/configuration.nix pprint \u0026#34;You can now install nixos using the \u0026#39;nixos-install\u0026#39; command.\u0026#34; ","permalink":"https://oblivious.observer/posts/nixos-zfs-install-script/","summary":"This is a rather short post and the start of what will hopefully become a nice little series of posts related to NixOS.\nI just had to bulk install NixOS on a bunch of Intel NUCs, here is a little script to automate the installation process. Some bits and pieces are taken from other installation scripts, but it\u0026rsquo;s been a while, so I don\u0026rsquo;t know who to credit for them, I\u0026rsquo;m fairly sure this wonderful post from the NixOS discourse was involved though.","title":"NixOS on ZFS Install Script"},{"content":"There is a new static site generator on the block.. also hello world..\nUpdate (2021): guess what, I\u0026rsquo;m back to hugo.. Not necessarily because I started to dislike the idea of oblivious, I think it still is a pretty interesting concept and I particularly love the idea of being able to create a site, while having minimal dependencies and being pretty much language agnostic. The reason I switched was that on the one hand I got a bit more involved with NixOS, which seems to be even better suited to some of the things oblivious ended up taking care of (such as fetching resources) and on the other hand I just want to be able to publish something by pressing a bunch of keys without having to actively maintain the underlying tooling.\nIf you wan to check out how my site looked like with oblivious, then you can head over to the blogs repo, checkout any commit preious to 6b200fa5c8c91719f615884f79d7e2163c0c7433 and drop the files into a webserver using e.g.:\npython3 -m http.server I guess I\u0026rsquo;ll just keep this online in case anyone finds this interesting, anyways, I hope you enjoy:\nOver the last weekend and a half, I\u0026rsquo;ve spent quite some time writing oblivious, which is essentially a little static site generator written in bash and org-mode and emacs-lisp.\nPrevious to writing oblivious I looked at other blogging solutions, I actually already tried using org-mode combined with ox-hugo in the past. While ox-hugo definitely does it\u0026rsquo;s job I ended up spending quite a big amount of time with\ntheming the whole thing, being annoyed with the go templating language, using the wrong version of hugo generally trying to figure out how to use ox-hugo manually downloading Javascript and CSS files so I could serve them locally While both hugo as well as ox-hugo are really nice projects, using it I found myself thinking about how nice it would be to have a native way to create a static site directly from within Emacs.\nThere are actually quite a bunch of options when it comes to creating websites using Emacs and org-mode, I\u0026rsquo;ve tried some of them out and finally settled with the built-in org-publish, which is what I\u0026rsquo;m using now.\nBut you\u0026rsquo;ve just said you\u0026rsquo;ve written your own static site generator!\nWell yes, but actually no. I lied, kind of.. but maybe just bear with me for a while, it\u0026rsquo;ll make sense eventually.\nI haven\u0026rsquo;t written a static site generator, that would have been a really bad idea, after all I\u0026rsquo;m an utter illiterate when it comes to web development and up until this point I\u0026rsquo;ve basically just cargo culted my way through all of the web-development parts of creating this website. I\u0026rsquo;ve used org-publish in order to generate everything, because it\u0026rsquo;s a nice little built-in and basically just works out of the box.\nSo what is oblivious then?\nBasically oblivious is a small little command line application you can use to [automatically] create a website.\nEmacs and org-publish still do the heavy lifting, when it comes to the actual export of your org-files. oblivious takes care of a few other quality of life things, such as:\ninjecting your org-publish configuration into your Emacs configuration, which is an utterly bad and gruesome idea in general, but it was kind of cool to simply \u0026ldquo;infect\u0026rdquo; my own config instead of having to take care of it elsewhere (you can actually use whatever export backend you prefer with oblivious) housekeeping tasks: such as configuring the .gitignore file or setting up the git user fetching (even building) external resources, such as Javascript or CSS triggering emacs in batch mode in order to load or tangle the configuration of a website All of the configuration of a site resides in a single org file, which is then tangled and/or loaded into everything necessary to create a site.\nInside of the configuration file you define settings such as the value for #+author, ways to automatically fetch resources you need and templates for parts of the site.\nWhenever you change the configuration file, you run oblivious sync in order to generate all the templates etc and oblivious fetch, whenever you need to download resources. Using oblivious build you can then start a batch export, which creates your static site.\nHow exactly you set up your site is still pretty much up to you, but IHMO using oblivious basically provides you with a similiar interface to what you may know from using some of the more common static site generators, while at the same time being org-mode, emacs-lisp and bash under the hood.\nIf you\u0026rsquo;re intrigued and want to look at the gory details, I suggest you head over to the oblivious repo and look at the readme.org file, which is where I keep a detailed configuration of how this site was built at that time.\nOh and also hello world!\n","permalink":"https://oblivious.observer/posts/introducing-oblivious/","summary":"There is a new static site generator on the block.. also hello world..\nUpdate (2021): guess what, I\u0026rsquo;m back to hugo.. Not necessarily because I started to dislike the idea of oblivious, I think it still is a pretty interesting concept and I particularly love the idea of being able to create a site, while having minimal dependencies and being pretty much language agnostic. The reason I switched was that on the one hand I got a bit more involved with NixOS, which seems to be even better suited to some of the things oblivious ended up taking care of (such as fetching resources) and on the other hand I just want to be able to publish something by pressing a bunch of keys without having to actively maintain the underlying tooling.","title":"Introducing oblivious"},{"content":"This post introduces a fork of the FreeBSD beadm utility which can be used to manage Boot Environments on Proxmox ZFS Installations.\nIn this Post I will showcase how to use the beadm Boot Environment manager in Proxmox. After the showcase there are some notes on what I did to make beadm run on Linux in general and finally a part about what has been changed in order to make beadm work with Proxmox specifically.\nThe version of beadm that is compatible with Proxmox can be found on github.\nOverview Introduction Managing Boot Environments in Proxmox Making beadm work under Linux Making beadm work with Proxmox Conclusion and Further Work Introduction In my previous post I\u0026rsquo;ve already outlined how boot environments work in general, how you can make them work under Proxmox VE 6 and presented a Proof of Concept solution of how they can be set up programmatically using zedenv, a python-based manager for Boot Environments.\nSince my last post I\u0026rsquo;ve taken a bit of a closer look at both the code of zedenv as well as beadm and decided to fork the latter and make it work with Proxmox.\nWhile this is still a Proof of Concept, it can be simply plugged into an existing Proxmox ZFS installation in order to enable and manage Boot Environments.\nAs with the previous PoC, I\u0026rsquo;ve written the code in such a way that no parts of Proxmox have been changed. Apart from some additional files that are created, your system stays as it is, also all Boot Entries created by Proxmox are still available in parallel to the Boot Environment entries in your bootloader.\nSince the last post I\u0026rsquo;ve simply reinstalled Proxmox with 10GB ESPs on both of my system drives, so I have enough space to store more Boot Environments. In order to install with a bigger ESP, install the system with custom (smaller) ZFS partition size, then after the installation, remove a drive from your ZFS pool, delete the ZFS partition, resize the ESP, create a new ZFS partition, add it back to the pool, resilver and repeat these steps for the second drive.\nManaging Boot Environments in Proxmox In order to use beadm with Proxmox you can grab a copy from github, make it executable and start using it right away. I do strongly encurage you to either use a test installation or look at the code and read through this post and the previous one first, so you know what is going on.\nOn the first run you\u0026rsquo;ll see the following error message, read it, create the template file and you are good to go (also remember that from now on, any changes to the cmdline file should be made to the template file instead, because the cmdline file will be autogenerated):\nroot@caliban:~# ./beadm ERROR: /etc/kernel/cmdline.template not found! You need a template file for the kernel commandline. The template file has to be identical to the /etc/kernel/cmdline file, but instead of a specific root it must contain the string \u0026#34;__BE_NAME\u0026#34; The template file is used in order to create a valid kernel commandline for systemd-boot, which points to the correct boot environment. In order to use beadm, create a valid /etc/kernel/cmdline.template Example: if your /etc/kernel/cmdline looks like this: root=ZFS=rpool/ROOT/pve-1 boot=zfs then the template file should contain: root=ZFS=rpool/ROOT/__BE_NAME boot=zfs After that you can list your boot environments with:\nroot@caliban:~# ./beadm list Boot Environments on caliban: name state used ds snaps ubrr refs creation date origin ---- ----- ---- -- ----- ---- ---- --------------------- ------ pve-1 NR 1.20G 1.15G 44.8M 0B 1.15G Sun Sep 1 17:08 2019 - At this point you don\u0026rsquo;t have a pve-1 Boot Environment, because the file structure of the ESPs is currently just pointing to the default Proxmox config. For the pve-1 dataset (or any dataset you create by hand) we can create the necessary files with the init command:\nroot@caliban:~# ./beadm init pve-1 Boot Environment for pve-1 has been generated successfully! Now you can create a new Boot Environment using create:\nroot@caliban:~# ./beadm init pve-2 Running hook script \u0026#39;pve-auto-removal\u0026#39;.. Running hook script \u0026#39;zz-pve-efiboot\u0026#39;.. Re-executing \u0026#39;/etc/kernel/postinst.d/zz-pve-efiboot\u0026#39; in new private mount namespace.. Copying and configuring kernels on /dev/disk/by-uuid/XXXX-XXXX Copying kernel and creating boot-entry for 5.0.15-1-pve Copying kernel and creating boot-entry for 5.0.21-1-pve Copying and configuring kernels on /dev/disk/by-uuid/YYYY-YYYY Copying kernel and creating boot-entry for 5.0.15-1-pve Copying kernel and creating boot-entry for 5.0.21-1-pve Created successfully Now we have a second Boot Environment, but it is not active. As you can see right now (N) pve-1 is active and after the next reboot (R) pve-1 will be active again:\nroot@caliban:~# ./beadm list Boot Environments on caliban: name state used ds snaps ubrr refs creation date origin ---- ----- ---- -- ----- ---- ---- --------------------- ------ pve-1 NR 1.20G 1.15G 44.8M 0B 1.15G Sun Sep 1 17:08 2019 - pve-2 - 8K 8K 0B 0B 1.15G Sun Oct 6 14:12 2019 rpool/ROOT/pve-1@2019-10-06-11:36:14 In order to activate the Boot Environment, run:\nroot@caliban:~# ./beadm activate pve-2 pve-2 has been activated successfully Now you can see that the Boot Environment will be active after a reboot:\nroot@caliban:~# ./beadm list Boot Environments on caliban: name state used ds snaps ubrr refs creation date origin ---- ----- ---- -- ----- ---- ---- --------------------- ------ pve-1 N 1.20G 1.15G 44.8M 0B 1.15G Sun Sep 1 17:08 2019 - pve-2 R 8K 8K 0B 0B 1.15G Sun Oct 6 14:12 2019 rpool/ROOT/pve-1@2019-10-06-11:36:14 If you don\u0026rsquo;t like the name, you can rename pve-2 with:\nroot@caliban:~# ./beadm rename pve-2 pve-002 root@caliban:~# ./beadm list Boot Environments on caliban: name state used ds snaps ubrr refs creation date origin ---- ----- ---- -- ----- ---- ---- --------------------- ------ pve-1 N 1.20G 1.15G 44.8M 0B 1.15G Sun Sep 1 17:08 2019 - pve-002 R 8K 8K 0B 0B 1.15G Sun Oct 6 14:12 2019 rpool/ROOT/pve-1@2019-10-06-11:36:14 You can also mount Boot Environments:\nroot@caliban:~# ls beadm root@caliban:~# ./beadm mount pve-002 mountpoint Mounted successfully on \u0026#39;mountpoint\u0026#39; root@caliban:~# ls beadm mountpoint root@caliban:~# ls mountpoint/ bin boot dev etc home lib lib32 lib64 libx32 media mnt opt proc root rpool run sbin srv sys tmp usr var root@caliban:~# ./beadm umount pve-002 Unmounted successfully root@caliban:~# ls beadm mountpoint root@caliban:~# rmdir mountpoint If you reboot, you should be booting into the active Boot Environment. Install something (e.g. htop), open it, then use beadm to activate the previous Boot Environment, reboot and notice how the program you\u0026rsquo;ve just installed has dissapeared, because you just went back in time.\nIf you plan on setting up your Proxmox host with Boot Environments, don\u0026rsquo;t forget to make sure you have your ZFS datasets set up properly, I\u0026rsquo;ve written about that in my previous post.\nMaking beadm work under Linux This part contains notes about which parts of the original beadm I changed to make it work on linux. You can skip this part if you don\u0026rsquo;t care about the details that are not specific to Proxmox.\nbeadm is originally a Boot Environment manager for FreeBSD by vermaden. It is written in shell and enables you to create new Boot Environments or create them from ZFS snapshots, to activate, rename or destroy Boot Environments, list them and also mount and unmount them:\nroot@caliban:~# beadm usage: beadm activate \u0026lt;beName\u0026gt; beadm create [-e nonActiveBe | -e beName@snapshot] \u0026lt;beName\u0026gt; beadm create \u0026lt;beName@snapshot\u0026gt; beadm destroy [-F] \u0026lt;beName | beName@snapshot\u0026gt; beadm list [-a] [-s] [-D] [-H] beadm rename \u0026lt;origBeName\u0026gt; \u0026lt;newBeName\u0026gt; beadm mount \u0026lt;beName\u0026gt; [mountpoint] beadm { umount | unmount } [-f] \u0026lt;beName\u0026gt; beadm version The script was originally about 900 LOC long and consisted of a bunch of helper functions followed by a big case statement, which takes care of running the code specific to the supplied parameter.\nAfter applying the following changes, apart from setting up the boot loader specific stuff, beadm should work on linux:\nRemoving the bootloader specific code Apart from managing the ZFS part it also takes care of managing FreeBSDs own bootloader or optionally grub2. I basically just removed these FreeBSD specific parts.\nReplacing the awk code of the list command The list section of the script also contains a rather large piece of awk code for displaying the available/active Boot Environments. I\u0026rsquo;ve replaced this part with shell, because I\u0026rsquo;m not yet familiar enough with awk to use it inside of a script. Therefore the new list command is probably a bit simpler than it\u0026rsquo;s FreeBSD parent. I also made sure that the correct next boot environment is marked, by looking into the content of /etc/kernel/cmdline.\nReplace FreeBSD date with it\u0026rsquo;s Linux counterpart For the destroy operation I had to replace the line containing the date command, since FreeBSDs date supports the -f option, while the linux date command does not.\nMissing -u Parameter in the Linux ZFS Implementation The rename operation uses the zfs rename with the -u parameter. This parameter does not exist in linux and according to the FreeBSD man-page, it makes sure that filesystems are not remounted during the rename operation:\n-u Do not remount file systems during rename. If a file system\u0026#39;s mountpoint property is set to legacy or none, file system is not unmounted even if this option is not given. Mounting ZFS datasets under Linux In order to make the mount operation work, the -o zfsutil option had to be applied to the zfs mount command.\nZFS automount with systemd As I worked on the linux version of beadm, I also noticed that apart from making sure all the ESPs are working as intended, when booting into a new Boot environment, only / was mounted.\nThis was due to the systemd zfs-mount service not running. This service makes sure that all the zfs datasets are mounted on boot, but was failing to execute zfs mount -a, because there is more than one dataset that should be mounted to /.\nIn order to fix this, we can use the canmount option and set it to off for all Boot Environments:\ncanmount=on | off | noauto If this property is set to off, the file system cannot be mounted, and is ignored by zfs mount -a. Setting this property to off is similar to setting the mountpoint property to none, except that the dataset still has a normal mountpoint property, which can be inherited. Setting this property to off allows datasets to be used solely as a mechanism to inherit properties. One example of setting canmount=off is to have two datasets with the same mountpoint, so that the children of both datasets appear in the same directory, but might have different inherited characteristics. When the noauto option is set, a dataset can only be mounted and unmounted explicitly. The dataset is not mounted automatically when the dataset is created or imported, nor is it mounted by the zfs mount -a command or unmounted by the zfs unmount -a command. This property is not inherited. Making beadm work with Proxmox This part describes the changes I made to beadm that were Proxmox specific.\nIn my last post I described that the content of the Boot Environments could be housed in a ZFS dataset and then copied over to the ESPs (EFI System Partitions). I only proposed this however, because the amount of space boot environments need long term is higher than the amount of space that is available on a default Proxmox ESP and because shrinking a ZFS partition (which usually fills the rest of the harddrive) is not really something ZFS does.\nRecap: How do Boot Environments work on Proxmox systemd-boot is used since there can be multiple ESPs, they are not mounted at runtime a script called pve-efiboot-tool is used to refresh the bootloader configuration on all ESPs the pve-efiboot-tool calls another script zz-pve-efiboot, which iterates over all ESPs, mounts them, updates the boot loader configuration and finally unnmounts them zz-pve-efiboot gets its information about which ESPs exist from /etc/kernel/pve-efiboot-uuids and the kernel commandline from /etc/kernel/cmdline the kernel commandline contains the mountpoint for /, which is the part of this file we need to exchange in order to activate a Boot Environment In the last post I proposed the use of a template file for the kernel commandline /etc/kernel/cmdline.template, from which the /etc/kernel/cmdline file is generated before calling pve-efiboot-tool refresh. We will be using such a file here. On the ESPs we need to create a directory structure housing the Boot Environment configuration files Check for /etc/kernel/cmdline.template We need to make sure that the beadm script fails if no /etc/kernel/cmdline.template is found or if it is found, but does not contain the correct string (”__BE_NAME”), which we replace to create the cmdline file.\nif [ ! -f /etc/kernel/cmdline.template ] || ! $(grep -q \u0026#34;__BE_NAME\u0026#34; /etc/kernel/cmdline.template) then echo \u0026#34;ERROR: /etc/kernel/cmdline.template not found!\u0026#34; # [...] exit 1 fi Temporary files beadm usually is able to tell which Boot Environment will be active after the next boot, by simply looking into the ESP or boot loader configuration. Since on Proxmox the ESPs are not always mounted, we keep this information inside /tmp/beadm/. We store a version of the current loader.conf as well as the name of the next active boot environment in /tmp/beadm so we don\u0026rsquo;t have to mount/unmount ESPs every time we use list. We do this in a helper function:\n__get_active_be_from_esp() { if [ ! -f /tmp/beadm/loader.conf ] then mount /dev/disk/by-uuid/$(cat /etc/kernel/pve-efiboot-uuids | head -n 1) /boot/efi mkdir -p /tmp/beadm/ cp /boot/efi/loader/loader.conf /tmp/beadm/loader.conf default_entry=\u0026#34;$(cat /tmp/beadm/loader.conf | grep default | cut -d \u0026#39; \u0026#39; -f2 | sed \u0026#39;s/-\\*$//\u0026#39;)\u0026#34; next_boot_environment=\u0026#34;$(cat /boot/efi/loader/entries/$(ls /boot/efi/loader/entries/ | \\ grep $default_entry | head -n 1) | grep options | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | \\ grep root= | cut -d \u0026#39;/\u0026#39; -f3)\u0026#34; echo $next_boot_environment \u0026gt; /tmp/beadm/next_boot_environment # [...] umount /boot/efi fi } The init Parameter The init operation is a new operation that was added to beadm so a manually created dataset or a dataset that has no Boot Environment files on the ESP (such as the default /rpool/ROOT/pve-1 dataset) can be converted to a working Boot Environment. This command basically just creates some files on the ESPs.\nThis one is new and quite important: use it to initialize a Boot Environment for a dataset that already exists (you probably only need it for rpool/ROOT/pve-1 or whenever you\u0026rsquo;ve created snapshots by hand), but it\u0026rsquo;s nice to have.\nif $(zfs get canmount rpool/ROOT/$be_name | grep -q on) then zfs set canmount=off ${POOL}/${BEDS}/$be_name fi __get_active_be_from_esp # [...] old_be_name=\u0026#34;$(cat /etc/kernel/cmdline | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | grep root | cut -d \u0026#39;/\u0026#39; -f3)\u0026#34; cat /etc/kernel/cmdline.template | sed \u0026#34;s/__BE_NAME/$be_name/\u0026#34; \u0026gt; /etc/kernel/cmdline pve-efiboot-tool refresh cat /etc/kernel/cmdline.template | sed \u0026#34;s/__BE_NAME/$old_be_name/\u0026#34; \u0026gt; /etc/kernel/cmdline for esp in $(cat /etc/kernel/pve-efiboot-uuids) do mount /dev/disk/by-uuid/$esp /boot/efi mkdir -p /boot/efi/env/$be_name cp -r /boot/efi/EFI/proxmox/* /boot/efi/env/$be_name/ cat /boot/efi/loader/loader.conf | sed \u0026#34;/default/c\\default $be_name-*\u0026#34; \u0026gt; /boot/efi/env/$be_name/loader.conf for entry in $(ls /boot/efi/loader/entries/ | grep proxmox-.*-pve.conf) do new_entry=\u0026#34;$(echo $entry | sed \u0026#34;s/proxmox/$be_name/;s/-pve//\u0026#34;)\u0026#34; cp /boot/efi/loader/entries/$entry /boot/efi/loader/entries/$new_entry sed -i \u0026#34;s/EFI/env/\u0026#34; /boot/efi/loader/entries/$new_entry sed -i \u0026#34;s/proxmox/$be_name/\u0026#34; /boot/efi/loader/entries/$new_entry sed -i \u0026#34;s/Proxmox Virtual Environment/$be_name/\u0026#34; /boot/efi/loader/entries/$new_entry done rm -f /boot/efi/loader/loader.conf cp /tmp/beadm/loader.conf /boot/efi/loader/loader.conf umount /boot/efi done The list Parameter We check if we find a file containing the loader.conf in /tmp, if not we mount a ESP and copy loader.conf to /tmp, unmount the ESP and use this file to store the name of the next active BE. Apart from that, we can just create some output containing basically the same information as the original beadm list produces:\nzfs_list=\u0026#34;$(zfs list -H -t filesystem,snapshot,volume -s creation -o name,used,usedds,usedbysnapshots,usedrefreserv,refer,creation,origin -r $POOL/$BEDS | grep \u0026#34;^$POOL/$BEDS/\u0026#34; | sed \u0026#39;s/\\t/,/g;s/\\n/;/g;s/ /_/g\u0026#39;)\u0026#34; __get_active_be_from_esp printf \u0026#34;%-20s %-10s %-10s %-7s %-8s %-7s %-7s %-30s %-10s\\n\u0026#34; name state used ds snaps ubrr refs \u0026#34;creation date\u0026#34; origin printf \u0026#34;%-20s %-10s %-10s %-7s %-8s %-7s %-7s %-30s %-10s\\n\u0026#34; ---- ----- ---- -- ----- ---- ---- --------------------- ------ for line in $(echo $zfs_list | tr \u0026#39;;\u0026#39; \u0026#39;\\n\u0026#39;); do BE_NAME=\u0026#34;$(echo $line | cut -d \u0026#39;,\u0026#39; -f1 | sed \u0026#34;s/^$POOL\\/$BEDS\\///\u0026#34;)\u0026#34; BE_ACTIVE=\u0026#34;$(if [ \u0026#34;$ROOTFS\u0026#34; = \u0026#34;$(echo $line | cut -d \u0026#39;,\u0026#39; -f1)\u0026#34; ]; then echo N; else echo \u0026#39; \u0026#39;; fi)\u0026#34; BE_NEXT=\u0026#34;$(if [ \u0026#34;$BE_NAME\u0026#34; = \u0026#34;$(cat /tmp/beadm/next_boot_environment)\u0026#34; ]; then echo R; else echo \u0026#39; \u0026#39;; fi)\u0026#34; BE_STATE=\u0026#34;$(if [ \u0026#34;$BE_ACTIVE$BE_NEXT\u0026#34; = \u0026#34; \u0026#34; ]; then echo \u0026#34;-\u0026#34;; else echo \u0026#34;$BE_ACTIVE$BE_NEXT\u0026#34;; fi)\u0026#34; BE_USED=\u0026#34;$(echo $line | cut -d \u0026#39;,\u0026#39; -f2)\u0026#34; BE_USEDDS=\u0026#34;$(echo $line | cut -d \u0026#39;,\u0026#39; -f3)\u0026#34; BE_USEDBYSNAPSHOTS=\u0026#34;$(echo $line | cut -d \u0026#39;,\u0026#39; -f4)\u0026#34; BE_USEDREFRESERV=\u0026#34;$(echo $line | cut -d \u0026#39;,\u0026#39; -f5)\u0026#34; BE_REFER=\u0026#34;$(echo $line | cut -d \u0026#39;,\u0026#39; -f6)\u0026#34; BE_DATE=\u0026#34;$(echo $line | cut -d \u0026#39;,\u0026#39; -f7 | sed \u0026#39;s/_/ /g\u0026#39;)\u0026#34; BE_ORIGIN=\u0026#34;$(echo $line | cut -d \u0026#39;,\u0026#39; -f8)\u0026#34; if ! echo $line | cut -d \u0026#39;,\u0026#39; -f 1 | grep -q \u0026#34;@\u0026#34;; then printf \u0026#34;%-20s %-10s %-10s %-7s %-8s %-7s %-7s %-30s %-10s\\n\u0026#34; $BE_NAME $BE_STATE $BE_USED $BE_USEDDS $BE_USEDBYSNAPSHOTS $BE_USEDREFRESERV $BE_REFER \u0026#34;$BE_DATE\u0026#34; \u0026#34;$BE_ORIGIN\u0026#34; fi done The create Parameter The create operation is used to create a new bootable dataset. After the operation the newly created Boot Environment is not active.\nIn order to make this operation work with Proxmox, first we need to grab a copy of the currently active Boot Environment from one ESP and store it in /tmp/beadm.\nThen we create a new /etc/kernel/cmdline from the template file and run pve-efiboot-tool refresh, which creates a valid bootloader configuration on all of our ESPs. Next we copy the kernel files into $ESP/env/$BE_NAME, rename all the conf files and restore the previous loader.conf that we saved in /tmp/beadm/loader.conf. This way we can create a new Boot Environment without activating it.\nzfs set canmount=off ${POOL}/${BEDS}/$be_name __get_active_be_from_esp old_be_name=\u0026#34;$(cat /etc/kernel/cmdline | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | grep root | cut -d \u0026#39;/\u0026#39; -f3)\u0026#34; cat /etc/kernel/cmdline.template | sed \u0026#34;s/__BE_NAME/$be_name/\u0026#34; \u0026gt; /etc/kernel/cmdline pve-efiboot-tool refresh cat /etc/kernel/cmdline.template | sed \u0026#34;s/__BE_NAME/$old_be_name/\u0026#34; \u0026gt; /etc/kernel/cmdline for esp in $(cat /etc/kernel/pve-efiboot-uuids) do mount /dev/disk/by-uuid/$esp /boot/efi mkdir -p /boot/efi/env/$be_name cp -r /boot/efi/EFI/proxmox/* /boot/efi/env/$be_name/ cat /boot/efi/loader/loader.conf | sed \u0026#34;/default/c\\default $be_name-*\u0026#34; \u0026gt; /boot/efi/env/$be_name/loader.conf for entry in $(ls /boot/efi/loader/entries/ | grep proxmox-.*-pve.conf) do new_entry=\u0026#34;$(echo $entry | sed \u0026#34;s/proxmox/$be_name/;s/-pve//\u0026#34;)\u0026#34; cp /boot/efi/loader/entries/$entry /boot/efi/loader/entries/$new_entry sed -i \u0026#34;s/EFI/env/\u0026#34; /boot/efi/loader/entries/$new_entry sed -i \u0026#34;s/proxmox/$be_name/\u0026#34; /boot/efi/loader/entries/$new_entry sed -i \u0026#34;s/Proxmox Virtual Environment/$be_name/\u0026#34; /boot/efi/loader/entries/$new_entry done rm /boot/efi/loader/loader.conf cp /tmp/beadm/loader.conf /boot/efi/loader/loader.conf umount /boot/efi done The activate Parameter Here we activate a Boot Environment, that has already been created, so we only need to mount all the ESPs and copy the correct conf file so it replaces the loader.conf. We also need to write the name of the active Boot Environment to /tmp/beadm.\nfor esp in $(cat /etc/kernel/pve-efiboot-uuids) do mount /dev/disk/by-uuid/$esp /boot/efi rm -f /boot/efi/loader/loader.conf # [...] cp /boot/efi/env/$be_name/loader.conf /boot/efi/loader/loader.conf umount /boot/efi done The rename Parameter Here, we need to rename the folder inside the env directory as well as the conf files in the entries directory. We also have to make sure they point to the new directory.\nfor esp in $(cat /etc/kernel/pve-efiboot-uuids) do mount /dev/disk/by-uuid/$esp /boot/efi mv /boot/efi/env/$be_name/ /boot/efi/env/$new_be_name/ sed -i \u0026#34;s/$be_name/$new_be_name/\u0026#34; /boot/efi/env/$new_be_name/loader.conf sed -i \u0026#34;s/$be_name/$new_be_name/\u0026#34; /boot/efi/loader/loader.conf for entry in $(ls /boot/efi/loader/entries/ | grep $be_name) do sed -i \u0026#34;s/$be_name/$new_be_name/\u0026#34; /boot/efi/loader/entries/$entry new_entry_name=$(echo $entry | sed \u0026#34;s/$be_name/$new_be_name/\u0026#34;) mv /boot/efi/loader/entries/$entry /boot/efi/loader/entries/$new_entry_name done umount /boot/efi done The destroy Parameter We need to delete the content of the env directory as well as the config files.\nfor esp in $(cat /etc/kernel/pve-efiboot-uuids) do mount /dev/disk/by-uuid/$esp /boot/efi rm -rf /boot/efi/env/$be_name/ for entry in $(ls /boot/efi/loader/entries/ | grep $be_name) do rm -f /boot/efi/loader/entries/$entry done umount /boot/efi done Conclusion and Further Work This post has introduced beadm, a Boot Environment Manager for the Proxmox Virtualization Plattform, which has been forked from it\u0026rsquo;s FreeBSD counterpart. First the usage of beadm is showcased, then some notes on porting beadm from FreeBSD to Linux are presented and finally the Proxmox specific additions to beadm are explained.\nThe way beadm is implemented in this post could partially be considered bad design, however this is due to beadm being an external tool and not part of the native Proxmox tooling. The scope of this was to implement Boot Environments without changing the Proxmox code. I\u0026rsquo;m sure the design will be improved if this functionality is added to Proxmox. The following Improvements can be made:\ndeduplication of kernel files (Reference counting could help to manage this) minimize the amount of mount / unmount operations add the ability to inject kernels into Boot Environments add the ability to prune kernels from Boot Environments add the ability to export / import Boot Environments together with the relevant files from the ESPs Now that Boot Environments on Proxmox (or more general on Linux) are a possibility, the next thing to do could be to write up how Boot Environments can be used in practice. For now the following scenarios, which could be interesting come to mind:\nUpdate systems using a new Boot Environment using chroot Migrate systems by moving Boot Environments Use a copy of a system in order to do forensic work Find out what has changed between two system states by comparing the filesystem snapshots Use the same “base” Boot Environment to run on multiple bare metal / virtualized machines, make an update on the base Environment, then push it out to multiple systems. Rollbacks in case of system failure: Reduce MTTR on bare metal systems ","permalink":"https://oblivious.observer/posts/proxmoxve6-boot-environment-manager/","summary":"This post introduces a fork of the FreeBSD beadm utility which can be used to manage Boot Environments on Proxmox ZFS Installations.\nIn this Post I will showcase how to use the beadm Boot Environment manager in Proxmox. After the showcase there are some notes on what I did to make beadm run on Linux in general and finally a part about what has been changed in order to make beadm work with Proxmox specifically.","title":"Introducing a Boot Environment Manager for Proxmox"},{"content":"Dear Reader, this time I would like to invite you onto a small journey: To boldly go where no man has gone before (Alright, that\u0026rsquo;s not true, but I think it\u0026rsquo;s the first time someone documents this kind of thing in the context of Proxmox). We\u0026rsquo;re about to embark on a journey to make your Proxmox host quite literally immortal. Also since what we are essentially doing here is only a Proof of concept, you probably shouldn\u0026rsquo;t use it in production, but as it\u0026rsquo;s really amazing, so you might want to try it out in a test environment.\nIn this article we are going to take a closer look at how Proxmox sets up the ESP (EFI Systems Partition) for systemd-boot and how we can adapt this process to support boot environments. Also this is going to be a long one, so you might want to grab a cup of coffee and some snacks to eat (And maybe start installing a Proxmox VE 6 VM with ZFS, because if boot environments are still new to you, at the point when you\u0026rsquo;ve read about halfway through this post, you will be eager to get your hands dirty and try this out for yourself).\nWhat are Boot Environments? Boot environments are a truly amazing feature, which originated somewhere in the Solaris/Illumos ecosystem (They have literally been around for ages, I\u0026rsquo;m not quite sure at which point in time they were introduced, but you can finde evidence at archeological digsites dating them back to at least 2003). and has since been adapted by other operating systems, such as FreeBSD, DragonflyBSD and others. The concept is actually quite simple:\nA boot environment is a bootable Oracle Solaris environment consisting of a root dataset and, optionally, other datasets mounted underneath it. Exactly one boot environment can be active at a time.\nOracle Solaris 11 Information Library In my own words, I would describe boot environments as snapshots of a [partial] system, which can booted from (that is when a Boot Environment is active) or be mounted at runtime (by the same system).\nThis enables a bunch of very interesting use-cases:\nRollbacks: This might not seem to be a pretty back deal at first, but once you realize that even after a major OS version upgrade, when something is suddenly broken, the previous version is just a reboot away. You can create bootable system snapshots on your bare metal machines, not only on your virtual machines. You can choose between creating a new boot environment to save the current systems state before updating or create a new boot environment, chroot into it, upgrade and reboot into a freshly upgraded system. You can quite literally take your work home if you like, by creating a boot environment and drum-roll taking it home. You you can of course also use this in order to create a virtual machine, container, jail or zone of your system in order to test something new or for forensic purposes. Are you hooked yet? Good, you really should be. If you\u0026rsquo;re not hooked, read till the end of the next section, you will be. (If you\u0026rsquo;re interested in boot environments, I would suggest, you take a look at vermadens presentation on ZFS boot environments, or generally searching a bit on the web for articles about Boot Environments on other unix systems, particularly there is quite a bit to be read on FreeBSD, which recently adopted them and which is far more in depth and better explained than what I\u0026rsquo;ll probably write down here.)\nBoot Environments on Linux While other operating systems have happily adapted boot environments, there is surprisingly (Or maybe not so surprisingly, if you remember how long zones and jails have been a thing, while linux just recently started doing containers. At least there\u0026rsquo;s still Windows to compare with.) apparently not too much going on in the linux world. The focus here seems to be more on containerizing applications in order to isolate them from the rest of the host system rather than to make the host system itself more solid (which is also great, but not the same).\nOn linux there are presently - at least to my knowledge - only the following projects that aim in a similiar direction:\nThere is snapper for btrfs, which seems to be a quite Suse specific solution. However according to it\u0026rsquo;s documentation: “A complete system rollback, restoring the complete system to the identical state as it was in when a snapshot was taken, is not possible.” This, at least without more explanation or context sounds quite a bit spooky. There is a Linux port of the FreeBSD beadm tool, which hasn\u0026rsquo;t been updated in ~3 years, while beadm has. It does not seem to be maintained any more and to be tailored to a single gentoo installation. There are a few btrfs specific scripts by a company called Pluribus Networks, which seem to have implemented their own version of beadm on top of btrfs. This apparently runs on some network devices. NixOS does something similiar to boot environments with their atomic update and rollback feature, but as far as I\u0026rsquo;ve understood this is still different from boot environments. Being functional, they don\u0026rsquo;t exactly roll back to a old version of the system based on a filesystem snapshot, but rather recreate an identical environment to a previous one. And finally there is zedenv, a boot environment manager that is written in python, supports both Linux and Freebsd and works really nice. It\u0026rsquo;s also the one that I\u0026rsquo;ve used before. It is also what we are going to use here, since there really isn\u0026rsquo;t an alternative when it comes to linux and ZFS. Poking around in Proxmox But before we start grabbing a copy of zedenv, we have to take a closer look into Proxmox itself in order to look at what we may have to adapt.\nBasically we already know that it is generally possible to use boot environments with ZFS and linux, so what we want is hopefully not exactly rocket science.\nWhat we are going to check is:\nHow do we have to adapt the Proxmox VE 6 rpool? How does Proxmox prepare the boot process and what do we have to tweak to make it boot into a boot environment? The Proxmox ZFS layout In this part we are going to take a look at how the ZFS layout is set up by the proxmox installer. This is because there\u0026rsquo;s a few things we have to consider when we use boot environments with Proxmox:\nWe do not ever want to interfere in the operation of our guest machines: Since we have the ability to snapshot and restore virtual machines and containers, there is really no benefit to include them into the snapshots of our boot environments, on the contrary, we really don\u0026rsquo;t want to end up with guests of our tenants missing files just because we\u0026rsquo;ve made a rollback. Is the ZFS layout compatible with running boot environments? Not all systems with ZFS are automatically compatible with using Boot Environments, basically if you just mount your ZFS pool as /, it won\u0026rsquo;t work Are there any directories we have to exclude from the root dataset? So lets look at Proxmox: By default after installing with ZFS root you get a pool called rpool which is split up into rpool/ROOT as well as rpool/data and looks similiar to this (zfs list):\nrpool 4.28G 445G 104K /rpool rpool/ROOT 2.43G 445G 96K /rpool/ROOT rpool/ROOT/pve-1 2.43G 445G 2.43G / rpool/data 1.84G 445G 104K /rpool/data rpool/data/subvol-101-disk-0 831M 7.19G 831M /rpool/data/subvol-101-disk-0 rpool/data/vm-100-disk-0 1.03G 445G 1.03G - rpool/data contains the virtual machines as well as the containers as you can see in the output of zfs list above. That\u0026rsquo;s great, we don\u0026rsquo;t have to manually move them. This takes care of the second point of our checklist from above.\nAlso rpool/ROOT/pve-1 is mounted as /, so we have rpool/ROOT which can potentially hold more than one snapshot of /, that is actually exactly what we need in order to use boot environments, the Proxmox team just saved us a bunch of time!\nThis only leaves the third part of our little checklist open. Which directories are left that we don\u0026rsquo;t want to snapshot as part of our boot environments? We can find a pretty important one in this context by checking /etc/pve/storage.cfg:\ndir: local path /var/lib/vz content iso,vztmpl,backup zfspool: local-zfs pool rpool/data sparse content images,rootdir So while the virtual machines and the containers are part of rpool/data, iso files, templates and backups are still located in rpool/root/pve-1. That\u0026rsquo;s not really what we want, imagine rolling back to a Boot Environment from a week ago and suddenly missing a weeks worth of Backups, that would be pretty annoying. Iso files as well as container templates are probably not worth keeping in our boot environments either.\nSo lets take /var/lib/vz out of rpool/root/pve-1, first create a new dataset:\nroot@caliban:/var/lib# zfs create -o mountpoint=/var/lib/vz rpool/vz cannot mount \u0026#39;/var/lib/vz\u0026#39;: directory is not empty Then move over the content of /var/lib/vz into the newly created and not yet mounted dataset:\nmv /var/lib/vz/ vz.old/ \u0026amp;\u0026amp; zfs mount rpool/vz \u0026amp;\u0026amp; mv vz.old/* /var/lib/vz/ \u0026amp;\u0026amp; rmdir vz.old If you don\u0026rsquo;t have any images, templates or backups yet, or you just don\u0026rsquo;t particularly care about them, you can of course also just remove /var/lib/vz/* entirely, mount rpool/vz and recreate the folder structure:\nroot@caliban:~# tree -a /var/lib/vz/ /var/lib/vz/ ├── dump └── template ├── cache ├── iso └── qemu Ok, now that\u0026rsquo;s out of the way, we should in general be able to make snapshots, roll them back without disturbing the operation of the proxmox server too much.\nBUT: this might not apply to your server, since there is still a lot of other stuff in /var/lib/ that you may want to include or exclude from snapshots! Better be sure to check what\u0026rsquo;s in there.\nAlso there are some other directories we might want to exclude. There is for example /tmp as well as /var/tmp/ which shouldn\u0026rsquo;t include anything that is worth keeping, but which of course would be snapshotted as well, we can create datasets for them as well and they should be automounted on reboot:\nzfs create -o mountpoint=/tmp rpool/tmp zfs create -o mountpoint=/var/tmp rpool/var_tmp If you\u0026rsquo;ve users that can connect directly to your Proxmox host, you might want to exclude /home/ as well. /root/ might be another good candidate, you may want to keep all of your shell history available at all times and regardless of which snapshot you\u0026rsquo;re currently in. You can also think about whether or not you want to have your logs, mail and proabably a bunch of other things included or excluded, I guess both variants have their use cases.\nOn my system zfs list returns something like this:\nNAME USED AVAIL REFER MOUNTPOINT rpool 5.25G 444G 104K /rpool rpool/ROOT 1.34G 444G 96K /rpool/ROOT rpool/ROOT/pve-1 1.30G 444G 1.16G / rpool/data 2.59G 444G 104K /rpool/data rpool/home_root 7.94M 444G 7.94M /root rpool/tmp 128K 444G 128K /tmp rpool/var_tmp 136K 444G 136K /var/tmp rpool/vz 1.30G 444G 1.30G /var/lib/vz At this point we\u0026rsquo;ve made sure that:\nthe Proxmox ZFS layout is indeed compatible with Boot Environments pretty much out of the box we moved the directories that might impact day to day operations out of what we want to snapshot we also excluded a few more directories, which is optional The Boot Preparation So after we\u0026rsquo;ve made sure that our ZFS layout works in this step we have to take a closer look at how the boot process is prepared in Proxmox. That is because as you might have noticed Proxmox does this a bit different from what you might be used to from other linux systems.\nAs an example this is what lsblk looks like on my local machine:\nnvme0n1 259:0 0 477G 0 disk ├─nvme0n1p1 259:1 0 2G 0 part /boot/efi └─nvme0n1p2 259:2 0 475G 0 part └─crypt 253:0 0 475G 0 crypt ├─system-swap 253:1 0 16G 0 lvm [SWAP] └─system-root 253:2 0 100G 0 lvm / And this is lsblk on Proxmox:\nsda 8:0 0 465.8G 0 disk ├─sda1 8:1 0 1007K 0 part ├─sda2 8:2 0 512M 0 part └─sda3 8:3 0 465.3G 0 part └─cryptrpool1 253:0 0 465.3G 0 crypt sdd 8:48 0 465.8G 0 disk ├─sdd1 8:49 0 1007K 0 part ├─sdd2 8:50 0 512M 0 part └─sdd3 8:51 0 465.3G 0 part └─cryptrpool2 253:1 0 465.3G 0 crypt Notice how there is no mounted EFI Systems Partition? That\u0026rsquo;s because both (Actually the UUIDs of all used ESP Partitions are stored in /etc/kernel/pve-efiboot-uuids) of the /dev/sdX2 devices, which are involved holding my mirrored proot pool contain a valid ESP. Also proxmox does not mount these partitions by default but rather encurages the use of their pve-efiboot-tool, which then takes care of putting a valid boot configuration on all involved drives, so you can boot off any of them.\nThis is not at all bad design, on the contrary, it is however noteworthy, because it bit it is different from what other systems with boot environments are using.\nHere is a quick recap on how in Proxmox the boot process is prepared:\nInitially something happens that requires an update of the bootloader configuration (e.g. a new kernel is installed or you\u0026rsquo;ve just set up an full disk encryption, changed something in the initramfs) This leads to /usr/sbin/pve-efiboot-tool refresh being run (either automated or manually), which at some point executes /etc/kernel/postinst.d/zz-pve-efiboot, which is the script that loops over the ESPs (which are defined by their UUID in /etc/kernel/pve-efiboot-uuids), mounts them and generates the boot loader configuration on them according to what Proxmox (or you as the user) has defined as kernel versions to keep. The bootloader configuration is created for every kernel and configured with the kernel commandline options from /etc/kernel/cmdline. On reboot you can use any harddrive that holds a EFI System Partition to boot from. Incidentally the /etc/kernel/cmdline file is also the one we configured in the previous post in order to enable remote decryption on a fully encrypted Proxmox host. Apart from the the options we added to it last time, it also contains another very interesting one:\nroot=ZFS=rpool/ROOT/pve-1 A Simple Proof of Concept At this point we already have everything we need:\nzfs snapshot rpool/ROOT/pve-1@test zfs clone rpool/ROOT/pve-1@test rpool/ROOT/pve-2 zfs set mountpoint=/ rpool/ROOT/pve-2 sed -i \u0026#39;s/pve-1/pve-2/\u0026#39; /etc/kernel/cmdline pve-efiboot-tool refresh reboot Tadaa!\nroot@caliban:~# mount | grep rpool rpool/ROOT/pve-2 on / type zfs (rw,relatime,xattr,noacl)rpool on /rpool type zfs (rw,noatime,xattr,noacl)rpool/var_tmp on /var/tmp type zfs (rw,noatime,xattr,noacl)rpool/home_root on /root type zfs (rw,noatime,xattr,noacl) rpool/tmp on /tmp type zfs (rw,noatime,xattr,noacl) rpool/vz on /var/lib/vz type zfs (rw,noatime,xattr,noacl) rpool/ROOT on /rpool/ROOT type zfs (rw,noatime,xattr,noacl) rpool/data on /rpool/data type zfs (rw,noatime,xattr,noacl) rpool/data/subvol-101-disk-0 on /rpool/data/subvol-101-disk-0 type zfs (rw,noatime,xattr,posixacl) Congratulations, you\u0026rsquo;ve just created your first boot environment! If you\u0026rsquo;re not convinced yet, just install something such as htop, enjoy the colors for a bit and run:\nsed -i \u0026#39;s/pve-2/pve-1/\u0026#39; /etc/kernel/cmdline pve-efiboot-tool refresh reboot And finally try to run htop again. Notice how it\u0026rsquo;s not only gone, in fact it was never even there in the first place, at least in from the systems point of view! Let that sink in for a moment. You want this.\nAt this point you might want to take a small break, grab another cup of coffee, lean back and remember this one time, back in the day when you were just getting started with all this operations stuff, it was almost beer o\u0026rsquo;clock and before going home you just wanted to apply this one tiny little update, which of course led to the whole server breaking. Remember how, when you were refilling your coffee cup for the third time this old solaris guru walked by on his way home and he had this mysterious smile on his face. Yeah, he knew you were about to spend half of the night there fixing the issue and reinstalling everything, in fact he probably had a similiar issue at the same day, but then decided to just roll back go home a bit early and take care of it the next day.\nFrom one Proof of Concept to Another So at this point we know how to set up a boot environment by hand, that\u0026rsquo;s nice, but currently we only can hop back and forth between a single boot environment, which is not cool enough yet.\nWe basically need some tooling which we can use to make everything work together nicely.\nSo in this section we are going to look into tooling as well as into how we may be able to make Proxmox play well together with the boot environment manager of our (only) choice zedenv.\nOur new objective is to look at what we need to do in order to enable us to select and start any number of boot environments from the boot manager.\nSidenote: The Proxmox ESP Size But first a tiny bit of math: since Proxmox uses systemd-boot, the kernel and initrd are stored in the EFI Systems Partition, which in a normal installation is 512MB in size. That should be enogh for the default case, where Proxmox stores only a hand full of kernels to boot from.\nIn our case however we might want to be able to access a higher number of kernels, so we can travel back in time in order to also start old boot environments.\nA typical pair of kernel and initrd seems to be about 50MB in size, so we can currently store about 10 different kernels at a time.\nIf we want to increase the size of the ESP however we might be out of luck, since ZFS does not like to shrink, so if you\u0026rsquo;re in the situation of setting up a fresh Proxmox host, you can just set up the size of the ZFS partition in the advanced options in the installer you might want to plug in a USB Stick (with less storage than your drives) or something similiar and create a mirrored ZFS RAID1 with this device and the other two drives which you really want to use for storage. This way the size of the resulting ZFS partition will be smaller than the drives you actually use and after the initial installation you can just:\nRemove the first drive from rpool, delete the ZFS partition, increase the size of the ESP to whatever you want, recreate a ZFS partition and readd it to rpool wait until rpool has resilvered the drive repeat this with your second drive. zedenv: A Boot Environment Manager Now lets install zedenv (Be sure to read the documentation at some point in time. Also check out John Ramsdens blog, which contains a bit more info about zedenv, workint linux ZFS configuration and a bunch of other awesome stuff):\nroot@caliban:~# apt install git python3-venv root@caliban:~# mkdir opt root@caliban:~# cd opt root@caliban:~# git clone https://github.com/johnramsden/pyzfscmds root@caliban:~# git clone https://github.com/johnramsden/zedenv root@caliban:~/opt# python3.7 -m venv zedenv-venv root@caliban:~/opt# . zedenv-venv/bin/activate (zedenv-venv) root@caliban:~/opt# cd pyzfscmds/ (zedenv-venv) root@caliban:~/opt/pyzfscmds# python setup.py install (zedenv-venv) root@caliban:~/opt/pyzfscmds# cd ../zedenv (zedenv-venv) root@caliban:~/opt/zedenv# python setup.py install Now zedenv should be installed into our new zedenv-venv:\n(zedenv-venv) root@caliban:~# zedenv --help Usage: zedenv [OPTIONS] COMMAND [ARGS]... ZFS boot environment manager cli Options: --version --plugins List available plugins. --help Show this message and exit. Commands: activate Activate a boot environment. create Create a boot environment. destroy Destroy a boot environment or snapshot. get Print boot environment properties. list List all boot environments. mount Mount a boot environment temporarily. rename Rename a boot environment. set Set boot environment properties. umount Unmount a boot environment. As you can check systemd-boot seems to be supported out of the box:\n(zedenv-venv) root@caliban:~# zedenv --plugins Available plugins: systemdboot But since we are using Proxmox systemd-boot and zedenv are actually not really supported. Remember that Proxmox doesn\u0026rsquo;t actually mount the EFI System Partitions? Well zedenv makes the assumption that there is only one ESP, and that it is mounted somewhere at all times.\nNonetheless, lets explore zedenv a bit so you can see how using a boot environment manager looks like. Let\u0026rsquo;s list the available boot environments:\n(zedenv-venv) root@caliban:~# zedenv list Name Active Mountpoint Creation pve-1 NR / Mon-Aug-19-1:27-2019 (zedenv-venv) root@caliban:~# zfs list -r rpool/ROOT NAME USED AVAIL REFER MOUNTPOINT rpool/ROOT 1.17G 444G 96K /rpool/ROOT rpool/ROOT/pve-1 1.17G 444G 1.17G / Before we can create new boot environments, we have to outwit zedenv on our Proxmox host: we have to set the bootloader to systemd-boot and due to the assumption that the ESP has to be mounted, we also have to make zedenv believe that the ESP is mounted (/tmp/efi is a reasonably sane path for this since we won\u0026rsquo;t be really using zedenv to configure systemd-boot here):\nzedenv set org.zedenv:bootloader=systemdboot mkdir /tmp/efi zedenv set org.zedenv.systemdboot:esp=/tmp/efi We can now create new boot environments:\n(zedenv-venv) root@caliban:~# zedenv create default-000 (zedenv-venv) root@caliban:~# zedenv list Name Active Mountpoint Creation pve-1 NR / Mon-Aug-19-1:27-2019 default-000 - Sun-Aug-25-19:44-2019 (zedenv-venv) root@caliban:~# zfs list -r rpool/ROOT NAME USED AVAIL REFER MOUNTPOINT rpool/ROOT 1.17G 444G 96K /rpool/ROOT rpool/ROOT/default-000 8K 444G 1.17G / rpool/ROOT/pve-1 1.17G 444G 1.17G / Notice the NR? This shows us that the pve-1 boot environment is now active (N) and after the next reboot the pve-1 boot environment will be active (R).\nWe also get information on the mountpoint of the boot environment as well as the date, when the boot environment was created, so we get a bit more information than only having the name of the boot environment.\nOn a fully supported system we could now also activate the default-000 boot environment, that we\u0026rsquo;ve just created and we would then get an output similiar to this, showing us that default-000 would be active on the next reboot (zedenv can also destroy, mount and unmount boot environments as well as get and set some ZFS specific options, but right now what we want to focus on is how to get activation working with Proxmox):\n(zedenv-venv) root@caliban:~# zedenv activate default-000 (zedenv-venv) root@caliban:~# zedenv list Name Active Mountpoint Creation pve-1 N / Mon-Aug-19-1:27-2019 default-000 R - Sun-Aug-25-19:44-2019 Since we are on Proxmox however, instead we\u0026rsquo;ll get the following error message:\n(zedenv-venv) root@caliban:~# zedenv activate default-000 WARNING: Running activate without a bootloader. Re-run with a default bootloader, or with the \u0026#39;--bootloader/-b\u0026#39; flag. If you plan to manually edit your bootloader config this message can safely be ignored. At this point you have seen how a typical boot environment manager looks like and you now know what create and activate will usually do.\nsystemd-boot and the EFI System Partitions Next we\u0026rsquo;ll take a closer look into the content of these EFI System Partitions and the files systemd-boot is using to start our system so lets take a look at what is stored on a ESP in Proxmox:\n(zedenv-venv) root@caliban:~# mount /dev/sda2 /boot/efi/ (zedenv-venv) root@caliban:~# tree /boot/efi . ├── EFI │ ├── BOOT │ │ └── BOOTX64.EFI │ ├── proxmox │ │ ├── 5.0.15-1-pve │ │ │ ├── initrd.img-5.0.15-1-pve │ │ │ └── vmlinuz-5.0.15-1-pve │ │ └── 5.0.18-1-pve │ │ ├── initrd.img-5.0.18-1-pve │ │ └── vmlinuz-5.0.18-1-pve │ └── systemd │ └── systemd-bootx64.efi └── loader ├── entries │ ├── proxmox-5.0.15-1-pve.conf │ └── proxmox-5.0.18-1-pve.conf └── loader.conf So we have the kernels and initrd in EFI/proxmox and some configuration files in loader/.\nThe loader.conf file looks like this:\n(zedenv-venv) root@caliban:/boot/efi# cat loader/loader.conf timeout 3 default proxmox-* We have a 3 second timeout in systemd-boot and the default boot entry has to begin with the string proxmox. Nothing too complicated here.\nApart from that, we have the proxmox-5.X.X-pve.conf files which we already know from last time (they are what is generated by the /etc/kernel/postinst.d/zz-pve-efiboot script). They look like this:\n(zedenv-venv) root@caliban:/boot/efi# cat loader/entries/proxmox-5.0.18-1-pve.conf title Proxmox Virtual Environment version 5.0.18-1-pve options ip=[...] cryptdevice=UUID=[...] cryptdevice=UUID=[...] root=ZFS=rpool/ROOT/pve-1 boot=zfs linux /EFI/proxmox/5.0.18-1-pve/vmlinuz-5.0.18-1-pve initrd /EFI/proxmox/5.0.18-1-pve/initrd.img-5.0.18-1-pve So basically they just point to the kernel and initrd in the EFI/proxmox directory and start the kernel with the right root option so that the correct boot environment is mounted.\nAt this point it makes sense to reiterate what a boot evironment is. Up until now we have defined a boot environment loosely as a file system snapshot we can boot into. At this point we have to refine the “we can boot into” part of this definition: A Boot environment is a filesystem snapshot together with the bootloader configuration as well as the kernel and initrd files from the moment the snapshot was taken.\nThe boot environment of pve-1 consists specifically of the following files from the ESP partition:\n. ├── EFI │ ├── proxmox │ │ ├── 5.0.15-1-pve │ │ │ ├── initrd.img-5.0.15-1-pve │ │ │ └── vmlinuz-5.0.15-1-pve │ │ └── 5.0.18-1-pve │ │ ├── initrd.img-5.0.18-1-pve │ │ └── vmlinuz-5.0.18-1-pve └── loader └── entries ├── proxmox-5.0.15-1-pve.conf └── proxmox-5.0.18-1-pve.conf If you head over to the part of the zedenv documentation on systemd-boot, you see that there the creation of an /env directory that holds all of the boot environment specific files on the ESP is proposed in that coupled with a bit of bind-mount magic tricks the underlying system into always finding the right files inside of /boot, when actually only the files that that belong to the currently active boot environment are mounted.\nThis does not apply to our Proxmox situation, there is for example no mounted ESP. Also the pve-efiboot-tool takes care of the kernel versions that are available in the EFI/proxmox/ directory so unless they are marked as manually installed (which you can do in Proxmox) some of the kernel versions will disappear at some point in time rendering the boot environment incomplete.\nMaking zedenv and Proxmox play well together I should probably point out here, that this part is more of a proposition, of how this could work than necessarily a good solution (it does work though). I\u0026rsquo;m pretty new to Proxmox and not at all an expert, when it comes to boot environments, so better take everything here with a few grains of salt.\nAs we\u0026rsquo;ve learned in the previous part, zedenv is pretty awesome, but by design not exactly aimed at working with Proxmox. That being said, zedenv is actually written with plugins in mind, I\u0026rsquo;ve skimmed the code and there is a bunch of pre- and post-hooking going on, so I think it could be possible to just set up some sort of Proxmox plugin for zedenv. Since I\u0026rsquo;m not a python guy and there\u0026rsquo;s of course also the option to add support to this from the Proxmox side, I\u0026rsquo;ll just write down how I\u0026rsquo;d imagine Proxmox and a boot environment manager such as zedenv to work together without breaking too much on either side.\nFor this we have to consider the following things:\nremember the NR? I guess zedenv just checks what is currently mounted as / in order to find out what the currently active boot environment (N) is. In Proxmox in order to find out what the active boot-environment after a reboot (R) will be, we can just check the /etc/kernel/cmdline file for the root option\nactivate does not work with Proxmox, instead of creating the systemd-boot files, we could just run pve-efiboot-tool refresh, which creates a copy of the necessary bootloader files on all ESPs and doing so also activates the boot environment, that is referenced in the /etc/kernel/cmdline file. So with a template cmdline file like for example /etc/kernel/cmdline.template, we could run something like this, basically creating a cmdline, that points to the correct boot environment and refresh the content of all ESPs at the same time:\nsed \u0026#39;s/BOOTENVIRONMENT/pve-2/\u0026#39; /etc/kernel/cmdline.template \u0026gt; /etc/kernel/cmdline pve-efiboot-tool refresh That\u0026rsquo;s about everything we\u0026rsquo;d need to replace in order to get a single boot environment to work with zedenv.\nNow if we want to have access to multiple boot environments at the same time, we can just do something quite similiar to what Proxmox does: The idea here would be that after any run of pve-efiboot-tool refresh, we mount one of the ESPs and grab the all the files we need from EFI/proxmox/ as well as loader/entries/ and store them somewhere on rpool. We could for example create rpool/be for this exact reason.\nzfs create -o mountpoint=/be rpool/be The initrd that comes with a kernel might be customized, so although the initrd file is built against a kernel we want to always keep it directly tied to a specific boot environments. I\u0026rsquo;m not sure if the kernel files change over time or not though, so there\u0026rsquo;s two options here (Let\u0026rsquo;s just assume the first of the following to be sure):\nIf they are changing over time, while keeping the same kernel version, it is probably best to save them per boot environment If they don\u0026rsquo;t change over time, we could just grab them once and then link to the kernel once we\u0026rsquo;ve already saved it A better Proof of Concept So lets suppose that we have just freshly created the boot environment EXAMPLE and activated it as described above using pve-efiboot-tool refresh on a /etc/kernel/cmdline file that we derived from the /etc/kernel/cmdline.template and Proxmox has just set up all of our ESPs.\nNow we create a /be/EXAMPLE/ directory, then mount one of the ESPs and in the next step copy over the files we are interested in.\n(zedenv-venv) root@caliban:~# BOOTENVIRONMENT=EXAMPLE (zedenv-venv) root@caliban:~# mkdir -p /be/$BOOTENVIRONMENT/{kernel,entries} (zedenv-venv) root@caliban:~# zedenv create $BOOTENVIRONMENT (zedenv-venv) root@caliban:~# zedenv list Name Active Mountpoint Creation pve-1 NR - Mon-Aug-19-1:27-2019 EXAMPLE - Wed-Aug-28-20:24-2019 (zedenv-venv) root@caliban:~# pve-efiboot-tool refresh Running hook script \u0026#39;pve-auto-removal\u0026#39;.. Running hook script \u0026#39;zz-pve-efiboot\u0026#39;.. Re-executing \u0026#39;/etc/kernel/postinst.d/zz-pve-efiboot\u0026#39; in new private mount namespace.. Copying and configuring kernels on /dev/disk/by-uuid/EE5A-CB7D Copying kernel and creating boot-entry for 5.0.15-1-pve Copying kernel and creating boot-entry for 5.0.18-1-pve Copying and configuring kernels on /dev/disk/by-uuid/EE5B-4F9B Copying kernel and creating boot-entry for 5.0.15-1-pve Copying kernel and creating boot-entry for 5.0.18-1-pve (zedenv-venv) root@caliban:~# mount /dev/disk/by-uuid/$(cat /etc/kernel/pve-efiboot-uuids | head -n 1) /boot/efi (zedenv-venv) root@caliban:~# cp -r /boot/efi/EFI/proxmox/* /be/$BOOTENVIRONMENT/kernel/ (zedenv-venv) root@caliban:~# cp /boot/efi/loader/entries/proxmox-*.conf /be/$BOOTENVIRONMENT/entries/ Now that we have copied over all files we need, it\u0026rsquo;s time to modify them /be should look like this:\n/be/ └── EXAMPLE ├── entries │ ├── proxmox-5.0.15-1-pve.conf │ └── proxmox-5.0.18-1-pve.conf └── kernel ├── 5.0.15-1-pve │ ├── initrd.img-5.0.15-1-pve │ └── vmlinuz-5.0.15-1-pve └── 5.0.18-1-pve ├── initrd.img-5.0.18-1-pve └── vmlinuz-5.0.18-1-pve Lets first fix the file names, /entries/proxmox-X.X.X-X-pve.conf should be named after our boot environment EXAMPLE:\nBOOTENVIRONMENT=EXAMPLE cd /be/EXAMPLE/entries/ for f in *.conf; do mv $f $(echo $f | sed \u0026#34;s/proxmox/$BOOTENVIRONMENT/\u0026#34;); done Next let\u0026rsquo;s look into those configuration files:\ntitle Proxmox Virtual Environment version 5.0.18-1-pve options [...] root=ZFS=rpool/ROOT/EXAMPLE boot=zfs linux /EFI/proxmox/5.0.18-1-pve/vmlinuz-5.0.18-1-pve initrd /EFI/proxmox/5.0.18-1-pve/initrd.img-5.0.18-1-pve As you can see they reference the initrd as well as the kernel (with the ESP being /). We are going to put these into /env/EXAMPLE/ instead, so lets fix this. Also we want to change the Title from “Proxmox Virtual Environment” to the name of the boot environment.\ncd /be/EXAMPLE/entries BOOTENVIRONMENT=EXAMPLE sed -i \u0026#34;s/EFI/env/;s/proxmox/$BOOTENVIRONMENT/\u0026#34; *.conf sed -i \u0026#34;s/Proxmox Virtual Environment/$BOOTENVIRONMENT/\u0026#34; *.conf The configuration files now look like this:\n(zedenv-venv) root@caliban:/be/EXAMPLE/entries# cat EXAMPLE-5.0.18-1-pve.conf title EXAMPLE version 5.0.18-1-pve options [...] root=ZFS=rpool/ROOT/EXAMPLE boot=zfs linux /env/EXAMPLE/5.0.18-1-pve/vmlinuz-5.0.18-1-pve initrd /env/EXAMPLE/5.0.18-1-pve/initrd.img-5.0.18-1-pve Remember the loader.conf file that only consisted of 2 lines? We\u0026rsquo;ll also need to have one like it in order to boot into our new boot environment by default, so we\u0026rsquo;ll copy and modify it as well.\nBOOTENVIRONMENT=EXAMPLE cat /boot/efi/loader/loader.conf | sed \u0026#34;s/proxmox/$BOOTENVIRONMENT/\u0026#34; \u0026gt; /be/$BOOTENVIRONMENT/loader.conf Finally don\u0026rsquo;t forget to unmount the ESP:\n(zedenv-venv) root@caliban:~# umount /boot/efi/ Now /be should look similiar to this and at this point we are done with the part where we configure our files:\n(zedenv-venv) root@caliban:~# tree /be /be └── EXAMPLE ├── entries │ ├── EXAMPLE-5.0.15-1-pve.conf │ └── EXAMPLE-5.0.18-1-pve.conf ├── kernel │ ├── 5.0.15-1-pve │ │ ├── initrd.img-5.0.15-1-pve │ │ └── vmlinuz-5.0.15-1-pve │ └── 5.0.18-1-pve │ ├── initrd.img-5.0.18-1-pve │ └── vmlinuz-5.0.18-1-pve └── loader.conf Next we can deploy our configuration to the ESP, to do so, we\u0026rsquo;ll do the following:\niterate over the ESPs used by Proxmox (remember their UUIDs are in /etc/kernel/pve-efiboot-uuids) mount a ESP copy the files in /be/EXAMPLE/kernel into /env/EXAMPLE {{{sidenote(remember that this / is the root of the ESP)}} copy the config files in /be/EXAMPLE/entries into /loader/entries/ replace /loader/loader.conf with our modified loader.conf in order to activate the boot environment unmount the ESP repeat until we\u0026rsquo;ve updated all ESPs from /etc/kernel/pve-efiboot-uuids In bash this could look like this:\nBOOTENVIRONMENT=EXAMPLE for esp in $(cat /etc/kernel/pve-efiboot-uuids); do mount /dev/disk/by-uuid/$esp /boot/efi mkdir -p /boot/efi/env/$BOOTENVIRONMENT cp -r /be/$BOOTENVIRONMENT/kernel/* /boot/efi/env/$BOOTENVIRONMENT/ cp /be/$BOOTENVIRONMENT/entries/* /boot/efi/loader/entries/ cat /be/$BOOTENVIRONMENT/loader.conf \u0026gt; /boot/efi/loader/loader.conf umount /boot/efi done Also note that reactivating the boot environment is now simply a matter of replacing the loader.conf.\nNow let\u0026rsquo;s check if everything looks good. The ESPs should look like this:\n(zedenv-venv) root@caliban:~# tree /boot/efi/ /boot/efi/ ├── EFI │ ├── BOOT │ │ └── BOOTX64.EFI │ ├── proxmox │ │ ├── 5.0.15-1-pve │ │ │ ├── initrd.img-5.0.15-1-pve │ │ │ └── vmlinuz-5.0.15-1-pve │ │ └── 5.0.18-1-pve │ │ ├── initrd.img-5.0.18-1-pve │ │ └── vmlinuz-5.0.18-1-pve │ └── systemd │ └── systemd-bootx64.efi ├── env │ └── EXAMPLE │ ├── 5.0.15-1-pve │ │ ├── initrd.img-5.0.15-1-pve │ │ └── vmlinuz-5.0.15-1-pve │ └── 5.0.18-1-pve │ ├── initrd.img-5.0.18-1-pve │ └── vmlinuz-5.0.18-1-pve └── loader ├── entries │ ├── EXAMPLE-5.0.15-1-pve.conf │ ├── EXAMPLE-5.0.18-1-pve.conf │ ├── proxmox-5.0.15-1-pve.conf │ └── proxmox-5.0.18-1-pve.conf └── loader.conf The loader.conf should default to EXAMPLE entries:\n(zedenv-venv) root@caliban:~# cat /boot/efi/loader/loader.conf | grep EXAMPLE default EXAMPLE-* The EXAMPLE-*.conf files should point to the files in /env/EXAMPLE:\n(zedenv-venv) root@caliban:~# cat /boot/efi/loader/entries/EXAMPLE-*.conf | grep \u0026#34;env/EXAMPLE\u0026#34; linux /env/EXAMPLE/5.0.15-1-pve/vmlinuz-5.0.15-1-pve initrd /env/EXAMPLE/5.0.15-1-pve/initrd.img-5.0.15-1-pve linux /env/EXAMPLE/5.0.18-1-pve/vmlinuz-5.0.18-1-pve initrd /env/EXAMPLE/5.0.18-1-pve/initrd.img-5.0.18-1-pve At this point all that\u0026rsquo;s left is to reboot and check if everything works. And it does indeed:\nConclusion and Future Work We\u0026rsquo;ve done it! It works and lets face it, it is an amazing feature, we all want to have this on our Hypervisors.\nBut let\u0026rsquo;s not forget that what we\u0026rsquo;ve done here is pretty much to write down two Proofs of Concepts. In order to make sure everything works nicely we probably need to at least add a bunch of checks.\nFrom my point of view there\u0026rsquo;s basically two ways all of this could go forward:\nThe zedenv project adopts support for the Proxmox platform by relaxing their assumption that the EFI Systems Partition has to be mounted and writes a bunch of gluecode around the Proxmox tooling. The Proxmox team adds native support for Boot Environments to their pve-tooling. This would mean that they would have to add all the functionality of a boot environment manager such as zedenv or it\u0026rsquo;s unix counterpart beadm, they would also have to consider making EFI System Partition bigger, and the kernels and boot environments wouldn\u0026rsquo;t necessarily require something like rpool/be to be stored on (On the topic of not having to use some intermediate storage such as rpool/be, having the boot environments stored additionally on zfs would enable to keep the ESP relatively small and not only distinguish between active and inactive boot environments, but also between ones that are loaded onto the ESP and those that are merely available, this isn\u0026rsquo;t exactly what zedenv or beadm do, but it might be a nice feature to really be able to go back to older version in the Enterprise context of Proxmox) , but that should be trivial. Also I do like the idea of templating the /etc/kernel/cmdline file so that generating boot configurations works using the included pve tooling. Personally I think it would be great to get support for boot environments directly from Proxmox, since - let\u0026rsquo;s face it - it\u0026rsquo;s almost working out of the box anyway and a custom tool such as pve-beadm would better fit the way Proxmox handles the boot process than something that is build around it.\nAnyway that\u0026rsquo;s about all from me this time. I\u0026rsquo;m now a few days into my little venture of ‘just installing Proxmox because it will work out of the box which will save me some time\u0026rsquo; and I feel quite confident that I\u0026rsquo;ve almost reached the stage where I\u0026rsquo;m actually done with installing the host system and can start running some guests. There\u0026rsquo;s literally only one or two things left to try out..\nIf anyone at Proxmox reads this, you guys are amazing! I haven\u0026rsquo;t even really finished my first install yet and I\u0026rsquo;m hooked with your system!\n","permalink":"https://oblivious.observer/posts/poc-boot-environments-proxmoxve6/","summary":"Dear Reader, this time I would like to invite you onto a small journey: To boldly go where no man has gone before (Alright, that\u0026rsquo;s not true, but I think it\u0026rsquo;s the first time someone documents this kind of thing in the context of Proxmox). We\u0026rsquo;re about to embark on a journey to make your Proxmox host quite literally immortal. Also since what we are essentially doing here is only a Proof of concept, you probably shouldn\u0026rsquo;t use it in production, but as it\u0026rsquo;s really amazing, so you might want to try it out in a test environment.","title":"Proof of Concept: Adding Boot Environments to Proxmox VE 6"},{"content":"This describes how to set up a fully encrypted Proxmox VE 6 host with ZFS root and unlocking it remotely using the dropbear ssh server. Also it describes how you can do that, while keeping systemd-boot and thus also the pve tooling intact. (I\u0026rsquo;m not sure if the pve tooling still works if you replace systemd-boot with grub, which seems to be the common solution to creating this kind of setup, maybe it does.)\nUpdate: This post has been translated into czech language and was published on abclinuxu.cz.\nOverview We are going to do the following:\nInstall Proxmox VE 6 on our machine\nMinimally configure the Installation\nEncrypt the Installation:\nRemove a Disk from the ZFS-Pool Encrypt the Disk with LUKS Add it back to the ZFS Pool Repeat until all disks are encrypted Set up Dropbear and Systemd-boot to enable remote unlocking\nPrerequisites There really only is one prerequisite apart from having a machine you want to install Proxmox onto: You need a second harddrive, which we will setup in a ZFS RAID1 configuration. If you don\u0026rsquo;t want to have your root devices mirrored, you will still need a second drive that you can use as a temporary mirrored root device, otherwise you\u0026rsquo;d have to install and set up an encrypted debian and then install proxmox on top of that.\nApart from that I\u0026rsquo;ll assume that you are probably fairly familiar with how full disk encryption works on linux systems, if not you might want to read up on that before you start messing around with any hardware. Please don\u0026rsquo;t try this out on a production system, if you don\u0026rsquo;t exactly know what you\u0026rsquo;re doing.\nInstalling Proxmox VE 6 The only thing you have to make sure is to set up the ZFS RAID 1 during the installation. The rest should be pretty much straight-forward.\nMinimal post-installation For some odd reason PATH in a regular shell is different from PATH in the javascript terminal from the webinterface. You might want to take care of that:\necho \u0026#34;export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34; \u0026gt;\u0026gt; ~/.bashrc Remove the subscription popup notice (source):\nsed -i.bak \u0026#34;s/data.status !== \u0026#39;Active\u0026#39;/false/g\u0026#34; /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js \u0026amp;\u0026amp; systemctl restart pveproxy.service Set up the community repositories:\nrm /etc/apt/sources.list.d/pve-enterprise.list echo \u0026#39;deb http://download.proxmox.com/debian/pve buster pve-no-subscription\u0026#39; \u0026gt; pve-community.list Update the host:\napt update apt upgrade Encrypt your installation This is partly taken over from this wonderful post. (The GRUB_ENABLE_CRYPTODISK option that is mentioned in the forum post does not apply here, since the boot partition is not encrypted. If you want this level of security, then this is probably not the right guide for you. Also from my understanding encrypting the boot partition means that you can\u0026rsquo;t use dropbear to unlock the system remotely since nothing has booted so far. It is a pretty nice way to set up fully encrypted laptops though, so you should definitely look into this if you haven\u0026rsquo;t already!)\nRight after the installation the host should look similiar to this (lsblk):\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 465.8G 0 disk ├─sda1 8:1 0 1007K 0 part ├─sda2 8:2 0 512M 0 part └─sda3 8:3 0 465.3G 0 part sdb 8:16 0 931.5G 0 disk sdc 8:32 0 931.5G 0 disk sdd 8:48 0 465.8G 0 disk ├─sdd1 8:49 0 1007K 0 part ├─sdd2 8:50 0 512M 0 part └─sdd3 8:51 0 465.3G 0 part The third partition of both harddrives contains our installation, the first and second are the boot and efi partitions.\nzpool status should return something like this:\nNAME STATE READ WRITE CKSUM rpool ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 ata-Samsung_SSD_850_EVO_500GB_XXXXXXXXXXXXXXX-part3 ONLINE 0 0 0 ata-WDC_WDS500G2B0A-XXXXXX_XXXXXXXXXXXX-part3 ONLINE 0 0 0 You might want to install cryptsetup at this point:\napt install cryptsetup Remove the first partition from rpool, then encrypt it, mount it to /dev/mapper/cryptrpool1 and reattach it to rpool:\nzpool detach rpool ata-Samsung_SSD_850_EVO_500GB_XXXXXXXXXXXXXXX-part3 cryptsetup luksFormat /dev/disk/by-id/ata-Samsung_SSD_850_EVO_500GB_XXXXXXXXXXXXXXX-part3 cryptsetup luksOpen /dev/disk/by-id/ata-Samsung_SSD_850_EVO_500GB_XXXXXXXXXXXXXXX-part3 cryptrpool1 zpool attach rpool ata-WDC_WDS500G2B0A-XXXXXX_XXXXXXXXXXXX-part3 cryptrpool1 Wait until the scan line of zpool status displays that the drive has been resilvered successfully. You should see something similiar to this:\nscan: resilvered 1022M in 0 days 00:00:04 with 0 errors on Wed Aug 21 17:27:55 2019 Now repeat this step with the other drive:\nzpool detach rpool ata-WDC_WDS500G2B0A-XXXXXX_XXXXXXXXXXXX-part3 cryptsetup luksFormat /dev/disk/by-id/ata-WDC_WDS500G2B0A-XXXXXX_XXXXXXXXXXXX-part3 cryptsetup luksOpen /dev/disk/by-id/ata-WDC_WDS500G2B0A-XXXXXX_XXXXXXXXXXXX-part3 cryptrpool2 zpool attach rpool cryptrpool1 cryptrpool2 At this point lsblk should output something like this:\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 465.8G 0 disk ├─sda1 8:1 0 1007K 0 part ├─sda2 8:2 0 512M 0 part └─sda3 8:3 0 465.3G 0 part └─cryptrpool1 253:0 0 465.3G 0 crypt sdb 8:16 0 931.5G 0 disk sdc 8:32 0 931.5G 0 disk sdd 8:48 0 465.8G 0 disk ├─sdd1 8:49 0 1007K 0 part ├─sdd2 8:50 0 512M 0 part └─sdd3 8:51 0 465.3G 0 part └─cryptrpool2 253:1 0 465.3G 0 crypt And zpool status should return something like this:\nNAME STATE READ WRITE CKSUM rpool ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 cryptrpool1 ONLINE 0 0 0 cryptrpool2 ONLINE 0 0 0 Next we want to set up /etc/crypttab, use blkid to get the PARTUUID from both harddrives:\nblkid -s PARTUUID -o value /dev/disk/by-id/ata-Samsung_SSD_850_EVO_500GB_XXXXXXXXXXXXXXX-part3 blkid -s PARTUUID -o value /dev/disk/by-id/ata-WDC_WDS500G2B0A-XXXXXX_XXXXXXXXXXXX-part3 Then add them to /etc/crypttab:\nroot@caliban:~# cat /etc/crypttab # \u0026lt;target name\u0026gt; \u0026lt;source device\u0026gt; \u0026lt;key file\u0026gt; \u0026lt;options\u0026gt; cryptrpool1 PARTUUID=XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX none luks,discard,initramfs cryptrpool2 PARTUUID=YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY none luks,discard,initramfs Then update the initramfs and make sure it is put on the boot partition (this is where we deviate from the forum post I\u0026rsquo;ve linked above):\nupdate-initramfs -u -k all pve-efiboot-tool refresh In case you\u0026rsquo;re wondering at this point, yes I\u0026rsquo;m also getting the cryptsetup error message on running update-initramfs, it still works though:\ncryptsetup: ERROR: Couldn\u0026#39;t resolve device rpool/ROOT/pve-1 cryptsetup: WARNING: Couldn\u0026#39;t determine root device Now you should be able to reboot and unlock the ZFS partitions by entering the passphrase.\nSetting up Dropbear to remotely unlock the partition Now to the fun part! Since we aren\u0026rsquo;t using grub here, we have to take a few different steps from what we usually do in this kind of setup.\nHere are a few interesting links you might want to look into as well:\nThis nicely explains how to use the keys Dropbear already generates on install instead of recreating them. The freedesktop page on systemd-boot This little article on setting up archlinux with dropbear does not fully apply to our Proxmox case, but it gives enough information on how we can tell systemd-boot to tell the kernel to start with the options we want (unlike the article states, we need to use the udev name for assigning the IP and I was getting error messages, when supplying nameserver IPs). First install dropbear and busybox:\napt install dropbear busybox In /etc/initramfs-tools/initramfs.conf enable busybox:\nroot@caliban:~# cat /etc/initramfs-tools/initramfs.conf | grep ^BUSYBOX BUSYBOX=y Then convert the dropbear keys:\ncd /etc/dropbear-initramfs/ /usr/lib/dropbear/dropbearconvert dropbear openssh dropbear_rsa_host_key id_rsa dropbearkey -y -f dropbear_rsa_host_key | grep \u0026#34;^ssh-rsa \u0026#34; \u0026gt; id_rsa.pub And add your public key to the authorized keys:\nvi /etc/dropbear-initramfs/authorized_keys Make sure dropbear starts by toggling the NO_START value in /etc/default/dropbear.\nroot@caliban:~# cat /etc/default/dropbear | grep ^NO_START NO_START=0 Finally configure dropbear to use a different Port than 22 in order to avoid getting the MITM warning, by changing the DROPBEAR_OPTIONS value in /etc/dropbear-initramfs/config:\nroot@caliban:~# cat /etc/dropbear-initramfs/config | grep ^DROPBEAR_OPTIONS DROPBEAR_OPTIONS=\u0026#34;-p 12345\u0026#34; You can then set up two entries in your ~/.ssh/config:\n$ cat ~/.ssh/config Host * ServerAliveInterval 120 Host unlock_caliban Hostname 1.2.3.4 User root Port 12345 Host caliban Hostname 1.2.3.4 Port 22 At this point I noticed, that only the third partition of both of the harddrives with the rpool were mounted. When mounting a boot partition, I found that there were systemd-boot configuration files, but they seemed to be autogenerated by Proxmox, whenever pve-efiboot-tool refresh was run. So I looked into /usr/sbin/pve-efiboot-tool, and followed the code until I came out in /etc/kernel/postinst.d/zz-pve-efiboot, which contains the code that generates the systemd-boot configuration files:\n# [...] for kver in ${BOOT_KVERS}; do linux_image=\u0026#34;/boot/vmlinuz-${kver}\u0026#34; initrd=\u0026#34;/boot/initrd.img-${kver}\u0026#34; if [ ! -f \u0026#34;${linux_image}\u0026#34; ]; then warn \u0026#34;No linux-image ${linux_image} found - skipping\u0026#34; continue fi if [ ! -f \u0026#34;${initrd}\u0026#34; ]; then warn \u0026#34;No initrd-image ${initrd} found - skipping\u0026#34; continue fi warn \u0026#34; Copying kernel and creating boot-entry for ${kver}\u0026#34; KERNEL_ESP_DIR=\u0026#34;${PMX_ESP_DIR}/${kver}\u0026#34; KERNEL_LIVE_DIR=\u0026#34;${esp}/${KERNEL_ESP_DIR}\u0026#34; mkdir -p \u0026#34;${KERNEL_LIVE_DIR}\u0026#34; cp -u --preserve=timestamps \u0026#34;${linux_image}\u0026#34; \u0026#34;${KERNEL_LIVE_DIR}/\u0026#34; cp -u --preserve=timestamps \u0026#34;${initrd}\u0026#34; \u0026#34;${KERNEL_LIVE_DIR}/\u0026#34; # create loader entry cat \u0026gt; \u0026#34;${esp}/loader/entries/proxmox-${kver}.conf\u0026#34; \u0026lt;\u0026lt;- EOF title ${LOADER_TITLE} version ${kver} options ${CMDLINE} linux /${KERNEL_ESP_DIR}/vmlinuz-${kver} initrd /${KERNEL_ESP_DIR}/initrd.img-${kver} EOF done # [...] For us, the cat part is especially interesting: the CMDLINE variable in the line beginning with “=options=” contains the boot options for the Linux kernel. This variable is assigned in the same file:\n# [...] if [ -f /etc/kernel/cmdline ]; then CMDLINE=\u0026#34;$(cat /etc/kernel/cmdline)\u0026#34; else warn \u0026#34;No /etc/kernel/cmdline found - falling back to /proc/cmdline\u0026#34; CMDLINE=\u0026#34;$(cat /proc/cmdline)\u0026#34; fi # [...] Apparently /etc/kernel/cmdline is the place where Proxmox stores it\u0026rsquo;s boot options. The file contains one single line:\nroot=ZFS=rpool/ROOT/pve-1 boot=zfs After finding the /etc/kernel/cmdline file, I did a bit of searching and according to the Proxmox documentation, it is actually the apropriate file to change in this case.\nNow that we have identified the file we can use to configure our kernel options, there are two things we want to add:\nwe want to make sure the network interface comes up so that we can ssh into the initramfs, we will use the ip option for that. It uses the following format (look here for further reading):\nip=\u0026lt;client-ip\u0026gt;:\u0026lt;server-ip\u0026gt;:\u0026lt;gw-ip\u0026gt;:\u0026lt;netmask\u0026gt;:\u0026lt;hostname\u0026gt;:\u0026lt;device\u0026gt;:\u0026lt;autoconf\u0026gt;: \u0026lt;dns0-ip\u0026gt;:\u0026lt;dns1-ip\u0026gt;:\u0026lt;ntp0-ip\u0026gt;: I omitted everything after autoconf, something like this works for me:\nip=1.2.3.4::1.2.3.1:255.255.255.0:caliban:enpXsY:none: also we have to tell the kernel which devices the cryptodevices are that we want to unlock, which is done using the cryptodevice option (here we have to supply the PARTUUIDs for both of our harddrives):\ncryptdevice=UUID=XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX cryptdevice=UUID=YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY The whole content of /etc/kernel/cmdline looks like this:\nip=1.2.3.4::1.2.3.1:255.255.255.0:caliban:enpXsY:none: cryptdevice=UUID=XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX cryptdevice=UUID=YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY root=ZFS=rpool/ROOT/pve-1 boot=zfs The last thing to do is to:\nupdate-initramfs -u -k all pve-efiboot-tool refresh Now you should be able to reboot your machine and ssh into the busybox on the port you just configured for dropbear. From there you can unlock the drives by running something like this (You\u0026rsquo;ll have to input it twice since you have two encrypted drives):\necho -n \u0026#34;password\u0026#34; \u0026gt; /lib/cryptsetup/passfifo Or:\n/lib/cryptsetup/askpass \u0026#34;password: \u0026#34; \u0026gt; /lib/cryptsetup/passfifo Or you can also use the cryptroot-unlock script that is preinstalled already, which also prompts you to enter the password twice.\nIf you\u0026rsquo;re lazy, you can also use put the following script into /etc/initramfs-tools/hooks and make it executable. I basically merged the above example of using /lib/cryptsetup/askpass with a version of a unlock script I had lying around, it looks like it might have been from this gist. It asks you for a passphrase and then uses echo to write it into /lib/cryptsetup/passfifo twice (since I use 2 harddrives) with one second delay in between, then kills the session so the system can come up (I noticed, that /etc/motd, which contains instructions on how to unlock your drive is not displayed in the busybox session. . You probably shouldn\u0026rsquo;t use it, but it seems to work for me):\n#!/bin/sh PREREQ=\u0026#34;dropbear\u0026#34; prereqs() { echo \u0026#34;$PREREQ\u0026#34; } case \u0026#34;$1\u0026#34; in prereqs) prereqs exit 0 ;; esac . \u0026#34;${CONFDIR}/initramfs.conf\u0026#34; . /usr/share/initramfs-tools/hook-functions if [ \u0026#34;${DROPBEAR}\u0026#34; != \u0026#34;n\u0026#34; ] \u0026amp;\u0026amp; [ -r \u0026#34;/etc/crypttab\u0026#34; ] ; then cat \u0026gt; \u0026#34;${DESTDIR}/bin/unlock\u0026#34; \u0026lt;\u0026lt; EOF #!/bin/sh unlock_devices() { pw=\u0026#34;\\$(/lib/cryptsetup/askpass \u0026#34;password: \u0026#34;)\u0026#34; echo -n \\$pw \u0026gt; /lib/cryptsetup/passfifo sleep 1 echo -n \\$pw \u0026gt; /lib/cryptsetup/passfifo } if unlock_devices; then # kill \\`ps | grep cryptroot | grep -v \u0026#34;grep\u0026#34; | awk \u0026#39;{print \\$1}\u0026#39;\\` # following line kill the remote shell right after the passphrase has # been entered. kill -9 \\`ps | grep \u0026#34;\\-sh\u0026#34; | grep -v \u0026#34;grep\u0026#34; | awk \u0026#39;{print \\$1}\u0026#39;\\` exit 0 fi exit 1 EOF chmod 755 \u0026#34;${DESTDIR}/bin/unlock\u0026#34; mkdir -p \u0026#34;${DESTDIR}/lib/unlock\u0026#34; cat \u0026gt; \u0026#34;${DESTDIR}/lib/unlock/plymouth\u0026#34; \u0026lt;\u0026lt; EOF #!/bin/sh [ \u0026#34;\\$1\u0026#34; == \u0026#34;--ping\u0026#34; ] \u0026amp;\u0026amp; exit 1 /bin/plymouth \u0026#34;\\$@\u0026#34; EOF chmod 755 \u0026#34;${DESTDIR}/lib/unlock/plymouth\u0026#34; echo To unlock root-partition run \u0026#34;unlock\u0026#34; \u0026gt;\u0026gt; ${DESTDIR}/etc/motd fi That\u0026rsquo;s pretty much all of it, you can now start enjoying remote reboots on your freshly encrypted Proxmox host.\n","permalink":"https://oblivious.observer/posts/proxmoxve6-zfs-luks-systemdboot-dropbear/","summary":"This describes how to set up a fully encrypted Proxmox VE 6 host with ZFS root and unlocking it remotely using the dropbear ssh server. Also it describes how you can do that, while keeping systemd-boot and thus also the pve tooling intact. (I\u0026rsquo;m not sure if the pve tooling still works if you replace systemd-boot with grub, which seems to be the common solution to creating this kind of setup, maybe it does.","title":"Encrypting Proxmox VE 6: ZFS, LUKS, systemd-boot and Dropbear"}]